{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MLFinal-MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAPXN1XeRWlZ",
        "colab_type": "code",
        "outputId": "c116f4c6-5ade-4d3e-9232-3f648b8608ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC-QrcB8SfYi",
        "colab_type": "code",
        "outputId": "4d06e7fc-d3b5-49c9-dd84-eac2261123cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://learner.csie.ntu.edu.tw/~judge/ml19spring/ml19spring.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-02 08:24:14--  https://learner.csie.ntu.edu.tw/~judge/ml19spring/ml19spring.zip\n",
            "Resolving learner.csie.ntu.edu.tw (learner.csie.ntu.edu.tw)... 140.112.90.193\n",
            "Connecting to learner.csie.ntu.edu.tw (learner.csie.ntu.edu.tw)|140.112.90.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3780741206 (3.5G) [application/zip]\n",
            "Saving to: ‘ml19spring.zip’\n",
            "\n",
            "ml19spring.zip      100%[===================>]   3.52G  16.3MB/s    in 5m 8s   \n",
            "\n",
            "2019-06-02 08:29:24 (11.7 MB/s) - ‘ml19spring.zip’ saved [3780741206/3780741206]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqFi8yQLSlm8",
        "colab_type": "code",
        "outputId": "79558587-ea7e-4967-fa05-ac60255aeaaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!unzip -q ml19spring.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open ml19spring.zip, ml19spring.zip.zip or ml19spring.zip.ZIP.\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4o6D8EJPRLS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRoIQ2-EbsYq",
        "colab_type": "code",
        "outputId": "2f3ed759-ec04-487b-f797-a77b6b9dfb84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "# Save model to your Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThyMATs2TBoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "DataDir='./gdrive/My Drive/term_2019_1/MLTech/Final/data/'\n",
        "x_test = np.load(DataDir+'X_test.npz')['arr_0']\n",
        "X = np.load(DataDir+'X_train.npz')['arr_0']\n",
        "Y = np.load(DataDir+'Y_train.npz')['arr_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gab1tIdQTze0",
        "colab_type": "code",
        "outputId": "589f520b-7935-4006-e613-d0e5d201d5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "source": [
        "dirPath = './MLP_output/'\n",
        "!mkdir ./MLP_output\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEqolJs4bSTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(copy=False)\n",
        "X = scaler.fit_transform(X)\n",
        "x_test = scaler.transform( x_test )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPeL0Z6ewL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_fea = np.load(DataDir+'important_feat_0.0002.npy')\n",
        "X=X[:,important_fea]\n",
        "x_test = x_test[:,important_fea]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ID5oapOlAQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del x_train,x_val,y_train,y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0otbP-OUN9X",
        "colab_type": "code",
        "outputId": "0201ab24-7f48-4db5-c4de-d1450e980ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "x_train,x_val,y_train,y_val=train_test_split(X,Y,test_size=0.1,random_state=44)\n",
        "print(x_train.shape,x_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(42750, 10000) (4750, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAydVCGJmtnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model\n",
        "# 23.82688"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDlM_4wOcgLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf5eojAHUdCk",
        "colab_type": "code",
        "outputId": "71220448-c43e-4fb1-9376-a565f54e8d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14419
        }
      },
      "source": [
        "import pickle\n",
        "from keras.layers import *\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "def MLP_MAE_onlyOutput(input_size):\n",
        "    same_input = Input(shape=(input_size,), dtype='float32',name='input')\n",
        "    X1 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X1 = PReLU()(X1)\n",
        "    X1 = Dense(128,kernel_initializer='normal')(X1)\n",
        "    X1 = PReLU()(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(68,kernel_initializer='normal',activation='tanh')(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(32,kernel_initializer='normal',activation='tanh')(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    y1 = Dense(1,kernel_initializer='normal',name='output_y1')(X1)\n",
        "    \n",
        "    X2 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X2 = PReLU()(X2)\n",
        "    X2 = Dense(128,kernel_initializer='normal')(X2)\n",
        "    X2 = PReLU()(X2)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(68,kernel_initializer='normal',activation='tanh')(X2)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(32,kernel_initializer='normal',activation='tanh')(X2)\n",
        "#     X2=BatchNormalization()(X2)\n",
        "    y2 = Dense(1,kernel_initializer='normal',name='output_y2')(X2)\n",
        "    \n",
        "    X3 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X3 = PReLU()(X3)\n",
        "    X3 = Dense(64,kernel_initializer='normal')(X3)\n",
        "    X3 = PReLU()(X3)\n",
        "    y3 = Dense(1,kernel_initializer='normal',name='output_y3')(X3)\n",
        "    model = Model(inputs=same_input, \n",
        "                 outputs=[y1,y2,y3])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'output_y1': 'mean_absolute_error',\n",
        "                       'output_y2': 'mean_absolute_error',\n",
        "                       'output_y3': 'mean_absolute_error'},loss_weights=[300,1,200])\n",
        "    return model\n",
        "def baseline_MLP(input_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512,input_dim=input_size,kernel_initializer='normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(128,kernel_initializer='normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(3,kernel_initializer='normal'))\n",
        "    model.compile(loss = 'mean_absolute_percentage_error',optimizer='adam')\n",
        "    return model\n",
        "def baseline_MLP_WMAE(input_size):\n",
        "    m_input = Input(shape=(input_size,), dtype='float32')\n",
        "    X = Dense(512,kernel_initializer='normal')(m_input)\n",
        "    X = PReLU()(X)\n",
        "    X = Dense(128,kernel_initializer='normal')(X)\n",
        "    X = PReLU()(X)\n",
        "    X1 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(32,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    #     X1 = PReLU()(X1)\n",
        "    X2 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(32,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    #     X2 = PReLU()(X2)\n",
        "    X3 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X3=BatchNormalization()(X3)\n",
        "#     X3 = PReLU()(X3)\n",
        "    y1 = Dense(1,kernel_initializer='normal')(X1)\n",
        "    y2 = Dense(1,kernel_initializer='normal')(X2)\n",
        "    y3 = Dense(1,kernel_initializer='normal')(X3)\n",
        "    model = Model(inputs=m_input, outputs=[y1,y2,y3])\n",
        "    model.compile(optimizer='adam', loss='mean_absolute_error',\n",
        "              loss_weights=[300,1,200])\n",
        "    return model\n",
        "def cnn_WMAE(input1_size,input2_size):\n",
        "    print(input1_size,input2_size)\n",
        "    m_input1 = Input(shape=(5000,1,), dtype='float32')\n",
        "    m_input2 = Input(shape=(50,100,1,), dtype='float32')\n",
        "    x11 = Conv1D(16, 11,input_shape=(5000,1), activation='relu')(m_input1)\n",
        "    x11 =MaxPooling1D(pool_size=60)(x11)\n",
        "    x12 = Conv1D(16, 101,input_shape=(5000,1), activation='relu')(m_input1)\n",
        "    x12 =MaxPooling1D(pool_size=100)(x12)\n",
        "#     x11 = Conv1D(16, 11, activation='relu')(x11)\n",
        "#     x11 =MaxPooling1D(pool_size=8)(x11)\n",
        "#     x12 = Conv1D(64, 50,input_shape=(5000,1), activation='relu')(m_input1)\n",
        "#     x12 =GlobalMaxPooling1D()(x12)\n",
        "#     x13 = Conv1D(64, 100,input_shape=(5000,1), activation='relu')(m_input1)\n",
        "#     x13 =GlobalMaxPooling1D()(x13)\n",
        "#     x1 = Conv1D(32, 10, activation='relu')(x1)\n",
        "#     x1 =MaxPooling1D(pool_size=2)(x1)\n",
        "    x11 = Flatten()(x11)\n",
        "    x12 = Flatten()(x12)\n",
        "\n",
        "    x21 = Conv2D(16,kernel_size=(1,3),input_shape=(50,100,1), activation='relu')(m_input2)\n",
        "    x21 =MaxPooling2D(pool_size=(1,20))(x21)\n",
        "#     x21 = Conv2D(16,kernel_size=(1,3), activation='relu')(x21)\n",
        "#     x21 =MaxPooling2D(pool_size=(1,4))(x21)\n",
        "#     x22 = Conv2D(64,kernel_size=(1,5),input_shape=(50,100,1), activation='relu')(m_input2)\n",
        "#     x22 =GlobalMaxPooling2D()(x22)\n",
        "#     x23 = Conv2D(64,kernel_size=(1,10),input_shape=(50,100,1), activation='relu')(m_input2)\n",
        "#     x23 =GlobalMaxPooling2D()(x23)\n",
        "    x21 = Flatten()(x21)\n",
        "    merged = concatenate(\n",
        "        [x11,x12, x21],\n",
        "        axis=-1)\n",
        "    merged = Dropout(0.15)(merged)\n",
        "    merged=BatchNormalization()(merged)\n",
        "    merged = Dense(units=512,kernel_initializer='normal')(merged)\n",
        "    merged=PReLU()(merged)\n",
        "    merged = Dense(units=128,kernel_initializer='normal')(merged)\n",
        "    merged=PReLU()(merged)\n",
        "#     merged=BatchNormalization()(merged)\n",
        "    y1 = Dense(units=1)(merged)\n",
        "    y2 = Dense(units=1)(merged)\n",
        "    y3 = Dense(units=1)(merged)\n",
        "\n",
        "    model = Model(\n",
        "        inputs=[m_input1, m_input2],\n",
        "        outputs=[y1,y2,y3])\n",
        "    model.compile(optimizer='adam', loss='mean_absolute_error',\n",
        "              loss_weights=[300,1,200])\n",
        "#     model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "    return model\n",
        "def resize_input(X):\n",
        "    X1 = X[:,:5000].reshape(-1,5000,1)\n",
        "    X2 = X[:,5000:].reshape(-1,50,100,1)\n",
        "    print('X1 shape:',X1.shape,' X2 shape:',X2.shape)\n",
        "    return X1,X2\n",
        "dirPath = './MLP_output/' \n",
        "# model = baseline_MLP_WMAE(x_train.shape[1])\n",
        "# model = baseline_MLP(x_train.shape[1])\n",
        "# model = MLP_MAE_onlyOutput(x_train.shape[1])\n",
        "x_train1,x_train2 = resize_input(x_train)\n",
        "x_val1,x_val2 = resize_input(x_val)\n",
        "model = cnn_WMAE(x_train1.shape[1:3],x_train2.shape[1:4])\n",
        "model.summary()\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "NUM_EPOCHS = 200\n",
        "\n",
        "# checkpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss',patience=30)\n",
        "MODELPATH=dirPath+\"MLP_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(MODELPATH, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "print(y_train.shape)\n",
        "y_train_T = np.transpose(y_train)\n",
        "print(y_train_T.shape)\n",
        "y_val_T = np.transpose(y_val)\n",
        "print(y_val_T.shape)\n",
        "# history = model.fit(x=x_train,y=[y_train_T[0],y_train_T[1],y_train_T[2]],batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,[y_val_T[0],y_val_T[1],y_val_T[2]]),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "# history = model.fit(x=x_train,y=y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,y_val),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "# history = model.fit(x=x_train,y=y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,y_val),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "history = model.fit(x=[x_train1,x_train2],y=[y_train_T[0],y_train_T[1],y_train_T[2]],batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "        validation_data=([x_val1,x_val2],[y_val_T[0],y_val_T[1],y_val_T[2]]),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "file_his = open(dirPath+'MLP_history.pickle', 'wb')\n",
        "pickle.dump(history.history, file_his)\n",
        "file_his.close()\n",
        "#25 100 114"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 shape: (42750, 5000, 1)  X2 shape: (42750, 50, 100, 1)\n",
            "X1 shape: (4750, 5000, 1)  X2 shape: (4750, 50, 100, 1)\n",
            "(5000, 1) (50, 100, 1)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 5000, 1)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            (None, 50, 100, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 4990, 16)     192         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 4900, 16)     1632        input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 50, 98, 16)   64          input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1D)  (None, 83, 16)       0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, 49, 16)       0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 50, 4, 16)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 1328)         0           max_pooling1d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 784)          0           max_pooling1d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 3200)         0           max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 5312)         0           flatten_10[0][0]                 \n",
            "                                                                 flatten_11[0][0]                 \n",
            "                                                                 flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 5312)         0           concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 5312)         21248       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 512)          2720256     batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_7 (PReLU)               (None, 512)          512         dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 128)          65664       p_re_lu_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_8 (PReLU)               (None, 128)          128         dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 1)            129         p_re_lu_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1)            129         p_re_lu_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 1)            129         p_re_lu_8[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,810,083\n",
            "Trainable params: 2,799,459\n",
            "Non-trainable params: 10,624\n",
            "__________________________________________________________________________________________________\n",
            "(42750, 3)\n",
            "(3, 42750)\n",
            "(3, 4750)\n",
            "Train on 42750 samples, validate on 4750 samples\n",
            "Epoch 1/200\n",
            "42750/42750 [==============================] - 12s 274us/step - loss: 770.4541 - dense_16_loss: 1.3941 - dense_17_loss: 129.4307 - dense_18_loss: 1.1139 - val_loss: 358.8693 - val_dense_16_loss: 0.5935 - val_dense_17_loss: 127.1426 - val_dense_18_loss: 0.2683\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 358.86932, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 2/200\n",
            "42750/42750 [==============================] - 10s 237us/step - loss: 299.5537 - dense_16_loss: 0.3675 - dense_17_loss: 120.2013 - dense_18_loss: 0.3455 - val_loss: 250.3769 - val_dense_16_loss: 0.2304 - val_dense_17_loss: 118.4905 - val_dense_18_loss: 0.3138\n",
            "\n",
            "Epoch 00002: val_loss improved from 358.86932 to 250.37691, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 3/200\n",
            "42750/42750 [==============================] - 10s 238us/step - loss: 269.9413 - dense_16_loss: 0.3403 - dense_17_loss: 107.9633 - dense_18_loss: 0.2994 - val_loss: 239.8237 - val_dense_16_loss: 0.2577 - val_dense_17_loss: 109.8105 - val_dense_18_loss: 0.2635\n",
            "\n",
            "Epoch 00003: val_loss improved from 250.37691 to 239.82367, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 4/200\n",
            "42750/42750 [==============================] - 10s 238us/step - loss: 225.6468 - dense_16_loss: 0.2769 - dense_17_loss: 93.0749 - dense_18_loss: 0.2476 - val_loss: 214.1623 - val_dense_16_loss: 0.2658 - val_dense_17_loss: 93.1147 - val_dense_18_loss: 0.2066\n",
            "\n",
            "Epoch 00004: val_loss improved from 239.82367 to 214.16234, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 5/200\n",
            "42750/42750 [==============================] - 10s 238us/step - loss: 201.1423 - dense_16_loss: 0.2650 - dense_17_loss: 78.5726 - dense_18_loss: 0.2153 - val_loss: 169.1139 - val_dense_16_loss: 0.2139 - val_dense_17_loss: 75.9485 - val_dense_18_loss: 0.1450\n",
            "\n",
            "Epoch 00005: val_loss improved from 214.16234 to 169.11394, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 6/200\n",
            "42750/42750 [==============================] - 10s 238us/step - loss: 176.4003 - dense_16_loss: 0.2183 - dense_17_loss: 66.6717 - dense_18_loss: 0.2212 - val_loss: 151.7153 - val_dense_16_loss: 0.2131 - val_dense_17_loss: 63.8976 - val_dense_18_loss: 0.1195\n",
            "\n",
            "Epoch 00006: val_loss improved from 169.11394 to 151.71526, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 7/200\n",
            "42750/42750 [==============================] - 10s 240us/step - loss: 164.3810 - dense_16_loss: 0.2247 - dense_17_loss: 61.1083 - dense_18_loss: 0.1793 - val_loss: 145.9021 - val_dense_16_loss: 0.1862 - val_dense_17_loss: 55.7225 - val_dense_18_loss: 0.1717\n",
            "\n",
            "Epoch 00007: val_loss improved from 151.71526 to 145.90207, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 8/200\n",
            "42750/42750 [==============================] - 10s 240us/step - loss: 142.8622 - dense_16_loss: 0.1952 - dense_17_loss: 55.1926 - dense_18_loss: 0.1456 - val_loss: 129.5947 - val_dense_16_loss: 0.1761 - val_dense_17_loss: 50.1942 - val_dense_18_loss: 0.1329\n",
            "\n",
            "Epoch 00008: val_loss improved from 145.90207 to 129.59471, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 9/200\n",
            "42750/42750 [==============================] - 10s 239us/step - loss: 132.2029 - dense_16_loss: 0.1863 - dense_17_loss: 51.1234 - dense_18_loss: 0.1259 - val_loss: 116.6999 - val_dense_16_loss: 0.1597 - val_dense_17_loss: 48.0461 - val_dense_18_loss: 0.1037\n",
            "\n",
            "Epoch 00009: val_loss improved from 129.59471 to 116.69988, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 10/200\n",
            "42750/42750 [==============================] - 10s 241us/step - loss: 126.8110 - dense_16_loss: 0.1783 - dense_17_loss: 48.3725 - dense_18_loss: 0.1248 - val_loss: 119.8468 - val_dense_16_loss: 0.1488 - val_dense_17_loss: 47.2143 - val_dense_18_loss: 0.1399\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 116.69988\n",
            "Epoch 11/200\n",
            "42750/42750 [==============================] - 10s 240us/step - loss: 124.0104 - dense_16_loss: 0.1788 - dense_17_loss: 46.4259 - dense_18_loss: 0.1197 - val_loss: 125.7987 - val_dense_16_loss: 0.1775 - val_dense_17_loss: 47.5819 - val_dense_18_loss: 0.1248\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 116.69988\n",
            "Epoch 12/200\n",
            "42750/42750 [==============================] - 10s 240us/step - loss: 122.8125 - dense_16_loss: 0.1732 - dense_17_loss: 45.2283 - dense_18_loss: 0.1282 - val_loss: 126.6253 - val_dense_16_loss: 0.1557 - val_dense_17_loss: 47.5165 - val_dense_18_loss: 0.1620\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 116.69988\n",
            "Epoch 13/200\n",
            "42750/42750 [==============================] - 10s 240us/step - loss: 117.4225 - dense_16_loss: 0.1720 - dense_17_loss: 44.0488 - dense_18_loss: 0.1089 - val_loss: 116.4502 - val_dense_16_loss: 0.1800 - val_dense_17_loss: 46.4305 - val_dense_18_loss: 0.0801\n",
            "\n",
            "Epoch 00013: val_loss improved from 116.69988 to 116.45019, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 14/200\n",
            "42750/42750 [==============================] - 10s 241us/step - loss: 115.9210 - dense_16_loss: 0.1744 - dense_17_loss: 43.4154 - dense_18_loss: 0.1010 - val_loss: 115.2428 - val_dense_16_loss: 0.1821 - val_dense_17_loss: 46.0206 - val_dense_18_loss: 0.0730\n",
            "\n",
            "Epoch 00014: val_loss improved from 116.45019 to 115.24276, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 15/200\n",
            "42750/42750 [==============================] - 10s 241us/step - loss: 112.2889 - dense_16_loss: 0.1651 - dense_17_loss: 42.6034 - dense_18_loss: 0.1007 - val_loss: 133.0834 - val_dense_16_loss: 0.1797 - val_dense_17_loss: 44.5259 - val_dense_18_loss: 0.1732\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 115.24276\n",
            "Epoch 16/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 113.5482 - dense_16_loss: 0.1590 - dense_17_loss: 42.0489 - dense_18_loss: 0.1190 - val_loss: 126.7439 - val_dense_16_loss: 0.1624 - val_dense_17_loss: 45.1275 - val_dense_18_loss: 0.1645\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 115.24276\n",
            "Epoch 17/200\n",
            "42750/42750 [==============================] - 10s 241us/step - loss: 104.8159 - dense_16_loss: 0.1539 - dense_17_loss: 40.9706 - dense_18_loss: 0.0884 - val_loss: 97.4678 - val_dense_16_loss: 0.1447 - val_dense_17_loss: 42.0432 - val_dense_18_loss: 0.0601\n",
            "\n",
            "Epoch 00017: val_loss improved from 115.24276 to 97.46775, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 18/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 99.2771 - dense_16_loss: 0.1455 - dense_17_loss: 40.0136 - dense_18_loss: 0.0781 - val_loss: 93.8122 - val_dense_16_loss: 0.1506 - val_dense_17_loss: 40.4687 - val_dense_18_loss: 0.0409\n",
            "\n",
            "Epoch 00018: val_loss improved from 97.46775 to 93.81223, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 19/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 96.6913 - dense_16_loss: 0.1495 - dense_17_loss: 39.2691 - dense_18_loss: 0.0628 - val_loss: 102.7690 - val_dense_16_loss: 0.1759 - val_dense_17_loss: 39.4367 - val_dense_18_loss: 0.0529\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 93.81223\n",
            "Epoch 20/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 97.7700 - dense_16_loss: 0.1473 - dense_17_loss: 38.4792 - dense_18_loss: 0.0755 - val_loss: 109.2825 - val_dense_16_loss: 0.1463 - val_dense_17_loss: 38.1766 - val_dense_18_loss: 0.1361\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 93.81223\n",
            "Epoch 21/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 91.6723 - dense_16_loss: 0.1408 - dense_17_loss: 37.5723 - dense_18_loss: 0.0594 - val_loss: 84.4820 - val_dense_16_loss: 0.1254 - val_dense_17_loss: 37.4110 - val_dense_18_loss: 0.0473\n",
            "\n",
            "Epoch 00021: val_loss improved from 93.81223 to 84.48196, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 22/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 93.1055 - dense_16_loss: 0.1423 - dense_17_loss: 36.8571 - dense_18_loss: 0.0678 - val_loss: 100.5387 - val_dense_16_loss: 0.1597 - val_dense_17_loss: 36.3945 - val_dense_18_loss: 0.0812\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 84.48196\n",
            "Epoch 23/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 91.8493 - dense_16_loss: 0.1420 - dense_17_loss: 36.0778 - dense_18_loss: 0.0658 - val_loss: 95.3559 - val_dense_16_loss: 0.1352 - val_dense_17_loss: 35.5026 - val_dense_18_loss: 0.0964\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 84.48196\n",
            "Epoch 24/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 90.2597 - dense_16_loss: 0.1404 - dense_17_loss: 35.3204 - dense_18_loss: 0.0642 - val_loss: 90.6260 - val_dense_16_loss: 0.1456 - val_dense_17_loss: 34.5323 - val_dense_18_loss: 0.0620\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 84.48196\n",
            "Epoch 25/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 89.1395 - dense_16_loss: 0.1421 - dense_17_loss: 34.6458 - dense_18_loss: 0.0593 - val_loss: 83.9501 - val_dense_16_loss: 0.1311 - val_dense_17_loss: 33.8391 - val_dense_18_loss: 0.0539\n",
            "\n",
            "Epoch 00025: val_loss improved from 84.48196 to 83.95012, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 26/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 88.1030 - dense_16_loss: 0.1363 - dense_17_loss: 34.0138 - dense_18_loss: 0.0660 - val_loss: 78.5418 - val_dense_16_loss: 0.1256 - val_dense_17_loss: 33.5113 - val_dense_18_loss: 0.0368\n",
            "\n",
            "Epoch 00026: val_loss improved from 83.95012 to 78.54182, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 27/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 85.6764 - dense_16_loss: 0.1329 - dense_17_loss: 33.2960 - dense_18_loss: 0.0625 - val_loss: 79.9112 - val_dense_16_loss: 0.1227 - val_dense_17_loss: 32.4953 - val_dense_18_loss: 0.0530\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 78.54182\n",
            "Epoch 28/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 83.6646 - dense_16_loss: 0.1302 - dense_17_loss: 32.7807 - dense_18_loss: 0.0591 - val_loss: 76.8230 - val_dense_16_loss: 0.1260 - val_dense_17_loss: 32.5957 - val_dense_18_loss: 0.0321\n",
            "\n",
            "Epoch 00028: val_loss improved from 78.54182 to 76.82301, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 29/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 82.8046 - dense_16_loss: 0.1319 - dense_17_loss: 32.1084 - dense_18_loss: 0.0557 - val_loss: 77.2127 - val_dense_16_loss: 0.1199 - val_dense_17_loss: 31.6954 - val_dense_18_loss: 0.0478\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 76.82301\n",
            "Epoch 30/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 86.6293 - dense_16_loss: 0.1297 - dense_17_loss: 31.7478 - dense_18_loss: 0.0799 - val_loss: 81.8490 - val_dense_16_loss: 0.1313 - val_dense_17_loss: 31.7282 - val_dense_18_loss: 0.0537\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 76.82301\n",
            "Epoch 31/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 84.6077 - dense_16_loss: 0.1296 - dense_17_loss: 31.4564 - dense_18_loss: 0.0713 - val_loss: 79.0824 - val_dense_16_loss: 0.1179 - val_dense_17_loss: 30.5532 - val_dense_18_loss: 0.0659\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 76.82301\n",
            "Epoch 32/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 83.1049 - dense_16_loss: 0.1301 - dense_17_loss: 31.1644 - dense_18_loss: 0.0646 - val_loss: 77.7867 - val_dense_16_loss: 0.1178 - val_dense_17_loss: 30.9152 - val_dense_18_loss: 0.0576\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 76.82301\n",
            "Epoch 33/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 80.0936 - dense_16_loss: 0.1269 - dense_17_loss: 30.7046 - dense_18_loss: 0.0566 - val_loss: 74.8872 - val_dense_16_loss: 0.1234 - val_dense_17_loss: 29.9566 - val_dense_18_loss: 0.0396\n",
            "\n",
            "Epoch 00033: val_loss improved from 76.82301 to 74.88716, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 34/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 77.2127 - dense_16_loss: 0.1254 - dense_17_loss: 30.1365 - dense_18_loss: 0.0472 - val_loss: 75.1452 - val_dense_16_loss: 0.1228 - val_dense_17_loss: 29.6642 - val_dense_18_loss: 0.0433\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 74.88716\n",
            "Epoch 35/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 78.3989 - dense_16_loss: 0.1296 - dense_17_loss: 29.8868 - dense_18_loss: 0.0482 - val_loss: 80.4235 - val_dense_16_loss: 0.1236 - val_dense_17_loss: 30.3708 - val_dense_18_loss: 0.0649\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 74.88716\n",
            "Epoch 36/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 79.1507 - dense_16_loss: 0.1274 - dense_17_loss: 29.7070 - dense_18_loss: 0.0562 - val_loss: 88.8973 - val_dense_16_loss: 0.1190 - val_dense_17_loss: 32.3293 - val_dense_18_loss: 0.1044\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 74.88716\n",
            "Epoch 37/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 77.9798 - dense_16_loss: 0.1298 - dense_17_loss: 29.3572 - dense_18_loss: 0.0484 - val_loss: 77.2575 - val_dense_16_loss: 0.1293 - val_dense_17_loss: 29.3390 - val_dense_18_loss: 0.0456\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 74.88716\n",
            "Epoch 38/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 77.6147 - dense_16_loss: 0.1278 - dense_17_loss: 29.2204 - dense_18_loss: 0.0503 - val_loss: 73.1009 - val_dense_16_loss: 0.1260 - val_dense_17_loss: 29.5162 - val_dense_18_loss: 0.0289\n",
            "\n",
            "Epoch 00038: val_loss improved from 74.88716 to 73.10089, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 39/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 76.3448 - dense_16_loss: 0.1259 - dense_17_loss: 29.0031 - dense_18_loss: 0.0478 - val_loss: 77.6384 - val_dense_16_loss: 0.1170 - val_dense_17_loss: 27.6357 - val_dense_18_loss: 0.0745\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 73.10089\n",
            "Epoch 40/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 76.0718 - dense_16_loss: 0.1257 - dense_17_loss: 28.7052 - dense_18_loss: 0.0483 - val_loss: 83.0575 - val_dense_16_loss: 0.1281 - val_dense_17_loss: 32.1848 - val_dense_18_loss: 0.0622\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 73.10089\n",
            "Epoch 41/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 74.7934 - dense_16_loss: 0.1233 - dense_17_loss: 28.4010 - dense_18_loss: 0.0470 - val_loss: 67.7307 - val_dense_16_loss: 0.1133 - val_dense_17_loss: 27.8001 - val_dense_18_loss: 0.0297\n",
            "\n",
            "Epoch 00041: val_loss improved from 73.10089 to 67.73066, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 42/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 72.6601 - dense_16_loss: 0.1219 - dense_17_loss: 27.9732 - dense_18_loss: 0.0406 - val_loss: 67.1142 - val_dense_16_loss: 0.1119 - val_dense_17_loss: 27.6703 - val_dense_18_loss: 0.0294\n",
            "\n",
            "Epoch 00042: val_loss improved from 67.73066 to 67.11425, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 43/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 74.7759 - dense_16_loss: 0.1236 - dense_17_loss: 27.7083 - dense_18_loss: 0.0499 - val_loss: 71.9197 - val_dense_16_loss: 0.1208 - val_dense_17_loss: 26.8318 - val_dense_18_loss: 0.0443\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 67.11425\n",
            "Epoch 44/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 73.0344 - dense_16_loss: 0.1236 - dense_17_loss: 27.2553 - dense_18_loss: 0.0436 - val_loss: 73.8717 - val_dense_16_loss: 0.1247 - val_dense_17_loss: 26.4571 - val_dense_18_loss: 0.0500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 67.11425\n",
            "Epoch 45/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 71.0808 - dense_16_loss: 0.1214 - dense_17_loss: 26.8373 - dense_18_loss: 0.0392 - val_loss: 75.3943 - val_dense_16_loss: 0.1306 - val_dense_17_loss: 27.3165 - val_dense_18_loss: 0.0445\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 67.11425\n",
            "Epoch 46/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 73.3206 - dense_16_loss: 0.1218 - dense_17_loss: 26.9249 - dense_18_loss: 0.0493 - val_loss: 86.2169 - val_dense_16_loss: 0.1477 - val_dense_17_loss: 31.0344 - val_dense_18_loss: 0.0544\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 67.11425\n",
            "Epoch 47/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 71.5658 - dense_16_loss: 0.1200 - dense_17_loss: 26.7807 - dense_18_loss: 0.0440 - val_loss: 71.5297 - val_dense_16_loss: 0.1160 - val_dense_17_loss: 25.0190 - val_dense_18_loss: 0.0585\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 67.11425\n",
            "Epoch 48/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 71.7615 - dense_16_loss: 0.1206 - dense_17_loss: 26.4274 - dense_18_loss: 0.0458 - val_loss: 69.6195 - val_dense_16_loss: 0.1257 - val_dense_17_loss: 25.2932 - val_dense_18_loss: 0.0331\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 67.11425\n",
            "Epoch 49/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 70.5011 - dense_16_loss: 0.1200 - dense_17_loss: 26.2436 - dense_18_loss: 0.0413 - val_loss: 64.7136 - val_dense_16_loss: 0.1161 - val_dense_17_loss: 25.5835 - val_dense_18_loss: 0.0215\n",
            "\n",
            "Epoch 00049: val_loss improved from 67.11425 to 64.71360, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 50/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 71.3244 - dense_16_loss: 0.1178 - dense_17_loss: 26.2201 - dense_18_loss: 0.0488 - val_loss: 68.6248 - val_dense_16_loss: 0.1114 - val_dense_17_loss: 24.6632 - val_dense_18_loss: 0.0527\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 64.71360\n",
            "Epoch 51/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 70.5826 - dense_16_loss: 0.1185 - dense_17_loss: 26.0344 - dense_18_loss: 0.0449 - val_loss: 64.0933 - val_dense_16_loss: 0.1069 - val_dense_17_loss: 24.4674 - val_dense_18_loss: 0.0378\n",
            "\n",
            "Epoch 00051: val_loss improved from 64.71360 to 64.09334, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 52/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 68.3451 - dense_16_loss: 0.1161 - dense_17_loss: 25.9071 - dense_18_loss: 0.0381 - val_loss: 65.8312 - val_dense_16_loss: 0.1157 - val_dense_17_loss: 24.8377 - val_dense_18_loss: 0.0314\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 64.09334\n",
            "Epoch 53/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 67.0113 - dense_16_loss: 0.1145 - dense_17_loss: 25.5212 - dense_18_loss: 0.0358 - val_loss: 66.8155 - val_dense_16_loss: 0.1161 - val_dense_17_loss: 25.8836 - val_dense_18_loss: 0.0306\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 64.09334\n",
            "Epoch 54/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 69.6050 - dense_16_loss: 0.1182 - dense_17_loss: 25.4176 - dense_18_loss: 0.0436 - val_loss: 66.4334 - val_dense_16_loss: 0.1194 - val_dense_17_loss: 24.1174 - val_dense_18_loss: 0.0325\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 64.09334\n",
            "Epoch 55/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 67.7780 - dense_16_loss: 0.1177 - dense_17_loss: 25.4302 - dense_18_loss: 0.0352 - val_loss: 68.0107 - val_dense_16_loss: 0.1173 - val_dense_17_loss: 24.2998 - val_dense_18_loss: 0.0426\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 64.09334\n",
            "Epoch 56/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 67.6843 - dense_16_loss: 0.1157 - dense_17_loss: 25.3199 - dense_18_loss: 0.0382 - val_loss: 62.9339 - val_dense_16_loss: 0.1105 - val_dense_17_loss: 24.2466 - val_dense_18_loss: 0.0276\n",
            "\n",
            "Epoch 00056: val_loss improved from 64.09334 to 62.93386, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 57/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 66.3478 - dense_16_loss: 0.1140 - dense_17_loss: 24.8832 - dense_18_loss: 0.0363 - val_loss: 64.5618 - val_dense_16_loss: 0.1113 - val_dense_17_loss: 23.2637 - val_dense_18_loss: 0.0395\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 62.93386\n",
            "Epoch 58/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 67.0490 - dense_16_loss: 0.1147 - dense_17_loss: 24.9143 - dense_18_loss: 0.0386 - val_loss: 65.5434 - val_dense_16_loss: 0.1205 - val_dense_17_loss: 23.4996 - val_dense_18_loss: 0.0295\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 62.93386\n",
            "Epoch 59/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 66.0528 - dense_16_loss: 0.1127 - dense_17_loss: 24.7084 - dense_18_loss: 0.0377 - val_loss: 64.7765 - val_dense_16_loss: 0.1059 - val_dense_17_loss: 24.6071 - val_dense_18_loss: 0.0420\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 62.93386\n",
            "Epoch 60/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 67.8341 - dense_16_loss: 0.1143 - dense_17_loss: 24.9431 - dense_18_loss: 0.0430 - val_loss: 64.2618 - val_dense_16_loss: 0.1134 - val_dense_17_loss: 24.8884 - val_dense_18_loss: 0.0268\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 62.93386\n",
            "Epoch 61/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 65.8939 - dense_16_loss: 0.1114 - dense_17_loss: 24.5892 - dense_18_loss: 0.0394 - val_loss: 77.1369 - val_dense_16_loss: 0.1209 - val_dense_17_loss: 25.3518 - val_dense_18_loss: 0.0776\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 62.93386\n",
            "Epoch 62/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 67.4674 - dense_16_loss: 0.1125 - dense_17_loss: 24.4978 - dense_18_loss: 0.0461 - val_loss: 66.8275 - val_dense_16_loss: 0.1121 - val_dense_17_loss: 24.9421 - val_dense_18_loss: 0.0413\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 62.93386\n",
            "Epoch 63/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 65.8732 - dense_16_loss: 0.1139 - dense_17_loss: 24.2714 - dense_18_loss: 0.0372 - val_loss: 67.9389 - val_dense_16_loss: 0.1182 - val_dense_17_loss: 24.2617 - val_dense_18_loss: 0.0411\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 62.93386\n",
            "Epoch 64/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 63.7788 - dense_16_loss: 0.1097 - dense_17_loss: 24.3156 - dense_18_loss: 0.0328 - val_loss: 60.7818 - val_dense_16_loss: 0.1086 - val_dense_17_loss: 23.0201 - val_dense_18_loss: 0.0260\n",
            "\n",
            "Epoch 00064: val_loss improved from 62.93386 to 60.78182, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 65/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 63.2298 - dense_16_loss: 0.1079 - dense_17_loss: 23.8710 - dense_18_loss: 0.0349 - val_loss: 67.9082 - val_dense_16_loss: 0.1070 - val_dense_17_loss: 24.4001 - val_dense_18_loss: 0.0570\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 60.78182\n",
            "Epoch 66/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 61.9077 - dense_16_loss: 0.1083 - dense_17_loss: 23.8319 - dense_18_loss: 0.0279 - val_loss: 62.0365 - val_dense_16_loss: 0.1030 - val_dense_17_loss: 24.1659 - val_dense_18_loss: 0.0348\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 60.78182\n",
            "Epoch 67/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 63.6298 - dense_16_loss: 0.1084 - dense_17_loss: 24.0325 - dense_18_loss: 0.0354 - val_loss: 62.5887 - val_dense_16_loss: 0.1131 - val_dense_17_loss: 22.4335 - val_dense_18_loss: 0.0312\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 60.78182\n",
            "Epoch 68/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 65.6245 - dense_16_loss: 0.1112 - dense_17_loss: 24.0133 - dense_18_loss: 0.0413 - val_loss: 58.7562 - val_dense_16_loss: 0.1093 - val_dense_17_loss: 22.3171 - val_dense_18_loss: 0.0183\n",
            "\n",
            "Epoch 00068: val_loss improved from 60.78182 to 58.75625, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 69/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 66.1495 - dense_16_loss: 0.1126 - dense_17_loss: 24.1693 - dense_18_loss: 0.0410 - val_loss: 73.3888 - val_dense_16_loss: 0.1188 - val_dense_17_loss: 24.2169 - val_dense_18_loss: 0.0677\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 58.75625\n",
            "Epoch 70/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 64.2907 - dense_16_loss: 0.1110 - dense_17_loss: 24.0627 - dense_18_loss: 0.0347 - val_loss: 59.7826 - val_dense_16_loss: 0.1068 - val_dense_17_loss: 22.7579 - val_dense_18_loss: 0.0250\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 58.75625\n",
            "Epoch 71/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 64.3713 - dense_16_loss: 0.1123 - dense_17_loss: 24.0151 - dense_18_loss: 0.0333 - val_loss: 64.8819 - val_dense_16_loss: 0.1102 - val_dense_17_loss: 22.5771 - val_dense_18_loss: 0.0462\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 58.75625\n",
            "Epoch 72/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 63.5384 - dense_16_loss: 0.1095 - dense_17_loss: 23.7941 - dense_18_loss: 0.0344 - val_loss: 70.3343 - val_dense_16_loss: 0.1156 - val_dense_17_loss: 26.1538 - val_dense_18_loss: 0.0475\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 58.75625\n",
            "Epoch 73/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 64.9481 - dense_16_loss: 0.1098 - dense_17_loss: 23.6880 - dense_18_loss: 0.0417 - val_loss: 63.3574 - val_dense_16_loss: 0.1119 - val_dense_17_loss: 22.2948 - val_dense_18_loss: 0.0375\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 58.75625\n",
            "Epoch 74/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 62.8741 - dense_16_loss: 0.1098 - dense_17_loss: 23.5717 - dense_18_loss: 0.0318 - val_loss: 62.1495 - val_dense_16_loss: 0.1064 - val_dense_17_loss: 22.7598 - val_dense_18_loss: 0.0374\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 58.75625\n",
            "Epoch 75/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 64.1046 - dense_16_loss: 0.1110 - dense_17_loss: 23.5480 - dense_18_loss: 0.0363 - val_loss: 65.4928 - val_dense_16_loss: 0.1049 - val_dense_17_loss: 22.7447 - val_dense_18_loss: 0.0564\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 58.75625\n",
            "Epoch 76/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 63.1526 - dense_16_loss: 0.1091 - dense_17_loss: 23.5078 - dense_18_loss: 0.0345 - val_loss: 64.9968 - val_dense_16_loss: 0.1052 - val_dense_17_loss: 25.1878 - val_dense_18_loss: 0.0412\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 58.75625\n",
            "Epoch 77/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 63.4975 - dense_16_loss: 0.1081 - dense_17_loss: 23.5618 - dense_18_loss: 0.0376 - val_loss: 59.8183 - val_dense_16_loss: 0.1040 - val_dense_17_loss: 24.5146 - val_dense_18_loss: 0.0205\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 58.75625\n",
            "Epoch 78/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 61.8959 - dense_16_loss: 0.1099 - dense_17_loss: 23.2206 - dense_18_loss: 0.0285 - val_loss: 68.6789 - val_dense_16_loss: 0.1108 - val_dense_17_loss: 26.2383 - val_dense_18_loss: 0.0460\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 58.75625\n",
            "Epoch 79/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 63.0176 - dense_16_loss: 0.1083 - dense_17_loss: 23.4498 - dense_18_loss: 0.0354 - val_loss: 64.4053 - val_dense_16_loss: 0.1052 - val_dense_17_loss: 21.6263 - val_dense_18_loss: 0.0562\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 58.75625\n",
            "Epoch 80/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 63.2729 - dense_16_loss: 0.1073 - dense_17_loss: 23.7497 - dense_18_loss: 0.0366 - val_loss: 56.2975 - val_dense_16_loss: 0.1011 - val_dense_17_loss: 21.9272 - val_dense_18_loss: 0.0202\n",
            "\n",
            "Epoch 00080: val_loss improved from 58.75625 to 56.29745, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 81/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 62.3698 - dense_16_loss: 0.1069 - dense_17_loss: 23.1353 - dense_18_loss: 0.0358 - val_loss: 57.3387 - val_dense_16_loss: 0.1073 - val_dense_17_loss: 22.0541 - val_dense_18_loss: 0.0155\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 56.29745\n",
            "Epoch 82/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 60.1023 - dense_16_loss: 0.1073 - dense_17_loss: 23.0083 - dense_18_loss: 0.0246 - val_loss: 58.1732 - val_dense_16_loss: 0.1047 - val_dense_17_loss: 21.3072 - val_dense_18_loss: 0.0272\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 56.29745\n",
            "Epoch 83/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 60.7414 - dense_16_loss: 0.1075 - dense_17_loss: 22.8216 - dense_18_loss: 0.0284 - val_loss: 58.7241 - val_dense_16_loss: 0.1090 - val_dense_17_loss: 21.1918 - val_dense_18_loss: 0.0241\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 56.29745\n",
            "Epoch 84/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 60.8839 - dense_16_loss: 0.1066 - dense_17_loss: 22.9390 - dense_18_loss: 0.0298 - val_loss: 58.8262 - val_dense_16_loss: 0.1057 - val_dense_17_loss: 21.3305 - val_dense_18_loss: 0.0289\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 56.29745\n",
            "Epoch 85/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 61.5016 - dense_16_loss: 0.1061 - dense_17_loss: 23.0170 - dense_18_loss: 0.0333 - val_loss: 59.4899 - val_dense_16_loss: 0.1019 - val_dense_17_loss: 21.6318 - val_dense_18_loss: 0.0364\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 56.29745\n",
            "Epoch 86/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 60.4601 - dense_16_loss: 0.1055 - dense_17_loss: 22.8852 - dense_18_loss: 0.0296 - val_loss: 54.1632 - val_dense_16_loss: 0.1009 - val_dense_17_loss: 21.3793 - val_dense_18_loss: 0.0125\n",
            "\n",
            "Epoch 00086: val_loss improved from 56.29745 to 54.16324, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 87/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 60.8369 - dense_16_loss: 0.1069 - dense_17_loss: 22.9055 - dense_18_loss: 0.0294 - val_loss: 59.9037 - val_dense_16_loss: 0.1019 - val_dense_17_loss: 20.8656 - val_dense_18_loss: 0.0423\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 54.16324\n",
            "Epoch 88/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 59.9043 - dense_16_loss: 0.1026 - dense_17_loss: 22.7236 - dense_18_loss: 0.0320 - val_loss: 59.3703 - val_dense_16_loss: 0.1060 - val_dense_17_loss: 22.6109 - val_dense_18_loss: 0.0249\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 54.16324\n",
            "Epoch 89/200\n",
            "42750/42750 [==============================] - 11s 246us/step - loss: 59.7133 - dense_16_loss: 0.1033 - dense_17_loss: 22.4098 - dense_18_loss: 0.0316 - val_loss: 60.9763 - val_dense_16_loss: 0.1175 - val_dense_17_loss: 21.6114 - val_dense_18_loss: 0.0206\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 54.16324\n",
            "Epoch 90/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 59.3759 - dense_16_loss: 0.1049 - dense_17_loss: 22.3413 - dense_18_loss: 0.0278 - val_loss: 63.4034 - val_dense_16_loss: 0.1173 - val_dense_17_loss: 22.2895 - val_dense_18_loss: 0.0296\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 54.16324\n",
            "Epoch 91/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 59.6452 - dense_16_loss: 0.1041 - dense_17_loss: 22.5845 - dense_18_loss: 0.0291 - val_loss: 55.8383 - val_dense_16_loss: 0.1013 - val_dense_17_loss: 21.6710 - val_dense_18_loss: 0.0189\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 54.16324\n",
            "Epoch 92/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 59.2927 - dense_16_loss: 0.1049 - dense_17_loss: 22.3848 - dense_18_loss: 0.0272 - val_loss: 61.5843 - val_dense_16_loss: 0.1074 - val_dense_17_loss: 21.0961 - val_dense_18_loss: 0.0413\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 54.16324\n",
            "Epoch 93/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 59.4587 - dense_16_loss: 0.1063 - dense_17_loss: 22.2039 - dense_18_loss: 0.0269 - val_loss: 57.0826 - val_dense_16_loss: 0.1059 - val_dense_17_loss: 20.6723 - val_dense_18_loss: 0.0232\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 54.16324\n",
            "Epoch 94/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 59.9378 - dense_16_loss: 0.1041 - dense_17_loss: 22.2408 - dense_18_loss: 0.0323 - val_loss: 58.5568 - val_dense_16_loss: 0.1027 - val_dense_17_loss: 21.1074 - val_dense_18_loss: 0.0332\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 54.16324\n",
            "Epoch 95/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 58.9536 - dense_16_loss: 0.1027 - dense_17_loss: 22.3149 - dense_18_loss: 0.0291 - val_loss: 57.5590 - val_dense_16_loss: 0.1091 - val_dense_17_loss: 22.0254 - val_dense_18_loss: 0.0140\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 54.16324\n",
            "Epoch 96/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 59.2906 - dense_16_loss: 0.1059 - dense_17_loss: 22.3536 - dense_18_loss: 0.0258 - val_loss: 56.6968 - val_dense_16_loss: 0.1037 - val_dense_17_loss: 21.9000 - val_dense_18_loss: 0.0185\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 54.16324\n",
            "Epoch 97/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 59.2512 - dense_16_loss: 0.1057 - dense_17_loss: 22.1515 - dense_18_loss: 0.0269 - val_loss: 58.5541 - val_dense_16_loss: 0.1075 - val_dense_17_loss: 20.2452 - val_dense_18_loss: 0.0303\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 54.16324\n",
            "Epoch 98/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 59.4258 - dense_16_loss: 0.1037 - dense_17_loss: 22.1638 - dense_18_loss: 0.0307 - val_loss: 56.0496 - val_dense_16_loss: 0.1044 - val_dense_17_loss: 20.9061 - val_dense_18_loss: 0.0192\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 54.16324\n",
            "Epoch 99/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 58.4823 - dense_16_loss: 0.1028 - dense_17_loss: 22.0845 - dense_18_loss: 0.0277 - val_loss: 58.7612 - val_dense_16_loss: 0.1096 - val_dense_17_loss: 21.6903 - val_dense_18_loss: 0.0210\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 54.16324\n",
            "Epoch 100/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 57.4723 - dense_16_loss: 0.1018 - dense_17_loss: 21.9748 - dense_18_loss: 0.0248 - val_loss: 58.3760 - val_dense_16_loss: 0.1020 - val_dense_17_loss: 23.0286 - val_dense_18_loss: 0.0238\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 54.16324\n",
            "Epoch 101/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 58.1324 - dense_16_loss: 0.1000 - dense_17_loss: 22.0815 - dense_18_loss: 0.0302 - val_loss: 56.4225 - val_dense_16_loss: 0.1051 - val_dense_17_loss: 20.6511 - val_dense_18_loss: 0.0212\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 54.16324\n",
            "Epoch 102/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 57.1090 - dense_16_loss: 0.1017 - dense_17_loss: 21.8631 - dense_18_loss: 0.0237 - val_loss: 56.7721 - val_dense_16_loss: 0.1026 - val_dense_17_loss: 21.1473 - val_dense_18_loss: 0.0242\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 54.16324\n",
            "Epoch 103/200\n",
            "42750/42750 [==============================] - 11s 246us/step - loss: 58.1120 - dense_16_loss: 0.1029 - dense_17_loss: 21.8288 - dense_18_loss: 0.0271 - val_loss: 58.3699 - val_dense_16_loss: 0.1021 - val_dense_17_loss: 23.5862 - val_dense_18_loss: 0.0207\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 54.16324\n",
            "Epoch 104/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 57.9558 - dense_16_loss: 0.1040 - dense_17_loss: 21.7921 - dense_18_loss: 0.0248 - val_loss: 59.3682 - val_dense_16_loss: 0.1051 - val_dense_17_loss: 22.1992 - val_dense_18_loss: 0.0282\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 54.16324\n",
            "Epoch 105/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 57.9422 - dense_16_loss: 0.1017 - dense_17_loss: 22.2492 - dense_18_loss: 0.0260 - val_loss: 64.0064 - val_dense_16_loss: 0.1057 - val_dense_17_loss: 21.2324 - val_dense_18_loss: 0.0553\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 54.16324\n",
            "Epoch 106/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 59.5383 - dense_16_loss: 0.1050 - dense_17_loss: 21.8663 - dense_18_loss: 0.0308 - val_loss: 57.9273 - val_dense_16_loss: 0.1026 - val_dense_17_loss: 21.5482 - val_dense_18_loss: 0.0280\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 54.16324\n",
            "Epoch 107/200\n",
            "42750/42750 [==============================] - 10s 246us/step - loss: 56.1585 - dense_16_loss: 0.1004 - dense_17_loss: 21.6540 - dense_18_loss: 0.0220 - val_loss: 60.7849 - val_dense_16_loss: 0.1024 - val_dense_17_loss: 22.4132 - val_dense_18_loss: 0.0382\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 54.16324\n",
            "Epoch 108/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 57.5876 - dense_16_loss: 0.1000 - dense_17_loss: 21.9081 - dense_18_loss: 0.0285 - val_loss: 58.8474 - val_dense_16_loss: 0.1084 - val_dense_17_loss: 20.1712 - val_dense_18_loss: 0.0307\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 54.16324\n",
            "Epoch 109/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 56.2316 - dense_16_loss: 0.1015 - dense_17_loss: 21.3768 - dense_18_loss: 0.0220 - val_loss: 54.5954 - val_dense_16_loss: 0.1024 - val_dense_17_loss: 20.4486 - val_dense_18_loss: 0.0171\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 54.16324\n",
            "Epoch 110/200\n",
            "42750/42750 [==============================] - 10s 246us/step - loss: 56.7319 - dense_16_loss: 0.1017 - dense_17_loss: 21.5259 - dense_18_loss: 0.0235 - val_loss: 53.7176 - val_dense_16_loss: 0.1022 - val_dense_17_loss: 19.9401 - val_dense_18_loss: 0.0155\n",
            "\n",
            "Epoch 00110: val_loss improved from 54.16324 to 53.71762, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 111/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 57.2849 - dense_16_loss: 0.1007 - dense_17_loss: 21.9237 - dense_18_loss: 0.0257 - val_loss: 57.0715 - val_dense_16_loss: 0.1042 - val_dense_17_loss: 21.1300 - val_dense_18_loss: 0.0235\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 53.71762\n",
            "Epoch 112/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 56.0420 - dense_16_loss: 0.1015 - dense_17_loss: 21.5237 - dense_18_loss: 0.0204 - val_loss: 61.0027 - val_dense_16_loss: 0.1114 - val_dense_17_loss: 21.3451 - val_dense_18_loss: 0.0312\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 53.71762\n",
            "Epoch 113/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 56.7845 - dense_16_loss: 0.0994 - dense_17_loss: 21.6339 - dense_18_loss: 0.0267 - val_loss: 56.2930 - val_dense_16_loss: 0.1079 - val_dense_17_loss: 20.4056 - val_dense_18_loss: 0.0177\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 53.71762\n",
            "Epoch 114/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 56.6651 - dense_16_loss: 0.0993 - dense_17_loss: 21.2455 - dense_18_loss: 0.0281 - val_loss: 59.5532 - val_dense_16_loss: 0.1027 - val_dense_17_loss: 19.9475 - val_dense_18_loss: 0.0440\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 53.71762\n",
            "Epoch 115/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 57.5197 - dense_16_loss: 0.1008 - dense_17_loss: 21.4762 - dense_18_loss: 0.0291 - val_loss: 55.3466 - val_dense_16_loss: 0.1070 - val_dense_17_loss: 20.3232 - val_dense_18_loss: 0.0146\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 53.71762\n",
            "Epoch 116/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 55.2280 - dense_16_loss: 0.0986 - dense_17_loss: 21.2067 - dense_18_loss: 0.0222 - val_loss: 54.1013 - val_dense_16_loss: 0.0991 - val_dense_17_loss: 20.1555 - val_dense_18_loss: 0.0210\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 53.71762\n",
            "Epoch 117/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 57.6569 - dense_16_loss: 0.0998 - dense_17_loss: 21.2927 - dense_18_loss: 0.0321 - val_loss: 55.9900 - val_dense_16_loss: 0.1038 - val_dense_17_loss: 21.2377 - val_dense_18_loss: 0.0180\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 53.71762\n",
            "Epoch 118/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 55.9638 - dense_16_loss: 0.1009 - dense_17_loss: 21.3062 - dense_18_loss: 0.0219 - val_loss: 57.6009 - val_dense_16_loss: 0.1135 - val_dense_17_loss: 19.9989 - val_dense_18_loss: 0.0178\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 53.71762\n",
            "Epoch 119/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 55.8436 - dense_16_loss: 0.1000 - dense_17_loss: 21.5265 - dense_18_loss: 0.0216 - val_loss: 57.1421 - val_dense_16_loss: 0.1047 - val_dense_17_loss: 19.8474 - val_dense_18_loss: 0.0294\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 53.71762\n",
            "Epoch 120/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 56.1180 - dense_16_loss: 0.0993 - dense_17_loss: 21.3338 - dense_18_loss: 0.0250 - val_loss: 56.8201 - val_dense_16_loss: 0.1032 - val_dense_17_loss: 21.2196 - val_dense_18_loss: 0.0232\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 53.71762\n",
            "Epoch 121/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 55.7538 - dense_16_loss: 0.0988 - dense_17_loss: 21.2947 - dense_18_loss: 0.0240 - val_loss: 55.5539 - val_dense_16_loss: 0.1032 - val_dense_17_loss: 20.7066 - val_dense_18_loss: 0.0195\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 53.71762\n",
            "Epoch 122/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 55.4126 - dense_16_loss: 0.0978 - dense_17_loss: 21.1215 - dense_18_loss: 0.0248 - val_loss: 56.4565 - val_dense_16_loss: 0.1032 - val_dense_17_loss: 20.3332 - val_dense_18_loss: 0.0258\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 53.71762\n",
            "Epoch 123/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 54.5458 - dense_16_loss: 0.0969 - dense_17_loss: 21.3931 - dense_18_loss: 0.0205 - val_loss: 56.4050 - val_dense_16_loss: 0.1008 - val_dense_17_loss: 20.8501 - val_dense_18_loss: 0.0265\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 53.71762\n",
            "Epoch 124/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 56.1610 - dense_16_loss: 0.1002 - dense_17_loss: 21.1486 - dense_18_loss: 0.0248 - val_loss: 54.9150 - val_dense_16_loss: 0.1008 - val_dense_17_loss: 20.3014 - val_dense_18_loss: 0.0219\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 53.71762\n",
            "Epoch 125/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 54.6103 - dense_16_loss: 0.0961 - dense_17_loss: 20.9068 - dense_18_loss: 0.0243 - val_loss: 52.3344 - val_dense_16_loss: 0.0996 - val_dense_17_loss: 19.6852 - val_dense_18_loss: 0.0139\n",
            "\n",
            "Epoch 00125: val_loss improved from 53.71762 to 52.33438, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 126/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 55.1068 - dense_16_loss: 0.0979 - dense_17_loss: 20.9777 - dense_18_loss: 0.0238 - val_loss: 56.1474 - val_dense_16_loss: 0.1031 - val_dense_17_loss: 21.2002 - val_dense_18_loss: 0.0201\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 52.33438\n",
            "Epoch 127/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 54.2097 - dense_16_loss: 0.0973 - dense_17_loss: 20.8667 - dense_18_loss: 0.0207 - val_loss: 59.0294 - val_dense_16_loss: 0.1089 - val_dense_17_loss: 22.8808 - val_dense_18_loss: 0.0174\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 52.33438\n",
            "Epoch 128/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 54.5694 - dense_16_loss: 0.0968 - dense_17_loss: 21.0462 - dense_18_loss: 0.0225 - val_loss: 53.4881 - val_dense_16_loss: 0.1008 - val_dense_17_loss: 19.8411 - val_dense_18_loss: 0.0170\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 52.33438\n",
            "Epoch 129/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 54.7089 - dense_16_loss: 0.0975 - dense_17_loss: 20.8769 - dense_18_loss: 0.0229 - val_loss: 54.7319 - val_dense_16_loss: 0.1025 - val_dense_17_loss: 20.0431 - val_dense_18_loss: 0.0197\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 52.33438\n",
            "Epoch 130/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 53.7518 - dense_16_loss: 0.0969 - dense_17_loss: 20.7234 - dense_18_loss: 0.0198 - val_loss: 61.7779 - val_dense_16_loss: 0.1101 - val_dense_17_loss: 21.9276 - val_dense_18_loss: 0.0341\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 52.33438\n",
            "Epoch 131/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 54.2352 - dense_16_loss: 0.0965 - dense_17_loss: 20.7647 - dense_18_loss: 0.0227 - val_loss: 53.8331 - val_dense_16_loss: 0.0993 - val_dense_17_loss: 20.4578 - val_dense_18_loss: 0.0179\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 52.33438\n",
            "Epoch 132/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 53.7893 - dense_16_loss: 0.0959 - dense_17_loss: 20.7981 - dense_18_loss: 0.0210 - val_loss: 56.0913 - val_dense_16_loss: 0.1032 - val_dense_17_loss: 20.2161 - val_dense_18_loss: 0.0246\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 52.33438\n",
            "Epoch 133/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 53.8791 - dense_16_loss: 0.0959 - dense_17_loss: 20.8946 - dense_18_loss: 0.0210 - val_loss: 57.5845 - val_dense_16_loss: 0.1054 - val_dense_17_loss: 21.7932 - val_dense_18_loss: 0.0209\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 52.33438\n",
            "Epoch 134/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 54.8368 - dense_16_loss: 0.0982 - dense_17_loss: 21.3152 - dense_18_loss: 0.0204 - val_loss: 55.1249 - val_dense_16_loss: 0.1037 - val_dense_17_loss: 20.4388 - val_dense_18_loss: 0.0178\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 52.33438\n",
            "Epoch 135/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 53.6280 - dense_16_loss: 0.0953 - dense_17_loss: 20.7073 - dense_18_loss: 0.0217 - val_loss: 66.9026 - val_dense_16_loss: 0.1070 - val_dense_17_loss: 28.5710 - val_dense_18_loss: 0.0312\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 52.33438\n",
            "Epoch 136/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 53.7281 - dense_16_loss: 0.0967 - dense_17_loss: 20.6628 - dense_18_loss: 0.0202 - val_loss: 53.0113 - val_dense_16_loss: 0.1002 - val_dense_17_loss: 19.2664 - val_dense_18_loss: 0.0184\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 52.33438\n",
            "Epoch 137/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 52.7865 - dense_16_loss: 0.0955 - dense_17_loss: 20.5532 - dense_18_loss: 0.0180 - val_loss: 53.1179 - val_dense_16_loss: 0.1022 - val_dense_17_loss: 19.6911 - val_dense_18_loss: 0.0138\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 52.33438\n",
            "Epoch 138/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 53.7374 - dense_16_loss: 0.0951 - dense_17_loss: 20.8569 - dense_18_loss: 0.0218 - val_loss: 56.1812 - val_dense_16_loss: 0.1028 - val_dense_17_loss: 20.1663 - val_dense_18_loss: 0.0259\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 52.33438\n",
            "Epoch 139/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 54.0983 - dense_16_loss: 0.0957 - dense_17_loss: 20.7456 - dense_18_loss: 0.0233 - val_loss: 54.3090 - val_dense_16_loss: 0.1027 - val_dense_17_loss: 19.2684 - val_dense_18_loss: 0.0212\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 52.33438\n",
            "Epoch 140/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 52.7349 - dense_16_loss: 0.0944 - dense_17_loss: 20.5946 - dense_18_loss: 0.0191 - val_loss: 52.1190 - val_dense_16_loss: 0.1002 - val_dense_17_loss: 19.6541 - val_dense_18_loss: 0.0120\n",
            "\n",
            "Epoch 00140: val_loss improved from 52.33438 to 52.11904, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 141/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 53.4875 - dense_16_loss: 0.0961 - dense_17_loss: 20.5187 - dense_18_loss: 0.0206 - val_loss: 56.7725 - val_dense_16_loss: 0.1097 - val_dense_17_loss: 21.0993 - val_dense_18_loss: 0.0139\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 52.11904\n",
            "Epoch 142/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 53.4231 - dense_16_loss: 0.0960 - dense_17_loss: 20.5192 - dense_18_loss: 0.0205 - val_loss: 53.0674 - val_dense_16_loss: 0.1008 - val_dense_17_loss: 19.5841 - val_dense_18_loss: 0.0162\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 52.11904\n",
            "Epoch 143/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 53.2608 - dense_16_loss: 0.0940 - dense_17_loss: 20.7458 - dense_18_loss: 0.0216 - val_loss: 56.1773 - val_dense_16_loss: 0.1035 - val_dense_17_loss: 20.1474 - val_dense_18_loss: 0.0248\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 52.11904\n",
            "Epoch 144/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 52.7622 - dense_16_loss: 0.0933 - dense_17_loss: 20.5589 - dense_18_loss: 0.0211 - val_loss: 55.6609 - val_dense_16_loss: 0.1082 - val_dense_17_loss: 19.3284 - val_dense_18_loss: 0.0194\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 52.11904\n",
            "Epoch 145/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 54.2968 - dense_16_loss: 0.0965 - dense_17_loss: 20.8453 - dense_18_loss: 0.0225 - val_loss: 62.2470 - val_dense_16_loss: 0.1111 - val_dense_17_loss: 23.6344 - val_dense_18_loss: 0.0264\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 52.11904\n",
            "Epoch 146/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 52.7307 - dense_16_loss: 0.0935 - dense_17_loss: 20.7383 - dense_18_loss: 0.0197 - val_loss: 53.7937 - val_dense_16_loss: 0.1037 - val_dense_17_loss: 19.8540 - val_dense_18_loss: 0.0141\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 52.11904\n",
            "Epoch 147/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 53.1838 - dense_16_loss: 0.0932 - dense_17_loss: 20.8385 - dense_18_loss: 0.0220 - val_loss: 57.8039 - val_dense_16_loss: 0.1037 - val_dense_17_loss: 22.1936 - val_dense_18_loss: 0.0225\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 52.11904\n",
            "Epoch 148/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 53.2602 - dense_16_loss: 0.0922 - dense_17_loss: 20.6845 - dense_18_loss: 0.0246 - val_loss: 57.6968 - val_dense_16_loss: 0.1047 - val_dense_17_loss: 21.5117 - val_dense_18_loss: 0.0238\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 52.11904\n",
            "Epoch 149/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 51.7104 - dense_16_loss: 0.0914 - dense_17_loss: 20.5321 - dense_18_loss: 0.0188 - val_loss: 58.3258 - val_dense_16_loss: 0.1078 - val_dense_17_loss: 22.2982 - val_dense_18_loss: 0.0185\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 52.11904\n",
            "Epoch 150/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 52.8230 - dense_16_loss: 0.0953 - dense_17_loss: 20.3053 - dense_18_loss: 0.0196 - val_loss: 53.7893 - val_dense_16_loss: 0.1009 - val_dense_17_loss: 20.6092 - val_dense_18_loss: 0.0145\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 52.11904\n",
            "Epoch 151/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 53.4492 - dense_16_loss: 0.0942 - dense_17_loss: 20.7430 - dense_18_loss: 0.0222 - val_loss: 58.6575 - val_dense_16_loss: 0.1111 - val_dense_17_loss: 22.0542 - val_dense_18_loss: 0.0163\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 52.11904\n",
            "Epoch 152/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 52.6461 - dense_16_loss: 0.0953 - dense_17_loss: 20.3946 - dense_18_loss: 0.0183 - val_loss: 67.5046 - val_dense_16_loss: 0.1368 - val_dense_17_loss: 21.5364 - val_dense_18_loss: 0.0247\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 52.11904\n",
            "Epoch 153/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 52.4555 - dense_16_loss: 0.0935 - dense_17_loss: 20.3651 - dense_18_loss: 0.0202 - val_loss: 62.8995 - val_dense_16_loss: 0.1133 - val_dense_17_loss: 24.4219 - val_dense_18_loss: 0.0225\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 52.11904\n",
            "Epoch 154/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 51.8591 - dense_16_loss: 0.0920 - dense_17_loss: 20.3009 - dense_18_loss: 0.0198 - val_loss: 64.2423 - val_dense_16_loss: 0.1202 - val_dense_17_loss: 22.9744 - val_dense_18_loss: 0.0261\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 52.11904\n",
            "Epoch 155/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 52.1551 - dense_16_loss: 0.0936 - dense_17_loss: 20.1483 - dense_18_loss: 0.0196 - val_loss: 56.7511 - val_dense_16_loss: 0.1122 - val_dense_17_loss: 19.7381 - val_dense_18_loss: 0.0168\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 52.11904\n",
            "Epoch 156/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 51.3570 - dense_16_loss: 0.0921 - dense_17_loss: 19.9826 - dense_18_loss: 0.0188 - val_loss: 53.5506 - val_dense_16_loss: 0.1025 - val_dense_17_loss: 19.5957 - val_dense_18_loss: 0.0161\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 52.11904\n",
            "Epoch 157/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 51.6272 - dense_16_loss: 0.0899 - dense_17_loss: 20.0251 - dense_18_loss: 0.0231 - val_loss: 54.4185 - val_dense_16_loss: 0.1011 - val_dense_17_loss: 19.7581 - val_dense_18_loss: 0.0216\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 52.11904\n",
            "Epoch 158/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 51.3679 - dense_16_loss: 0.0903 - dense_17_loss: 20.0398 - dense_18_loss: 0.0213 - val_loss: 54.3763 - val_dense_16_loss: 0.1012 - val_dense_17_loss: 20.8956 - val_dense_18_loss: 0.0156\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 52.11904\n",
            "Epoch 159/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 53.2938 - dense_16_loss: 0.0902 - dense_17_loss: 20.2586 - dense_18_loss: 0.0299 - val_loss: 59.3489 - val_dense_16_loss: 0.1058 - val_dense_17_loss: 20.1363 - val_dense_18_loss: 0.0374\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 52.11904\n",
            "Epoch 160/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 51.9488 - dense_16_loss: 0.0931 - dense_17_loss: 20.0700 - dense_18_loss: 0.0198 - val_loss: 54.3601 - val_dense_16_loss: 0.1037 - val_dense_17_loss: 19.6879 - val_dense_18_loss: 0.0178\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 52.11904\n",
            "Epoch 161/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 51.2302 - dense_16_loss: 0.0896 - dense_17_loss: 20.1826 - dense_18_loss: 0.0208 - val_loss: 55.3810 - val_dense_16_loss: 0.1075 - val_dense_17_loss: 20.1871 - val_dense_18_loss: 0.0147\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 52.11904\n",
            "Epoch 162/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 50.8948 - dense_16_loss: 0.0898 - dense_17_loss: 20.0730 - dense_18_loss: 0.0194 - val_loss: 52.5372 - val_dense_16_loss: 0.1017 - val_dense_17_loss: 19.5525 - val_dense_18_loss: 0.0124\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 52.11904\n",
            "Epoch 163/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 50.2750 - dense_16_loss: 0.0897 - dense_17_loss: 19.9109 - dense_18_loss: 0.0172 - val_loss: 54.2379 - val_dense_16_loss: 0.1019 - val_dense_17_loss: 19.0926 - val_dense_18_loss: 0.0230\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 52.11904\n",
            "Epoch 164/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 51.2259 - dense_16_loss: 0.0909 - dense_17_loss: 20.0141 - dense_18_loss: 0.0198 - val_loss: 57.3532 - val_dense_16_loss: 0.1087 - val_dense_17_loss: 19.8143 - val_dense_18_loss: 0.0246\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 52.11904\n",
            "Epoch 165/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 50.2104 - dense_16_loss: 0.0893 - dense_17_loss: 19.8508 - dense_18_loss: 0.0178 - val_loss: 53.9728 - val_dense_16_loss: 0.1029 - val_dense_17_loss: 19.9852 - val_dense_18_loss: 0.0156\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 52.11904\n",
            "Epoch 166/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 51.2963 - dense_16_loss: 0.0917 - dense_17_loss: 20.0811 - dense_18_loss: 0.0185 - val_loss: 60.2947 - val_dense_16_loss: 0.1149 - val_dense_17_loss: 20.3865 - val_dense_18_loss: 0.0272\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 52.11904\n",
            "Epoch 167/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 51.1832 - dense_16_loss: 0.0885 - dense_17_loss: 20.2187 - dense_18_loss: 0.0221 - val_loss: 51.8356 - val_dense_16_loss: 0.1014 - val_dense_17_loss: 18.9303 - val_dense_18_loss: 0.0124\n",
            "\n",
            "Epoch 00167: val_loss improved from 52.11904 to 51.83556, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 168/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 50.6624 - dense_16_loss: 0.0895 - dense_17_loss: 19.8521 - dense_18_loss: 0.0198 - val_loss: 53.6328 - val_dense_16_loss: 0.1002 - val_dense_17_loss: 19.2736 - val_dense_18_loss: 0.0215\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 51.83556\n",
            "Epoch 169/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 49.5743 - dense_16_loss: 0.0873 - dense_17_loss: 19.6441 - dense_18_loss: 0.0188 - val_loss: 53.9652 - val_dense_16_loss: 0.1029 - val_dense_17_loss: 19.5367 - val_dense_18_loss: 0.0177\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 51.83556\n",
            "Epoch 170/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 49.7622 - dense_16_loss: 0.0881 - dense_17_loss: 19.7220 - dense_18_loss: 0.0180 - val_loss: 54.8943 - val_dense_16_loss: 0.1005 - val_dense_17_loss: 20.2430 - val_dense_18_loss: 0.0225\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 51.83556\n",
            "Epoch 171/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 50.0172 - dense_16_loss: 0.0878 - dense_17_loss: 19.7975 - dense_18_loss: 0.0193 - val_loss: 58.2637 - val_dense_16_loss: 0.1117 - val_dense_17_loss: 20.4153 - val_dense_18_loss: 0.0216\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 51.83556\n",
            "Epoch 172/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 49.5356 - dense_16_loss: 0.0871 - dense_17_loss: 19.7467 - dense_18_loss: 0.0183 - val_loss: 53.1659 - val_dense_16_loss: 0.1040 - val_dense_17_loss: 19.4373 - val_dense_18_loss: 0.0126\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 51.83556\n",
            "Epoch 173/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 49.5028 - dense_16_loss: 0.0860 - dense_17_loss: 19.7553 - dense_18_loss: 0.0198 - val_loss: 56.8868 - val_dense_16_loss: 0.1055 - val_dense_17_loss: 22.6765 - val_dense_18_loss: 0.0128\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 51.83556\n",
            "Epoch 174/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 49.1702 - dense_16_loss: 0.0872 - dense_17_loss: 19.6357 - dense_18_loss: 0.0169 - val_loss: 67.7331 - val_dense_16_loss: 0.1095 - val_dense_17_loss: 27.3291 - val_dense_18_loss: 0.0377\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 51.83556\n",
            "Epoch 175/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 48.5913 - dense_16_loss: 0.0854 - dense_17_loss: 19.6455 - dense_18_loss: 0.0167 - val_loss: 59.7465 - val_dense_16_loss: 0.1040 - val_dense_17_loss: 22.1581 - val_dense_18_loss: 0.0319\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 51.83556\n",
            "Epoch 176/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 48.9991 - dense_16_loss: 0.0861 - dense_17_loss: 19.6837 - dense_18_loss: 0.0174 - val_loss: 55.0020 - val_dense_16_loss: 0.1033 - val_dense_17_loss: 20.2119 - val_dense_18_loss: 0.0190\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 51.83556\n",
            "Epoch 177/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 49.1857 - dense_16_loss: 0.0864 - dense_17_loss: 19.8063 - dense_18_loss: 0.0173 - val_loss: 55.4506 - val_dense_16_loss: 0.1068 - val_dense_17_loss: 20.0855 - val_dense_18_loss: 0.0166\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 51.83556\n",
            "Epoch 178/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 48.8736 - dense_16_loss: 0.0860 - dense_17_loss: 19.6033 - dense_18_loss: 0.0173 - val_loss: 55.3268 - val_dense_16_loss: 0.1046 - val_dense_17_loss: 20.3739 - val_dense_18_loss: 0.0179\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 51.83556\n",
            "Epoch 179/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 47.9569 - dense_16_loss: 0.0841 - dense_17_loss: 19.3579 - dense_18_loss: 0.0169 - val_loss: 54.5978 - val_dense_16_loss: 0.1051 - val_dense_17_loss: 19.1537 - val_dense_18_loss: 0.0196\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 51.83556\n",
            "Epoch 180/200\n",
            "42750/42750 [==============================] - 10s 245us/step - loss: 48.8160 - dense_16_loss: 0.0860 - dense_17_loss: 19.4161 - dense_18_loss: 0.0180 - val_loss: 55.4352 - val_dense_16_loss: 0.1086 - val_dense_17_loss: 19.3626 - val_dense_18_loss: 0.0174\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 51.83556\n",
            "Epoch 181/200\n",
            "42750/42750 [==============================] - 10s 242us/step - loss: 47.5760 - dense_16_loss: 0.0841 - dense_17_loss: 19.1116 - dense_18_loss: 0.0161 - val_loss: 54.8062 - val_dense_16_loss: 0.1060 - val_dense_17_loss: 20.6564 - val_dense_18_loss: 0.0118\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 51.83556\n",
            "Epoch 182/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 48.4738 - dense_16_loss: 0.0872 - dense_17_loss: 19.3535 - dense_18_loss: 0.0149 - val_loss: 57.0223 - val_dense_16_loss: 0.1080 - val_dense_17_loss: 21.0330 - val_dense_18_loss: 0.0179\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 51.83556\n",
            "Epoch 183/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 48.8567 - dense_16_loss: 0.0874 - dense_17_loss: 19.3621 - dense_18_loss: 0.0164 - val_loss: 54.4805 - val_dense_16_loss: 0.1014 - val_dense_17_loss: 19.4182 - val_dense_18_loss: 0.0232\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 51.83556\n",
            "Epoch 184/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 48.3768 - dense_16_loss: 0.0848 - dense_17_loss: 19.3045 - dense_18_loss: 0.0182 - val_loss: 53.6088 - val_dense_16_loss: 0.1038 - val_dense_17_loss: 19.5398 - val_dense_18_loss: 0.0147\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 51.83556\n",
            "Epoch 185/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 48.1261 - dense_16_loss: 0.0844 - dense_17_loss: 19.2052 - dense_18_loss: 0.0179 - val_loss: 52.4033 - val_dense_16_loss: 0.1010 - val_dense_17_loss: 19.6365 - val_dense_18_loss: 0.0123\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 51.83556\n",
            "Epoch 186/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 48.4144 - dense_16_loss: 0.0850 - dense_17_loss: 19.3228 - dense_18_loss: 0.0180 - val_loss: 53.1651 - val_dense_16_loss: 0.1035 - val_dense_17_loss: 19.7908 - val_dense_18_loss: 0.0117\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 51.83556\n",
            "Epoch 187/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 48.1713 - dense_16_loss: 0.0846 - dense_17_loss: 19.2042 - dense_18_loss: 0.0179 - val_loss: 58.7293 - val_dense_16_loss: 0.1091 - val_dense_17_loss: 21.9133 - val_dense_18_loss: 0.0204\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 51.83556\n",
            "Epoch 188/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 48.1517 - dense_16_loss: 0.0857 - dense_17_loss: 19.1808 - dense_18_loss: 0.0164 - val_loss: 57.7756 - val_dense_16_loss: 0.1120 - val_dense_17_loss: 19.6711 - val_dense_18_loss: 0.0225\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 51.83556\n",
            "Epoch 189/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 48.2471 - dense_16_loss: 0.0842 - dense_17_loss: 19.2127 - dense_18_loss: 0.0189 - val_loss: 55.5247 - val_dense_16_loss: 0.1089 - val_dense_17_loss: 19.8344 - val_dense_18_loss: 0.0150\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 51.83556\n",
            "Epoch 190/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 46.8538 - dense_16_loss: 0.0826 - dense_17_loss: 19.1371 - dense_18_loss: 0.0147 - val_loss: 53.7763 - val_dense_16_loss: 0.1028 - val_dense_17_loss: 20.2735 - val_dense_18_loss: 0.0133\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 51.83556\n",
            "Epoch 191/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 48.9531 - dense_16_loss: 0.0876 - dense_17_loss: 19.2787 - dense_18_loss: 0.0169 - val_loss: 55.7846 - val_dense_16_loss: 0.1104 - val_dense_17_loss: 19.5349 - val_dense_18_loss: 0.0157\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 51.83556\n",
            "Epoch 192/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 48.3773 - dense_16_loss: 0.0867 - dense_17_loss: 19.0176 - dense_18_loss: 0.0167 - val_loss: 52.7821 - val_dense_16_loss: 0.1008 - val_dense_17_loss: 19.1090 - val_dense_18_loss: 0.0171\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 51.83556\n",
            "Epoch 193/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 46.8781 - dense_16_loss: 0.0803 - dense_17_loss: 18.8685 - dense_18_loss: 0.0196 - val_loss: 56.6350 - val_dense_16_loss: 0.1055 - val_dense_17_loss: 19.6518 - val_dense_18_loss: 0.0267\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 51.83556\n",
            "Epoch 194/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 47.0017 - dense_16_loss: 0.0814 - dense_17_loss: 18.9436 - dense_18_loss: 0.0182 - val_loss: 54.0857 - val_dense_16_loss: 0.1023 - val_dense_17_loss: 19.3149 - val_dense_18_loss: 0.0204\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 51.83556\n",
            "Epoch 195/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 47.0953 - dense_16_loss: 0.0830 - dense_17_loss: 18.9303 - dense_18_loss: 0.0163 - val_loss: 55.1995 - val_dense_16_loss: 0.1037 - val_dense_17_loss: 19.9397 - val_dense_18_loss: 0.0207\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 51.83556\n",
            "Epoch 196/200\n",
            "42750/42750 [==============================] - 10s 244us/step - loss: 46.7618 - dense_16_loss: 0.0832 - dense_17_loss: 18.6780 - dense_18_loss: 0.0157 - val_loss: 53.2676 - val_dense_16_loss: 0.1023 - val_dense_17_loss: 19.4515 - val_dense_18_loss: 0.0156\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 51.83556\n",
            "Epoch 197/200\n",
            "42750/42750 [==============================] - 10s 243us/step - loss: 46.1050 - dense_16_loss: 0.0801 - dense_17_loss: 18.9140 - dense_18_loss: 0.0158 - val_loss: 54.1502 - val_dense_16_loss: 0.1017 - val_dense_17_loss: 20.0234 - val_dense_18_loss: 0.0180\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 51.83556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PTmTP4dZBWa",
        "colab_type": "code",
        "outputId": "c4f10eaa-15e5-47fb-8252-7e8f32070512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "MODELPATH1=\"MLP_model1.hdf5\"\n",
        "if os.path.exists(MODELPATH1):\n",
        "#     model.load_weights(MODELPATH1)\n",
        "      model=load_model(MODELPATH1)\n",
        "    # 若成功加载前面保存的参数，输出下列信息\n",
        "      print(\"checkpoint_loaded\")\n",
        "# Y_hat = model.predict(X)\n",
        "# X1 = np.concatenate([x_train1,x_val1])\n",
        "# X2 = np.concatenate([x_train2,x_val2])\n",
        "X_1,X_2 = resize_input(X)\n",
        "Y_hat = model.predict([X_1,X_2])\n",
        "# Y_hat2 = model.predict([x_val1,x_val2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 shape: (47500, 5000, 1)  X2 shape: (47500, 50, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhhzJldgpGS7",
        "colab_type": "code",
        "outputId": "be83c4af-54d0-427f-afda-35814c95e0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_hat=np.array(Y_hat)\n",
        "Y_hat=Y_hat.reshape(3,-1)\n",
        "print(Y_hat.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 47500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI2wuqumcGAA",
        "colab_type": "code",
        "outputId": "3384c4d1-8477-4834-aa09-8dd3dc6c5bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Y_hat=np.array(Y_hat)\n",
        "# Y_hat = Y_hat.reshape(3,42750)\n",
        "# Y_hat = np.transpose(Y_hat)\n",
        "# print(Y_hat.shape)\n",
        "# Y_hat2=np.array(Y_hat2)\n",
        "# Y_hat2 = Y_hat2.reshape(3,4750)\n",
        "# Y_hat2 = np.transpose(Y_hat2)\n",
        "# print(Y_hat2.shape)\n",
        "# Y_hat = np.concatenate([Y_hat,Y_hat2])\n",
        "# print(Y_hat.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-92a1ffc293ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_hat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42750\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_hat2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 142500 into shape (3,42750)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwyeRzI4lOXw",
        "colab_type": "code",
        "outputId": "46f27a74-e96d-4219-c7d7-1828ac66a8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# yy=np.concatenate([y_train,y_val])\n",
        "# print(yy.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Uc6fdFZNgH",
        "colab_type": "code",
        "outputId": "42aed005-1dfe-4a6c-9f2f-ee71762f0881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Y_hat = model.predict(X)\n",
        "# Y_hat=np.array(Y_hat)\n",
        "# Y_hat = Y_hat.reshape(3,X1.shape[0])\n",
        "# Y_hat=np.array(Y_hat)\n",
        "Y_hat = np.transpose(Y_hat)\n",
        "# Y_hat = np.transpose(Y_hat).reshape(-1,3)\n",
        "print(Y_hat.shape)\n",
        "err_mat = np.abs(Y-Y_hat)/Y\n",
        "err_mat = np.sum(err_mat,axis=1)\n",
        "err_mat = np.sum(err_mat)\n",
        "print(err_mat/Y.shape[0])\n",
        "#0.9704514192587647\n",
        "#10000d 0.9945707841335775\n",
        "#0.5936615853774215\n",
        "#0.32381799619055995"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3)\n",
            "0.585150563532674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy68LySzkebL",
        "colab_type": "code",
        "outputId": "0e338496-fa3d-4113-b34c-3477328f5335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(Y_hat.shape,Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3) (47500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkMg_43QUYlH",
        "colab_type": "code",
        "outputId": "d6eabe00-e9e9-423d-de4d-064ad3c13ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "err_WMAE = np.dot(np.abs(Y-Y_hat),np.array([300,1,200]))\n",
        "err_WMAE = np.sum(err_WMAE)/Y.shape[0]\n",
        "print(err_WMAE)\n",
        "#145.68255323961225\n",
        "#25.64659003243506"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42.52386678819353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23qSUi4qEn63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('MLP_WMAE_lastEpoch.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t15AqhB-aUG7",
        "colab_type": "code",
        "outputId": "a6853f45-b7b5-4c0a-ecb6-72ec542c8d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtest_1,Xtest_2 = resize_input(x_test)\n",
        "Y_test_hat = np.array(model.predict([Xtest_1,Xtest_2]))\n",
        "Y_test_hat = Y_test_hat.reshape(3,-1)\n",
        "Y_test_hat = np.transpose(Y_test_hat)\n",
        "np.savetxt(dirPath+'CNN_11.csv',Y_test_hat,delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 shape: (2500, 5000, 1)  X2 shape: (2500, 50, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n34h467dHye",
        "colab_type": "code",
        "outputId": "9a395d7d-959e-4af9-e36f-6a42dc70c7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "!zip cnn_output_11 MLP_output/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: MLP_output/CNN_11.csv (deflated 61%)\n",
            "  adding: MLP_output/MLP_history.pickle (deflated 53%)\n",
            "  adding: MLP_output/MLP_model.hdf5 (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPtpSwm0dUaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv cnn_output_11.zip '/content/gdrive/My Drive/term_2019_1/MLTech/Final'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJNSUc9KxFIi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_0fZ47Sw5uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv MLP_output/MLP_history.pickle MLP_output/MLP_history1.pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra05uSdXW7KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}