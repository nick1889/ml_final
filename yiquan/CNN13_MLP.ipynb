{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MLFinal-MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAPXN1XeRWlZ",
        "colab_type": "code",
        "outputId": "48102f94-8fa1-4a0e-cfd7-6a9c63122c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC-QrcB8SfYi",
        "colab_type": "code",
        "outputId": "4d06e7fc-d3b5-49c9-dd84-eac2261123cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://learner.csie.ntu.edu.tw/~judge/ml19spring/ml19spring.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-02 08:24:14--  https://learner.csie.ntu.edu.tw/~judge/ml19spring/ml19spring.zip\n",
            "Resolving learner.csie.ntu.edu.tw (learner.csie.ntu.edu.tw)... 140.112.90.193\n",
            "Connecting to learner.csie.ntu.edu.tw (learner.csie.ntu.edu.tw)|140.112.90.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3780741206 (3.5G) [application/zip]\n",
            "Saving to: ‘ml19spring.zip’\n",
            "\n",
            "ml19spring.zip      100%[===================>]   3.52G  16.3MB/s    in 5m 8s   \n",
            "\n",
            "2019-06-02 08:29:24 (11.7 MB/s) - ‘ml19spring.zip’ saved [3780741206/3780741206]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqFi8yQLSlm8",
        "colab_type": "code",
        "outputId": "a828c3c4-529e-4312-dc65-b3e63976e60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!unzip -q ml19spring.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open ml19spring.zip, ml19spring.zip.zip or ml19spring.zip.ZIP.\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4o6D8EJPRLS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRoIQ2-EbsYq",
        "colab_type": "code",
        "outputId": "ea4099fa-14da-49a9-d1f5-130a109434bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "# Save model to your Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThyMATs2TBoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "DataDir='./gdrive/My Drive/term_2019_1/MLTech/Final/data/'\n",
        "x_test = np.load(DataDir+'X_test.npz')['arr_0']\n",
        "X = np.load(DataDir+'X_train.npz')['arr_0']\n",
        "Y = np.load(DataDir+'Y_train.npz')['arr_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gab1tIdQTze0",
        "colab_type": "code",
        "outputId": "3016bf76-b4a6-4cbb-d4d7-125112242f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "source": [
        "dirPath = './MLP_output/'\n",
        "!mkdir ./MLP_output\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEqolJs4bSTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(copy=False)\n",
        "X = scaler.fit_transform(X)\n",
        "x_test = scaler.transform( x_test )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPeL0Z6ewL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_fea = np.load(DataDir+'important_feat_0.0002.npy')\n",
        "X=X[:,important_fea]\n",
        "x_test = x_test[:,important_fea]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ID5oapOlAQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del x_train,x_val,y_train,y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0otbP-OUN9X",
        "colab_type": "code",
        "outputId": "d20b38b1-0e40-44a2-c0d1-c9c5461bc13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "x_train,x_val,y_train,y_val=train_test_split(X,Y,test_size=0.2,random_state=44)\n",
        "print(x_train.shape,x_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(38000, 10000) (9500, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAydVCGJmtnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model\n",
        "# 23.82688"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDlM_4wOcgLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del y_train_T,y_val_T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf5eojAHUdCk",
        "colab_type": "code",
        "outputId": "b73cb56c-c66f-4c44-8557-5a72620c0a38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14436
        }
      },
      "source": [
        "import pickle\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "def MLP_MAE_onlyOutput(input_size):\n",
        "    same_input = Input(shape=(input_size,), dtype='float32',name='input')\n",
        "    X1 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X1 = PReLU()(X1)\n",
        "    X1 = Dense(128,kernel_initializer='normal')(X1)\n",
        "    X1 = PReLU()(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(68,kernel_initializer='normal',activation='tanh')(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(32,kernel_initializer='normal',activation='tanh')(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    y1 = Dense(1,kernel_initializer='normal',name='output_y1')(X1)\n",
        "    \n",
        "    X2 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X2 = PReLU()(X2)\n",
        "    X2 = Dense(128,kernel_initializer='normal')(X2)\n",
        "    X2 = PReLU()(X2)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(68,kernel_initializer='normal',activation='tanh')(X2)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(32,kernel_initializer='normal',activation='tanh')(X2)\n",
        "#     X2=BatchNormalization()(X2)\n",
        "    y2 = Dense(1,kernel_initializer='normal',name='output_y2')(X2)\n",
        "    \n",
        "    X3 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X3 = PReLU()(X3)\n",
        "    X3 = Dense(64,kernel_initializer='normal')(X3)\n",
        "    X3 = PReLU()(X3)\n",
        "    y3 = Dense(1,kernel_initializer='normal',name='output_y3')(X3)\n",
        "    model = Model(inputs=same_input, \n",
        "                 outputs=[y1,y2,y3])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'output_y1': 'mean_absolute_error',\n",
        "                       'output_y2': 'mean_absolute_error',\n",
        "                       'output_y3': 'mean_absolute_error'},loss_weights=[300,1,200])\n",
        "    return model\n",
        "def baseline_MLP(input_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512,input_dim=input_size,kernel_initializer='normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(128,kernel_initializer='normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(3,kernel_initializer='normal'))\n",
        "    model.compile(loss = 'mean_absolute_percentage_error',optimizer='adam')\n",
        "    return model\n",
        "def baseline_MLP_WMAE(input_size):\n",
        "    m_input = Input(shape=(input_size,), dtype='float32')\n",
        "    X = Dense(512,kernel_initializer='normal')(m_input)\n",
        "    X = PReLU()(X)\n",
        "    X = Dense(128,kernel_initializer='normal')(X)\n",
        "    X = PReLU()(X)\n",
        "    X1 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(32,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    #     X1 = PReLU()(X1)\n",
        "    X2 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(32,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    #     X2 = PReLU()(X2)\n",
        "    X3 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X3=BatchNormalization()(X3)\n",
        "#     X3 = PReLU()(X3)\n",
        "    y1 = Dense(1,kernel_initializer='normal')(X1)\n",
        "    y2 = Dense(1,kernel_initializer='normal')(X2)\n",
        "    y3 = Dense(1,kernel_initializer='normal')(X3)\n",
        "    model = Model(inputs=m_input, outputs=[y1,y2,y3])\n",
        "    model.compile(optimizer='adam', loss='mean_absolute_error',\n",
        "              loss_weights=[300,1,200])\n",
        "    return model\n",
        "def cnn_WMAE(input1_size,input2_size):\n",
        "    print(input1_size,input2_size)\n",
        "    m_input1 = Input(shape=(5000,1,), dtype='float32')\n",
        "    m_input2 = Input(shape=(50,100,1,), dtype='float32')\n",
        "    x11 = Conv1D(64, 7,strides=4,input_shape=(5000,1), padding='same', activation='relu')(m_input1)\n",
        "    x11 =MaxPooling1D(pool_size=3,strides=4)(x11)\n",
        "    x11 = Conv1D(192, 3,strides=2, padding='same', activation='relu')(x11)\n",
        "    x11 =MaxPooling1D(pool_size=3,strides=2)(x11)\n",
        "    tower_1 = Conv1D(96, 3, padding='same', activation='relu')(x11)\n",
        "    tower_1 = Conv1D(128, 3, padding='same', activation='relu')(tower_1)\n",
        "\n",
        "    tower_2 = Conv1D(16, 1, padding='same', activation='relu')(x11)\n",
        "    tower_2 = Conv1D(32, 5, padding='same', activation='relu')(tower_2)\n",
        "\n",
        "    tower_3 = MaxPooling1D(3, strides=1, padding='same')(x11)\n",
        "    tower_3 = Conv1D(32, 1, padding='same', activation='relu')(tower_3)\n",
        "    tower_4 = Conv1D(16, 1, padding='same', activation='relu')(x11)\n",
        "\n",
        "    x11 = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "    x11 = GlobalAveragePooling1D()(x11)\n",
        "    \n",
        "#     x11 = Flatten()(x11)\n",
        "#     x12 = Flatten()(x12)\n",
        "#     x2 = Conv2D(64, 5,strides=2,input_shape=(50,100,1), padding='same', activation='relu')(m_input2)\n",
        "#     x2 =MaxPooling1D(pool_size=2,strides=2)(x2)\n",
        "#     x2 = Conv2D(192, 3,strides=2, padding='same', activation='relu')(x2)\n",
        "# #     x11 =MaxPooling1D(pool_size=3,strides=2)(x11)\n",
        "#     tower_1 = Conv2D(96, 3, padding='same', activation='relu')(x2)\n",
        "#     tower_1 = Conv2D(128, 3, padding='same', activation='relu')(tower_1)\n",
        "\n",
        "#     tower_2 = Conv2D(16, 1, padding='same', activation='relu')(x2)\n",
        "#     tower_2 = Conv2D(32, 5, padding='same', activation='relu')(tower_2)\n",
        "\n",
        "#     tower_3 = MaxPooling1D(3, strides=1, padding='same')(x2)\n",
        "#     tower_3 = Conv2D(32, 1, padding='same', activation='relu')(tower_3)\n",
        "#     tower_4 = Conv2D(64, 1, padding='same', activation='relu')(x2)\n",
        "\n",
        "#     x2 = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "#     x2 = GlobalAveragePooling2D()(x2)\n",
        "    \n",
        "    x21 = Conv2D(64,kernel_size=(1,3),input_shape=(50,100,1), activation='relu')(m_input2)\n",
        "    x21 = Conv2D(32,kernel_size=(1,1), activation='relu')(x21)\n",
        "    x21 =MaxPooling2D(pool_size=(1,24))(x21)\n",
        "\n",
        "    x21 = Flatten()(x21)\n",
        "    merged = concatenate(\n",
        "        [x11, x2],\n",
        "        axis=-1)\n",
        "    merged = Dropout(0.15)(merged)\n",
        "    merged=BatchNormalization()(merged)\n",
        "    merged = Dense(units=200,kernel_initializer='normal')(merged)\n",
        "    merged = PReLU()(merged)\n",
        "    merged = Dense(units=64,kernel_initializer='normal', activation='tanh')(merged)\n",
        "#     yy2 = Dense(units=64,kernel_initializer='normal')(merged)\n",
        "#     yy2=PReLU()(yy2)\n",
        "#     yy3 = Dense(units=64,kernel_initializer='normal', activation='tanh')(merged)\n",
        "#     merged = Dropout(0.15)(merged)\n",
        "#     merged = Dense(units=128,kernel_initializer='normal')(merged)\n",
        "#     merged=PReLU()(merged)\n",
        "#     merged = Dropout(0.2)(merged)\n",
        "#     merged=BatchNormalization()(merged)\n",
        "    y1 = Dense(units=1)(merged)\n",
        "    y2 = Dense(units=1)(merged)\n",
        "    y3 = Dense(units=1)(merged)\n",
        "\n",
        "    model = Model(\n",
        "        inputs=[m_input1, m_input2],\n",
        "        outputs=[y1,y2,y3])\n",
        "#     adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.98)\n",
        "    model.compile(optimizer='adam', loss='mean_absolute_error',\n",
        "              loss_weights=[300,1,200])\n",
        "#     model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "    return model\n",
        "def resize_input(X):\n",
        "    X1 = X[:,:5000].reshape(-1,5000,1)\n",
        "    X2 = X[:,5000:].reshape(-1,50,100,1)\n",
        "    print('X1 shape:',X1.shape,' X2 shape:',X2.shape)\n",
        "    return X1,X2\n",
        "dirPath = './MLP_output/' \n",
        "# model = baseline_MLP_WMAE(x_train.shape[1])\n",
        "# model = baseline_MLP(x_train.shape[1])\n",
        "# model = MLP_MAE_onlyOutput(x_train.shape[1])\n",
        "x_train1,x_train2 = resize_input(x_train)\n",
        "x_val1,x_val2 = resize_input(x_val)\n",
        "model = cnn_WMAE(x_train1.shape[1:3],x_train2.shape[1:4])\n",
        "model.summary()\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "NUM_EPOCHS = 200\n",
        "\n",
        "# checkpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss',patience=30)\n",
        "MODELPATH=dirPath+\"MLP_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(MODELPATH, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "print(y_train.shape)\n",
        "y_train = np.transpose(y_train)\n",
        "print(y_train.shape)\n",
        "y_val = np.transpose(y_val)\n",
        "print(y_val.shape)\n",
        "# history = model.fit(x=x_train,y=[y_train_T[0],y_train_T[1],y_train_T[2]],batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,[y_val_T[0],y_val_T[1],y_val_T[2]]),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "# history = model.fit(x=x_train,y=y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,y_val),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "# history = model.fit(x=x_train,y=y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,y_val),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "history = model.fit(x=[x_train1,x_train2],y=[y_train[0],y_train[1],y_train[2]],batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "        validation_data=([x_val1,x_val2],[y_val[0],y_val[1],y_val[2]]),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "file_his = open(dirPath+'MLP_history.pickle', 'wb')\n",
        "pickle.dump(history.history, file_his)\n",
        "file_his.close()\n",
        "#25 100 114"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0615 04:19:53.087494 139873632606080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0615 04:19:53.096724 139873632606080 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X1 shape: (38000, 5000, 1)  X2 shape: (38000, 50, 100, 1)\n",
            "X1 shape: (9500, 5000, 1)  X2 shape: (9500, 50, 100, 1)\n",
            "(5000, 1) (50, 100, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 04:19:53.203973 139873632606080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "W0615 04:19:53.262944 139873632606080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 5000, 1)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 1250, 64)     512         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling1D) (None, 312, 64)      0           conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 156, 192)     37056       max_pooling1d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling1D) (None, 77, 192)      0           conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            (None, 50, 100, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 77, 96)       55392       max_pooling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, 77, 16)       3088        max_pooling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling1D) (None, 77, 192)      0           max_pooling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 50, 98, 64)   256         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 77, 128)      36992       conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 77, 32)       2592        conv1d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, 77, 32)       6176        max_pooling1d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_32 (Conv1D)              (None, 77, 16)       3088        max_pooling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 50, 98, 32)   2080        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 77, 208)      0           conv1d_28[0][0]                  \n",
            "                                                                 conv1d_30[0][0]                  \n",
            "                                                                 conv1d_31[0][0]                  \n",
            "                                                                 conv1d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 50, 4, 32)    0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 208)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 6400)         0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 6608)         0           global_average_pooling1d_1[0][0] \n",
            "                                                                 flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 6608)         0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 6608)         26432       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 300)          1982700     batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_1 (PReLU)               (None, 300)          300         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           19264       p_re_lu_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            65          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            65          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            65          dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,176,123\n",
            "Trainable params: 2,162,907\n",
            "Non-trainable params: 13,216\n",
            "__________________________________________________________________________________________________\n",
            "(38000, 3)\n",
            "(3, 38000)\n",
            "(3, 9500)\n",
            "Train on 38000 samples, validate on 9500 samples\n",
            "Epoch 1/200\n",
            "38000/38000 [==============================] - 27s 700us/step - loss: 337.6359 - dense_3_loss: 0.4659 - dense_4_loss: 136.9818 - dense_5_loss: 0.3044 - val_loss: 235.5540 - val_dense_3_loss: 0.2638 - val_dense_4_loss: 136.3786 - val_dense_5_loss: 0.1001\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 235.55398, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 2/200\n",
            "38000/38000 [==============================] - 14s 366us/step - loss: 236.0031 - dense_3_loss: 0.2649 - dense_4_loss: 135.8582 - dense_5_loss: 0.1033 - val_loss: 227.2329 - val_dense_3_loss: 0.2546 - val_dense_4_loss: 135.1121 - val_dense_5_loss: 0.0788\n",
            "\n",
            "Epoch 00002: val_loss improved from 235.55398 to 227.23288, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 3/200\n",
            "38000/38000 [==============================] - 14s 367us/step - loss: 227.4301 - dense_3_loss: 0.2556 - dense_4_loss: 134.4799 - dense_5_loss: 0.0814 - val_loss: 221.7404 - val_dense_3_loss: 0.2504 - val_dense_4_loss: 133.7121 - val_dense_5_loss: 0.0646\n",
            "\n",
            "Epoch 00003: val_loss improved from 227.23288 to 221.74037, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 4/200\n",
            "38000/38000 [==============================] - 14s 367us/step - loss: 221.0078 - dense_3_loss: 0.2486 - dense_4_loss: 133.1059 - dense_5_loss: 0.0666 - val_loss: 214.8134 - val_dense_3_loss: 0.2392 - val_dense_4_loss: 132.2001 - val_dense_5_loss: 0.0542\n",
            "\n",
            "Epoch 00004: val_loss improved from 221.74037 to 214.81343, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 5/200\n",
            "38000/38000 [==============================] - 14s 367us/step - loss: 212.5500 - dense_3_loss: 0.2323 - dense_4_loss: 131.7205 - dense_5_loss: 0.0558 - val_loss: 209.0142 - val_dense_3_loss: 0.2266 - val_dense_4_loss: 130.8986 - val_dense_5_loss: 0.0506\n",
            "\n",
            "Epoch 00005: val_loss improved from 214.81343 to 209.01422, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 6/200\n",
            "38000/38000 [==============================] - 14s 368us/step - loss: 202.8779 - dense_3_loss: 0.2077 - dense_4_loss: 130.3565 - dense_5_loss: 0.0511 - val_loss: 218.1319 - val_dense_3_loss: 0.2534 - val_dense_4_loss: 129.5394 - val_dense_5_loss: 0.0628\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 209.01422\n",
            "Epoch 7/200\n",
            "38000/38000 [==============================] - 14s 369us/step - loss: 196.0321 - dense_3_loss: 0.1924 - dense_4_loss: 128.8851 - dense_5_loss: 0.0471 - val_loss: 200.0202 - val_dense_3_loss: 0.2070 - val_dense_4_loss: 127.2321 - val_dense_5_loss: 0.0534\n",
            "\n",
            "Epoch 00007: val_loss improved from 209.01422 to 200.02019, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 8/200\n",
            "38000/38000 [==============================] - 14s 369us/step - loss: 183.9822 - dense_3_loss: 0.1605 - dense_4_loss: 127.0182 - dense_5_loss: 0.0440 - val_loss: 196.2433 - val_dense_3_loss: 0.2082 - val_dense_4_loss: 124.2841 - val_dense_5_loss: 0.0475\n",
            "\n",
            "Epoch 00008: val_loss improved from 200.02019 to 196.24333, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 9/200\n",
            "38000/38000 [==============================] - 14s 371us/step - loss: 172.1269 - dense_3_loss: 0.1310 - dense_4_loss: 125.0461 - dense_5_loss: 0.0390 - val_loss: 184.4602 - val_dense_3_loss: 0.1772 - val_dense_4_loss: 121.4007 - val_dense_5_loss: 0.0495\n",
            "\n",
            "Epoch 00009: val_loss improved from 196.24333 to 184.46016, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 10/200\n",
            "38000/38000 [==============================] - 14s 371us/step - loss: 166.7599 - dense_3_loss: 0.1225 - dense_4_loss: 122.7868 - dense_5_loss: 0.0362 - val_loss: 175.0863 - val_dense_3_loss: 0.1527 - val_dense_4_loss: 118.4142 - val_dense_5_loss: 0.0543\n",
            "\n",
            "Epoch 00010: val_loss improved from 184.46016 to 175.08629, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 11/200\n",
            "38000/38000 [==============================] - 14s 370us/step - loss: 161.3103 - dense_3_loss: 0.1152 - dense_4_loss: 119.9295 - dense_5_loss: 0.0342 - val_loss: 165.2017 - val_dense_3_loss: 0.1394 - val_dense_4_loss: 113.1995 - val_dense_5_loss: 0.0509\n",
            "\n",
            "Epoch 00011: val_loss improved from 175.08629 to 165.20171, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 12/200\n",
            "38000/38000 [==============================] - 14s 371us/step - loss: 154.9036 - dense_3_loss: 0.1110 - dense_4_loss: 115.3178 - dense_5_loss: 0.0314 - val_loss: 159.8821 - val_dense_3_loss: 0.1414 - val_dense_4_loss: 107.8303 - val_dense_5_loss: 0.0481\n",
            "\n",
            "Epoch 00012: val_loss improved from 165.20171 to 159.88206, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 13/200\n",
            "38000/38000 [==============================] - 14s 373us/step - loss: 145.0900 - dense_3_loss: 0.1054 - dense_4_loss: 107.3577 - dense_5_loss: 0.0305 - val_loss: 154.8747 - val_dense_3_loss: 0.1304 - val_dense_4_loss: 103.8588 - val_dense_5_loss: 0.0595\n",
            "\n",
            "Epoch 00013: val_loss improved from 159.88206 to 154.87468, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 14/200\n",
            "38000/38000 [==============================] - 14s 373us/step - loss: 139.3814 - dense_3_loss: 0.1017 - dense_4_loss: 103.1531 - dense_5_loss: 0.0286 - val_loss: 149.9618 - val_dense_3_loss: 0.1314 - val_dense_4_loss: 100.5518 - val_dense_5_loss: 0.0500\n",
            "\n",
            "Epoch 00014: val_loss improved from 154.87468 to 149.96178, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 15/200\n",
            "38000/38000 [==============================] - 14s 373us/step - loss: 134.7886 - dense_3_loss: 0.0989 - dense_4_loss: 99.9666 - dense_5_loss: 0.0258 - val_loss: 142.5490 - val_dense_3_loss: 0.1180 - val_dense_4_loss: 97.8699 - val_dense_5_loss: 0.0463\n",
            "\n",
            "Epoch 00015: val_loss improved from 149.96178 to 142.54904, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 16/200\n",
            "38000/38000 [==============================] - 14s 374us/step - loss: 130.8879 - dense_3_loss: 0.0978 - dense_4_loss: 97.2198 - dense_5_loss: 0.0216 - val_loss: 137.3140 - val_dense_3_loss: 0.1160 - val_dense_4_loss: 95.4327 - val_dense_5_loss: 0.0354\n",
            "\n",
            "Epoch 00016: val_loss improved from 142.54904 to 137.31397, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 17/200\n",
            "38000/38000 [==============================] - 14s 374us/step - loss: 128.3814 - dense_3_loss: 0.0979 - dense_4_loss: 94.7874 - dense_5_loss: 0.0212 - val_loss: 133.8772 - val_dense_3_loss: 0.1067 - val_dense_4_loss: 93.1183 - val_dense_5_loss: 0.0438\n",
            "\n",
            "Epoch 00017: val_loss improved from 137.31397 to 133.87716, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 18/200\n",
            "38000/38000 [==============================] - 14s 374us/step - loss: 125.8313 - dense_3_loss: 0.0966 - dense_4_loss: 92.5709 - dense_5_loss: 0.0214 - val_loss: 128.3755 - val_dense_3_loss: 0.0988 - val_dense_4_loss: 91.0870 - val_dense_5_loss: 0.0383\n",
            "\n",
            "Epoch 00018: val_loss improved from 133.87716 to 128.37545, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 19/200\n",
            "38000/38000 [==============================] - 14s 375us/step - loss: 123.5967 - dense_3_loss: 0.0954 - dense_4_loss: 90.5064 - dense_5_loss: 0.0224 - val_loss: 126.0970 - val_dense_3_loss: 0.1058 - val_dense_4_loss: 89.1811 - val_dense_5_loss: 0.0259\n",
            "\n",
            "Epoch 00019: val_loss improved from 128.37545 to 126.09698, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 20/200\n",
            "38000/38000 [==============================] - 14s 374us/step - loss: 120.6172 - dense_3_loss: 0.0943 - dense_4_loss: 88.6024 - dense_5_loss: 0.0186 - val_loss: 125.8060 - val_dense_3_loss: 0.1084 - val_dense_4_loss: 87.4095 - val_dense_5_loss: 0.0293\n",
            "\n",
            "Epoch 00020: val_loss improved from 126.09698 to 125.80604, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 21/200\n",
            "38000/38000 [==============================] - 14s 374us/step - loss: 119.7993 - dense_3_loss: 0.0963 - dense_4_loss: 86.7799 - dense_5_loss: 0.0206 - val_loss: 124.4505 - val_dense_3_loss: 0.1025 - val_dense_4_loss: 85.5864 - val_dense_5_loss: 0.0405\n",
            "\n",
            "Epoch 00021: val_loss improved from 125.80604 to 124.45046, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 22/200\n",
            "38000/38000 [==============================] - 14s 376us/step - loss: 118.1681 - dense_3_loss: 0.0950 - dense_4_loss: 85.0766 - dense_5_loss: 0.0230 - val_loss: 117.6381 - val_dense_3_loss: 0.0949 - val_dense_4_loss: 84.0590 - val_dense_5_loss: 0.0256\n",
            "\n",
            "Epoch 00022: val_loss improved from 124.45046 to 117.63806, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 23/200\n",
            "38000/38000 [==============================] - 14s 375us/step - loss: 115.0835 - dense_3_loss: 0.0925 - dense_4_loss: 83.4719 - dense_5_loss: 0.0193 - val_loss: 116.2943 - val_dense_3_loss: 0.0962 - val_dense_4_loss: 82.5780 - val_dense_5_loss: 0.0243\n",
            "\n",
            "Epoch 00023: val_loss improved from 117.63806 to 116.29431, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 24/200\n",
            "38000/38000 [==============================] - 14s 376us/step - loss: 113.6862 - dense_3_loss: 0.0919 - dense_4_loss: 81.9614 - dense_5_loss: 0.0208 - val_loss: 113.4596 - val_dense_3_loss: 0.0936 - val_dense_4_loss: 81.0740 - val_dense_5_loss: 0.0216\n",
            "\n",
            "Epoch 00024: val_loss improved from 116.29431 to 113.45960, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 25/200\n",
            "38000/38000 [==============================] - 14s 376us/step - loss: 111.1826 - dense_3_loss: 0.0910 - dense_4_loss: 80.5281 - dense_5_loss: 0.0167 - val_loss: 110.8515 - val_dense_3_loss: 0.0933 - val_dense_4_loss: 79.7059 - val_dense_5_loss: 0.0158\n",
            "\n",
            "Epoch 00025: val_loss improved from 113.45960 to 110.85154, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 26/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 110.4702 - dense_3_loss: 0.0938 - dense_4_loss: 79.1408 - dense_5_loss: 0.0159 - val_loss: 111.8788 - val_dense_3_loss: 0.0957 - val_dense_4_loss: 78.4498 - val_dense_5_loss: 0.0236\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 110.85154\n",
            "Epoch 27/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 109.5688 - dense_3_loss: 0.0948 - dense_4_loss: 77.8363 - dense_5_loss: 0.0165 - val_loss: 115.9839 - val_dense_3_loss: 0.1155 - val_dense_4_loss: 77.0940 - val_dense_5_loss: 0.0212\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 110.85154\n",
            "Epoch 28/200\n",
            "38000/38000 [==============================] - 14s 376us/step - loss: 107.9324 - dense_3_loss: 0.0923 - dense_4_loss: 76.6000 - dense_5_loss: 0.0182 - val_loss: 107.0732 - val_dense_3_loss: 0.0954 - val_dense_4_loss: 75.9590 - val_dense_5_loss: 0.0125\n",
            "\n",
            "Epoch 00028: val_loss improved from 110.85154 to 107.07315, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 29/200\n",
            "38000/38000 [==============================] - 14s 376us/step - loss: 105.8204 - dense_3_loss: 0.0911 - dense_4_loss: 75.4395 - dense_5_loss: 0.0153 - val_loss: 107.6172 - val_dense_3_loss: 0.0960 - val_dense_4_loss: 74.7750 - val_dense_5_loss: 0.0202\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 107.07315\n",
            "Epoch 30/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 104.8824 - dense_3_loss: 0.0919 - dense_4_loss: 74.3087 - dense_5_loss: 0.0150 - val_loss: 105.3516 - val_dense_3_loss: 0.0966 - val_dense_4_loss: 73.7235 - val_dense_5_loss: 0.0132\n",
            "\n",
            "Epoch 00030: val_loss improved from 107.07315 to 105.35163, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 31/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 104.0482 - dense_3_loss: 0.0926 - dense_4_loss: 73.2308 - dense_5_loss: 0.0152 - val_loss: 104.0445 - val_dense_3_loss: 0.0955 - val_dense_4_loss: 72.6639 - val_dense_5_loss: 0.0137\n",
            "\n",
            "Epoch 00031: val_loss improved from 105.35163 to 104.04446, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 32/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 102.2066 - dense_3_loss: 0.0902 - dense_4_loss: 72.2164 - dense_5_loss: 0.0147 - val_loss: 102.5732 - val_dense_3_loss: 0.0938 - val_dense_4_loss: 71.6882 - val_dense_5_loss: 0.0137\n",
            "\n",
            "Epoch 00032: val_loss improved from 104.04446 to 102.57325, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 33/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 101.2330 - dense_3_loss: 0.0903 - dense_4_loss: 71.2374 - dense_5_loss: 0.0145 - val_loss: 102.1862 - val_dense_3_loss: 0.0963 - val_dense_4_loss: 70.7038 - val_dense_5_loss: 0.0129\n",
            "\n",
            "Epoch 00033: val_loss improved from 102.57325 to 102.18618, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 34/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 100.6946 - dense_3_loss: 0.0909 - dense_4_loss: 70.3122 - dense_5_loss: 0.0156 - val_loss: 99.6887 - val_dense_3_loss: 0.0919 - val_dense_4_loss: 69.8395 - val_dense_5_loss: 0.0114\n",
            "\n",
            "Epoch 00034: val_loss improved from 102.18618 to 99.68865, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 35/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 98.9357 - dense_3_loss: 0.0888 - dense_4_loss: 69.4518 - dense_5_loss: 0.0142 - val_loss: 99.7992 - val_dense_3_loss: 0.0929 - val_dense_4_loss: 68.9726 - val_dense_5_loss: 0.0148\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 99.68865\n",
            "Epoch 36/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 97.9274 - dense_3_loss: 0.0886 - dense_4_loss: 68.6140 - dense_5_loss: 0.0136 - val_loss: 100.2729 - val_dense_3_loss: 0.0954 - val_dense_4_loss: 68.2074 - val_dense_5_loss: 0.0172\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 99.68865\n",
            "Epoch 37/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 97.1441 - dense_3_loss: 0.0882 - dense_4_loss: 67.8204 - dense_5_loss: 0.0143 - val_loss: 98.0113 - val_dense_3_loss: 0.0920 - val_dense_4_loss: 67.3932 - val_dense_5_loss: 0.0151\n",
            "\n",
            "Epoch 00037: val_loss improved from 99.68865 to 98.01135, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 38/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 96.5855 - dense_3_loss: 0.0883 - dense_4_loss: 67.0821 - dense_5_loss: 0.0151 - val_loss: 97.7335 - val_dense_3_loss: 0.0956 - val_dense_4_loss: 66.7018 - val_dense_5_loss: 0.0117\n",
            "\n",
            "Epoch 00038: val_loss improved from 98.01135 to 97.73353, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 39/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 95.9653 - dense_3_loss: 0.0892 - dense_4_loss: 66.3659 - dense_5_loss: 0.0142 - val_loss: 96.8305 - val_dense_3_loss: 0.0941 - val_dense_4_loss: 66.0332 - val_dense_5_loss: 0.0129\n",
            "\n",
            "Epoch 00039: val_loss improved from 97.73353 to 96.83047, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 40/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 94.8160 - dense_3_loss: 0.0872 - dense_4_loss: 65.6933 - dense_5_loss: 0.0148 - val_loss: 99.0073 - val_dense_3_loss: 0.0939 - val_dense_4_loss: 65.3569 - val_dense_5_loss: 0.0274\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 96.83047\n",
            "Epoch 41/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 94.2887 - dense_3_loss: 0.0873 - dense_4_loss: 65.0443 - dense_5_loss: 0.0152 - val_loss: 96.9111 - val_dense_3_loss: 0.0963 - val_dense_4_loss: 64.7078 - val_dense_5_loss: 0.0166\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 96.83047\n",
            "Epoch 42/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 93.9310 - dense_3_loss: 0.0890 - dense_4_loss: 64.4211 - dense_5_loss: 0.0140 - val_loss: 94.4499 - val_dense_3_loss: 0.0929 - val_dense_4_loss: 64.0452 - val_dense_5_loss: 0.0126\n",
            "\n",
            "Epoch 00042: val_loss improved from 96.83047 to 94.44986, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 43/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 92.4399 - dense_3_loss: 0.0859 - dense_4_loss: 63.8242 - dense_5_loss: 0.0142 - val_loss: 96.0160 - val_dense_3_loss: 0.1006 - val_dense_4_loss: 63.4822 - val_dense_5_loss: 0.0117\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 94.44986\n",
            "Epoch 44/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 91.9406 - dense_3_loss: 0.0867 - dense_4_loss: 63.2537 - dense_5_loss: 0.0134 - val_loss: 93.4614 - val_dense_3_loss: 0.0939 - val_dense_4_loss: 62.9296 - val_dense_5_loss: 0.0118\n",
            "\n",
            "Epoch 00044: val_loss improved from 94.44986 to 93.46141, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 45/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 91.6977 - dense_3_loss: 0.0883 - dense_4_loss: 62.7105 - dense_5_loss: 0.0125 - val_loss: 95.5222 - val_dense_3_loss: 0.0973 - val_dense_4_loss: 62.3479 - val_dense_5_loss: 0.0199\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 93.46141\n",
            "Epoch 46/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 90.3474 - dense_3_loss: 0.0850 - dense_4_loss: 62.1945 - dense_5_loss: 0.0133 - val_loss: 92.4680 - val_dense_3_loss: 0.0936 - val_dense_4_loss: 61.9232 - val_dense_5_loss: 0.0124\n",
            "\n",
            "Epoch 00046: val_loss improved from 93.46141 to 92.46802, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 47/200\n",
            "38000/38000 [==============================] - 14s 379us/step - loss: 90.3412 - dense_3_loss: 0.0843 - dense_4_loss: 61.6693 - dense_5_loss: 0.0169 - val_loss: 91.5986 - val_dense_3_loss: 0.0933 - val_dense_4_loss: 61.3983 - val_dense_5_loss: 0.0111\n",
            "\n",
            "Epoch 00047: val_loss improved from 92.46802 to 91.59861, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 48/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 89.3000 - dense_3_loss: 0.0844 - dense_4_loss: 61.1564 - dense_5_loss: 0.0141 - val_loss: 91.2639 - val_dense_3_loss: 0.0917 - val_dense_4_loss: 60.8686 - val_dense_5_loss: 0.0145\n",
            "\n",
            "Epoch 00048: val_loss improved from 91.59861 to 91.26388, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 49/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 88.8478 - dense_3_loss: 0.0850 - dense_4_loss: 60.6327 - dense_5_loss: 0.0135 - val_loss: 90.7023 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 60.4347 - val_dense_5_loss: 0.0118\n",
            "\n",
            "Epoch 00049: val_loss improved from 91.26388 to 90.70234, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 50/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 88.8367 - dense_3_loss: 0.0856 - dense_4_loss: 60.0936 - dense_5_loss: 0.0153 - val_loss: 93.8426 - val_dense_3_loss: 0.1019 - val_dense_4_loss: 59.7193 - val_dense_5_loss: 0.0178\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 90.70234\n",
            "Epoch 51/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 88.5845 - dense_3_loss: 0.0872 - dense_4_loss: 59.5877 - dense_5_loss: 0.0142 - val_loss: 90.3727 - val_dense_3_loss: 0.0931 - val_dense_4_loss: 59.0251 - val_dense_5_loss: 0.0170\n",
            "\n",
            "Epoch 00051: val_loss improved from 90.70234 to 90.37274, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 52/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 86.6546 - dense_3_loss: 0.0853 - dense_4_loss: 58.0191 - dense_5_loss: 0.0152 - val_loss: 90.5846 - val_dense_3_loss: 0.1013 - val_dense_4_loss: 56.1994 - val_dense_5_loss: 0.0200\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 90.37274\n",
            "Epoch 53/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 84.8851 - dense_3_loss: 0.0877 - dense_4_loss: 55.3456 - dense_5_loss: 0.0161 - val_loss: 85.6770 - val_dense_3_loss: 0.0951 - val_dense_4_loss: 54.0905 - val_dense_5_loss: 0.0153\n",
            "\n",
            "Epoch 00053: val_loss improved from 90.37274 to 85.67705, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 54/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 82.8617 - dense_3_loss: 0.0873 - dense_4_loss: 53.0286 - dense_5_loss: 0.0182 - val_loss: 87.7107 - val_dense_3_loss: 0.1001 - val_dense_4_loss: 51.7147 - val_dense_5_loss: 0.0298\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 85.67705\n",
            "Epoch 55/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 80.7080 - dense_3_loss: 0.0869 - dense_4_loss: 50.6818 - dense_5_loss: 0.0198 - val_loss: 82.2949 - val_dense_3_loss: 0.0963 - val_dense_4_loss: 49.9566 - val_dense_5_loss: 0.0172\n",
            "\n",
            "Epoch 00055: val_loss improved from 85.67705 to 82.29493, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 56/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 78.2331 - dense_3_loss: 0.0868 - dense_4_loss: 48.3981 - dense_5_loss: 0.0190 - val_loss: 82.1234 - val_dense_3_loss: 0.0992 - val_dense_4_loss: 47.3385 - val_dense_5_loss: 0.0252\n",
            "\n",
            "Epoch 00056: val_loss improved from 82.29493 to 82.12340, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 57/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 76.8657 - dense_3_loss: 0.0869 - dense_4_loss: 46.7668 - dense_5_loss: 0.0202 - val_loss: 79.9728 - val_dense_3_loss: 0.1029 - val_dense_4_loss: 45.9856 - val_dense_5_loss: 0.0156\n",
            "\n",
            "Epoch 00057: val_loss improved from 82.12340 to 79.97281, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 58/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 74.1919 - dense_3_loss: 0.0861 - dense_4_loss: 45.1391 - dense_5_loss: 0.0161 - val_loss: 77.4485 - val_dense_3_loss: 0.0987 - val_dense_4_loss: 44.5275 - val_dense_5_loss: 0.0165\n",
            "\n",
            "Epoch 00058: val_loss improved from 79.97281 to 77.44851, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 59/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 72.9884 - dense_3_loss: 0.0857 - dense_4_loss: 43.8002 - dense_5_loss: 0.0174 - val_loss: 74.8833 - val_dense_3_loss: 0.0972 - val_dense_4_loss: 43.1342 - val_dense_5_loss: 0.0129\n",
            "\n",
            "Epoch 00059: val_loss improved from 77.44851 to 74.88328, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 60/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 71.0843 - dense_3_loss: 0.0845 - dense_4_loss: 42.4897 - dense_5_loss: 0.0162 - val_loss: 74.5127 - val_dense_3_loss: 0.0975 - val_dense_4_loss: 41.7746 - val_dense_5_loss: 0.0175\n",
            "\n",
            "Epoch 00060: val_loss improved from 74.88328 to 74.51273, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 61/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 70.4441 - dense_3_loss: 0.0855 - dense_4_loss: 41.3903 - dense_5_loss: 0.0170 - val_loss: 72.9792 - val_dense_3_loss: 0.0949 - val_dense_4_loss: 40.6750 - val_dense_5_loss: 0.0191\n",
            "\n",
            "Epoch 00061: val_loss improved from 74.51273 to 72.97917, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 62/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 68.5981 - dense_3_loss: 0.0832 - dense_4_loss: 40.2426 - dense_5_loss: 0.0170 - val_loss: 71.0719 - val_dense_3_loss: 0.0968 - val_dense_4_loss: 39.5094 - val_dense_5_loss: 0.0126\n",
            "\n",
            "Epoch 00062: val_loss improved from 72.97917 to 71.07188, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 63/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 67.2601 - dense_3_loss: 0.0821 - dense_4_loss: 39.0190 - dense_5_loss: 0.0180 - val_loss: 69.6818 - val_dense_3_loss: 0.0949 - val_dense_4_loss: 38.4460 - val_dense_5_loss: 0.0139\n",
            "\n",
            "Epoch 00063: val_loss improved from 71.07188 to 69.68178, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 64/200\n",
            "38000/38000 [==============================] - 14s 380us/step - loss: 65.1805 - dense_3_loss: 0.0809 - dense_4_loss: 37.9687 - dense_5_loss: 0.0148 - val_loss: 72.6984 - val_dense_3_loss: 0.1050 - val_dense_4_loss: 37.6274 - val_dense_5_loss: 0.0178\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 69.68178\n",
            "Epoch 65/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 65.2534 - dense_3_loss: 0.0839 - dense_4_loss: 37.0387 - dense_5_loss: 0.0153 - val_loss: 69.2910 - val_dense_3_loss: 0.0964 - val_dense_4_loss: 36.8384 - val_dense_5_loss: 0.0177\n",
            "\n",
            "Epoch 00065: val_loss improved from 69.68178 to 69.29102, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 66/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 64.5858 - dense_3_loss: 0.0841 - dense_4_loss: 36.2808 - dense_5_loss: 0.0154 - val_loss: 68.0003 - val_dense_3_loss: 0.0948 - val_dense_4_loss: 35.7384 - val_dense_5_loss: 0.0191\n",
            "\n",
            "Epoch 00066: val_loss improved from 69.29102 to 68.00028, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 67/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 62.3379 - dense_3_loss: 0.0792 - dense_4_loss: 35.3704 - dense_5_loss: 0.0160 - val_loss: 67.5040 - val_dense_3_loss: 0.1006 - val_dense_4_loss: 34.8977 - val_dense_5_loss: 0.0121\n",
            "\n",
            "Epoch 00067: val_loss improved from 68.00028 to 67.50403, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 68/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 61.9366 - dense_3_loss: 0.0818 - dense_4_loss: 34.4527 - dense_5_loss: 0.0148 - val_loss: 73.3484 - val_dense_3_loss: 0.1179 - val_dense_4_loss: 34.4613 - val_dense_5_loss: 0.0176\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 67.50403\n",
            "Epoch 69/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 60.8580 - dense_3_loss: 0.0804 - dense_4_loss: 33.6930 - dense_5_loss: 0.0152 - val_loss: 66.2321 - val_dense_3_loss: 0.0972 - val_dense_4_loss: 33.1455 - val_dense_5_loss: 0.0197\n",
            "\n",
            "Epoch 00069: val_loss improved from 67.50403 to 66.23210, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 70/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 59.9159 - dense_3_loss: 0.0787 - dense_4_loss: 33.0098 - dense_5_loss: 0.0165 - val_loss: 63.9268 - val_dense_3_loss: 0.0958 - val_dense_4_loss: 32.5512 - val_dense_5_loss: 0.0132\n",
            "\n",
            "Epoch 00070: val_loss improved from 66.23210 to 63.92682, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 71/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 58.3057 - dense_3_loss: 0.0776 - dense_4_loss: 32.1616 - dense_5_loss: 0.0144 - val_loss: 64.9988 - val_dense_3_loss: 0.1006 - val_dense_4_loss: 32.1022 - val_dense_5_loss: 0.0136\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 63.92682\n",
            "Epoch 72/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 59.2207 - dense_3_loss: 0.0820 - dense_4_loss: 31.5384 - dense_5_loss: 0.0154 - val_loss: 62.6374 - val_dense_3_loss: 0.0960 - val_dense_4_loss: 31.2394 - val_dense_5_loss: 0.0129\n",
            "\n",
            "Epoch 00072: val_loss improved from 63.92682 to 62.63744, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 73/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 57.5086 - dense_3_loss: 0.0772 - dense_4_loss: 30.9238 - dense_5_loss: 0.0171 - val_loss: 64.3032 - val_dense_3_loss: 0.0995 - val_dense_4_loss: 31.1009 - val_dense_5_loss: 0.0168\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 62.63744\n",
            "Epoch 74/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 57.0644 - dense_3_loss: 0.0782 - dense_4_loss: 30.2455 - dense_5_loss: 0.0168 - val_loss: 64.3807 - val_dense_3_loss: 0.0994 - val_dense_4_loss: 30.9115 - val_dense_5_loss: 0.0182\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 62.63744\n",
            "Epoch 75/200\n",
            "38000/38000 [==============================] - 14s 376us/step - loss: 56.4751 - dense_3_loss: 0.0783 - dense_4_loss: 29.6889 - dense_5_loss: 0.0165 - val_loss: 62.9559 - val_dense_3_loss: 0.0962 - val_dense_4_loss: 30.1650 - val_dense_5_loss: 0.0197\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 62.63744\n",
            "Epoch 76/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 55.0009 - dense_3_loss: 0.0755 - dense_4_loss: 29.0974 - dense_5_loss: 0.0162 - val_loss: 65.5251 - val_dense_3_loss: 0.1061 - val_dense_4_loss: 30.3731 - val_dense_5_loss: 0.0166\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 62.63744\n",
            "Epoch 77/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 53.9376 - dense_3_loss: 0.0750 - dense_4_loss: 28.5761 - dense_5_loss: 0.0143 - val_loss: 59.0851 - val_dense_3_loss: 0.0939 - val_dense_4_loss: 28.4288 - val_dense_5_loss: 0.0124\n",
            "\n",
            "Epoch 00077: val_loss improved from 62.63744 to 59.08508, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 78/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 53.2302 - dense_3_loss: 0.0746 - dense_4_loss: 27.9274 - dense_5_loss: 0.0147 - val_loss: 58.9800 - val_dense_3_loss: 0.0948 - val_dense_4_loss: 27.7936 - val_dense_5_loss: 0.0138\n",
            "\n",
            "Epoch 00078: val_loss improved from 59.08508 to 58.98000, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 79/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 52.3231 - dense_3_loss: 0.0733 - dense_4_loss: 27.4233 - dense_5_loss: 0.0146 - val_loss: 63.2057 - val_dense_3_loss: 0.0969 - val_dense_4_loss: 27.6623 - val_dense_5_loss: 0.0324\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 58.98000\n",
            "Epoch 80/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 52.2891 - dense_3_loss: 0.0730 - dense_4_loss: 27.0251 - dense_5_loss: 0.0168 - val_loss: 57.1365 - val_dense_3_loss: 0.0919 - val_dense_4_loss: 26.4472 - val_dense_5_loss: 0.0156\n",
            "\n",
            "Epoch 00080: val_loss improved from 58.98000 to 57.13655, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 81/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 51.2370 - dense_3_loss: 0.0718 - dense_4_loss: 26.5282 - dense_5_loss: 0.0158 - val_loss: 57.8183 - val_dense_3_loss: 0.0927 - val_dense_4_loss: 27.0596 - val_dense_5_loss: 0.0147\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 57.13655\n",
            "Epoch 82/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 50.1409 - dense_3_loss: 0.0705 - dense_4_loss: 26.0366 - dense_5_loss: 0.0147 - val_loss: 59.4824 - val_dense_3_loss: 0.0960 - val_dense_4_loss: 26.2849 - val_dense_5_loss: 0.0219\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 57.13655\n",
            "Epoch 83/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 49.6668 - dense_3_loss: 0.0706 - dense_4_loss: 25.7249 - dense_5_loss: 0.0139 - val_loss: 58.9196 - val_dense_3_loss: 0.0953 - val_dense_4_loss: 25.4788 - val_dense_5_loss: 0.0243\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 57.13655\n",
            "Epoch 84/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 49.1947 - dense_3_loss: 0.0700 - dense_4_loss: 25.2381 - dense_5_loss: 0.0147 - val_loss: 55.2409 - val_dense_3_loss: 0.0927 - val_dense_4_loss: 25.1268 - val_dense_5_loss: 0.0116\n",
            "\n",
            "Epoch 00084: val_loss improved from 57.13655 to 55.24089, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 85/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 49.0997 - dense_3_loss: 0.0717 - dense_4_loss: 24.9230 - dense_5_loss: 0.0134 - val_loss: 59.7060 - val_dense_3_loss: 0.1052 - val_dense_4_loss: 25.7525 - val_dense_5_loss: 0.0119\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 55.24089\n",
            "Epoch 86/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 48.6276 - dense_3_loss: 0.0711 - dense_4_loss: 24.4780 - dense_5_loss: 0.0141 - val_loss: 55.8588 - val_dense_3_loss: 0.0946 - val_dense_4_loss: 25.0704 - val_dense_5_loss: 0.0120\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 55.24089\n",
            "Epoch 87/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 48.4848 - dense_3_loss: 0.0703 - dense_4_loss: 24.1969 - dense_5_loss: 0.0160 - val_loss: 55.2109 - val_dense_3_loss: 0.0947 - val_dense_4_loss: 23.9113 - val_dense_5_loss: 0.0144\n",
            "\n",
            "Epoch 00087: val_loss improved from 55.24089 to 55.21089, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 88/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 47.4390 - dense_3_loss: 0.0695 - dense_4_loss: 23.7776 - dense_5_loss: 0.0141 - val_loss: 58.6939 - val_dense_3_loss: 0.1009 - val_dense_4_loss: 24.4378 - val_dense_5_loss: 0.0199\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 55.21089\n",
            "Epoch 89/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 47.3895 - dense_3_loss: 0.0694 - dense_4_loss: 23.6319 - dense_5_loss: 0.0147 - val_loss: 54.7098 - val_dense_3_loss: 0.0942 - val_dense_4_loss: 23.7093 - val_dense_5_loss: 0.0138\n",
            "\n",
            "Epoch 00089: val_loss improved from 55.21089 to 54.70975, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 90/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 45.8539 - dense_3_loss: 0.0664 - dense_4_loss: 23.1163 - dense_5_loss: 0.0140 - val_loss: 55.1915 - val_dense_3_loss: 0.0969 - val_dense_4_loss: 22.9970 - val_dense_5_loss: 0.0156\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 54.70975\n",
            "Epoch 91/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 45.9841 - dense_3_loss: 0.0673 - dense_4_loss: 22.9136 - dense_5_loss: 0.0143 - val_loss: 53.9961 - val_dense_3_loss: 0.0947 - val_dense_4_loss: 23.2842 - val_dense_5_loss: 0.0115\n",
            "\n",
            "Epoch 00091: val_loss improved from 54.70975 to 53.99614, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 92/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 45.0802 - dense_3_loss: 0.0661 - dense_4_loss: 22.5336 - dense_5_loss: 0.0137 - val_loss: 53.3023 - val_dense_3_loss: 0.0950 - val_dense_4_loss: 22.5070 - val_dense_5_loss: 0.0115\n",
            "\n",
            "Epoch 00092: val_loss improved from 53.99614 to 53.30234, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 93/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 45.1437 - dense_3_loss: 0.0654 - dense_4_loss: 22.4206 - dense_5_loss: 0.0155 - val_loss: 53.9362 - val_dense_3_loss: 0.0962 - val_dense_4_loss: 22.3567 - val_dense_5_loss: 0.0136\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 53.30234\n",
            "Epoch 94/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 44.3490 - dense_3_loss: 0.0647 - dense_4_loss: 22.0872 - dense_5_loss: 0.0143 - val_loss: 53.3824 - val_dense_3_loss: 0.0966 - val_dense_4_loss: 22.0899 - val_dense_5_loss: 0.0116\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 53.30234\n",
            "Epoch 95/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 43.6515 - dense_3_loss: 0.0635 - dense_4_loss: 21.7864 - dense_5_loss: 0.0141 - val_loss: 53.3513 - val_dense_3_loss: 0.0944 - val_dense_4_loss: 22.3143 - val_dense_5_loss: 0.0135\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 53.30234\n",
            "Epoch 96/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 44.1450 - dense_3_loss: 0.0658 - dense_4_loss: 21.6341 - dense_5_loss: 0.0139 - val_loss: 52.6713 - val_dense_3_loss: 0.0935 - val_dense_4_loss: 21.7341 - val_dense_5_loss: 0.0145\n",
            "\n",
            "Epoch 00096: val_loss improved from 53.30234 to 52.67134, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 97/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 43.0730 - dense_3_loss: 0.0632 - dense_4_loss: 21.4036 - dense_5_loss: 0.0136 - val_loss: 53.9249 - val_dense_3_loss: 0.0961 - val_dense_4_loss: 21.6380 - val_dense_5_loss: 0.0173\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 52.67134\n",
            "Epoch 98/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 44.1923 - dense_3_loss: 0.0668 - dense_4_loss: 21.1940 - dense_5_loss: 0.0148 - val_loss: 52.0517 - val_dense_3_loss: 0.0942 - val_dense_4_loss: 21.3992 - val_dense_5_loss: 0.0120\n",
            "\n",
            "Epoch 00098: val_loss improved from 52.67134 to 52.05167, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 99/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 43.6530 - dense_3_loss: 0.0655 - dense_4_loss: 21.0516 - dense_5_loss: 0.0148 - val_loss: 53.3456 - val_dense_3_loss: 0.0969 - val_dense_4_loss: 21.1950 - val_dense_5_loss: 0.0153\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 52.05167\n",
            "Epoch 100/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 43.0169 - dense_3_loss: 0.0637 - dense_4_loss: 20.9074 - dense_5_loss: 0.0150 - val_loss: 53.1740 - val_dense_3_loss: 0.0947 - val_dense_4_loss: 21.5437 - val_dense_5_loss: 0.0161\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 52.05167\n",
            "Epoch 101/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 42.1103 - dense_3_loss: 0.0620 - dense_4_loss: 20.6984 - dense_5_loss: 0.0141 - val_loss: 51.5766 - val_dense_3_loss: 0.0942 - val_dense_4_loss: 20.9185 - val_dense_5_loss: 0.0120\n",
            "\n",
            "Epoch 00101: val_loss improved from 52.05167 to 51.57663, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 102/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 41.7382 - dense_3_loss: 0.0608 - dense_4_loss: 20.4537 - dense_5_loss: 0.0152 - val_loss: 53.7252 - val_dense_3_loss: 0.0927 - val_dense_4_loss: 22.0963 - val_dense_5_loss: 0.0191\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 51.57663\n",
            "Epoch 103/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 40.9716 - dense_3_loss: 0.0598 - dense_4_loss: 20.2633 - dense_5_loss: 0.0139 - val_loss: 52.5142 - val_dense_3_loss: 0.0943 - val_dense_4_loss: 21.0854 - val_dense_5_loss: 0.0157\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 51.57663\n",
            "Epoch 104/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 41.4776 - dense_3_loss: 0.0619 - dense_4_loss: 20.0997 - dense_5_loss: 0.0141 - val_loss: 55.2833 - val_dense_3_loss: 0.1052 - val_dense_4_loss: 20.9183 - val_dense_5_loss: 0.0140\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 51.57663\n",
            "Epoch 105/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 41.4229 - dense_3_loss: 0.0621 - dense_4_loss: 20.0348 - dense_5_loss: 0.0138 - val_loss: 58.7737 - val_dense_3_loss: 0.1164 - val_dense_4_loss: 20.8820 - val_dense_5_loss: 0.0149\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 51.57663\n",
            "Epoch 106/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 42.2401 - dense_3_loss: 0.0643 - dense_4_loss: 19.9329 - dense_5_loss: 0.0150 - val_loss: 51.2923 - val_dense_3_loss: 0.0940 - val_dense_4_loss: 20.4941 - val_dense_5_loss: 0.0130\n",
            "\n",
            "Epoch 00106: val_loss improved from 51.57663 to 51.29232, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 107/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 40.4404 - dense_3_loss: 0.0587 - dense_4_loss: 19.7060 - dense_5_loss: 0.0156 - val_loss: 51.7844 - val_dense_3_loss: 0.0932 - val_dense_4_loss: 20.3517 - val_dense_5_loss: 0.0173\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 51.29232\n",
            "Epoch 108/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 40.0581 - dense_3_loss: 0.0589 - dense_4_loss: 19.4822 - dense_5_loss: 0.0145 - val_loss: 53.0161 - val_dense_3_loss: 0.0944 - val_dense_4_loss: 21.8279 - val_dense_5_loss: 0.0143\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 51.29232\n",
            "Epoch 109/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 40.2264 - dense_3_loss: 0.0588 - dense_4_loss: 19.3905 - dense_5_loss: 0.0160 - val_loss: 50.5098 - val_dense_3_loss: 0.0934 - val_dense_4_loss: 20.0102 - val_dense_5_loss: 0.0124\n",
            "\n",
            "Epoch 00109: val_loss improved from 51.29232 to 50.50980, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 110/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 39.8171 - dense_3_loss: 0.0588 - dense_4_loss: 19.1739 - dense_5_loss: 0.0151 - val_loss: 53.0976 - val_dense_3_loss: 0.0988 - val_dense_4_loss: 20.3697 - val_dense_5_loss: 0.0155\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 50.50980\n",
            "Epoch 111/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 39.9805 - dense_3_loss: 0.0601 - dense_4_loss: 19.2338 - dense_5_loss: 0.0135 - val_loss: 50.7833 - val_dense_3_loss: 0.0949 - val_dense_4_loss: 19.7400 - val_dense_5_loss: 0.0129\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 50.50980\n",
            "Epoch 112/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 39.7624 - dense_3_loss: 0.0582 - dense_4_loss: 19.2243 - dense_5_loss: 0.0153 - val_loss: 51.6107 - val_dense_3_loss: 0.0942 - val_dense_4_loss: 21.1293 - val_dense_5_loss: 0.0111\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 50.50980\n",
            "Epoch 113/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 38.4289 - dense_3_loss: 0.0567 - dense_4_loss: 18.8620 - dense_5_loss: 0.0128 - val_loss: 55.6641 - val_dense_3_loss: 0.1096 - val_dense_4_loss: 20.2932 - val_dense_5_loss: 0.0124\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 50.50980\n",
            "Epoch 114/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 39.8150 - dense_3_loss: 0.0606 - dense_4_loss: 18.9696 - dense_5_loss: 0.0133 - val_loss: 50.8921 - val_dense_3_loss: 0.0936 - val_dense_4_loss: 19.5245 - val_dense_5_loss: 0.0164\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 50.50980\n",
            "Epoch 115/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 38.3464 - dense_3_loss: 0.0560 - dense_4_loss: 18.6103 - dense_5_loss: 0.0147 - val_loss: 51.4153 - val_dense_3_loss: 0.0956 - val_dense_4_loss: 19.4640 - val_dense_5_loss: 0.0164\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 50.50980\n",
            "Epoch 116/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 38.1377 - dense_3_loss: 0.0559 - dense_4_loss: 18.5204 - dense_5_loss: 0.0143 - val_loss: 49.6426 - val_dense_3_loss: 0.0919 - val_dense_4_loss: 19.3088 - val_dense_5_loss: 0.0139\n",
            "\n",
            "Epoch 00116: val_loss improved from 50.50980 to 49.64255, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 117/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 37.5475 - dense_3_loss: 0.0548 - dense_4_loss: 18.4282 - dense_5_loss: 0.0133 - val_loss: 53.3267 - val_dense_3_loss: 0.0972 - val_dense_4_loss: 20.0238 - val_dense_5_loss: 0.0207\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 49.64255\n",
            "Epoch 118/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 38.1578 - dense_3_loss: 0.0554 - dense_4_loss: 18.3946 - dense_5_loss: 0.0157 - val_loss: 49.5719 - val_dense_3_loss: 0.0923 - val_dense_4_loss: 19.3841 - val_dense_5_loss: 0.0126\n",
            "\n",
            "Epoch 00118: val_loss improved from 49.64255 to 49.57186, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 119/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 37.7245 - dense_3_loss: 0.0541 - dense_4_loss: 18.2778 - dense_5_loss: 0.0161 - val_loss: 49.5822 - val_dense_3_loss: 0.0925 - val_dense_4_loss: 19.3236 - val_dense_5_loss: 0.0125\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 49.57186\n",
            "Epoch 120/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 36.9554 - dense_3_loss: 0.0534 - dense_4_loss: 18.0464 - dense_5_loss: 0.0144 - val_loss: 49.3789 - val_dense_3_loss: 0.0916 - val_dense_4_loss: 19.4089 - val_dense_5_loss: 0.0124\n",
            "\n",
            "Epoch 00120: val_loss improved from 49.57186 to 49.37887, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 121/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 36.6569 - dense_3_loss: 0.0535 - dense_4_loss: 18.0851 - dense_5_loss: 0.0125 - val_loss: 51.7053 - val_dense_3_loss: 0.1004 - val_dense_4_loss: 19.2403 - val_dense_5_loss: 0.0117\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 49.37887\n",
            "Epoch 122/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 37.5143 - dense_3_loss: 0.0558 - dense_4_loss: 18.0486 - dense_5_loss: 0.0136 - val_loss: 49.7596 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 19.0142 - val_dense_5_loss: 0.0142\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 49.37887\n",
            "Epoch 123/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 36.4998 - dense_3_loss: 0.0533 - dense_4_loss: 17.8797 - dense_5_loss: 0.0132 - val_loss: 50.9354 - val_dense_3_loss: 0.0977 - val_dense_4_loss: 18.9279 - val_dense_5_loss: 0.0135\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 49.37887\n",
            "Epoch 124/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 37.8083 - dense_3_loss: 0.0578 - dense_4_loss: 17.7613 - dense_5_loss: 0.0135 - val_loss: 49.1686 - val_dense_3_loss: 0.0931 - val_dense_4_loss: 18.8746 - val_dense_5_loss: 0.0118\n",
            "\n",
            "Epoch 00124: val_loss improved from 49.37887 to 49.16859, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 125/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 36.5090 - dense_3_loss: 0.0519 - dense_4_loss: 17.6955 - dense_5_loss: 0.0162 - val_loss: 50.1411 - val_dense_3_loss: 0.0936 - val_dense_4_loss: 19.2489 - val_dense_5_loss: 0.0141\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 49.16859\n",
            "Epoch 126/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 36.3351 - dense_3_loss: 0.0522 - dense_4_loss: 17.6691 - dense_5_loss: 0.0151 - val_loss: 51.0325 - val_dense_3_loss: 0.0936 - val_dense_4_loss: 19.3356 - val_dense_5_loss: 0.0181\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 49.16859\n",
            "Epoch 127/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 36.6358 - dense_3_loss: 0.0521 - dense_4_loss: 17.5250 - dense_5_loss: 0.0174 - val_loss: 50.3523 - val_dense_3_loss: 0.0948 - val_dense_4_loss: 19.0868 - val_dense_5_loss: 0.0141\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 49.16859\n",
            "Epoch 128/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 36.5822 - dense_3_loss: 0.0534 - dense_4_loss: 17.6579 - dense_5_loss: 0.0146 - val_loss: 50.1335 - val_dense_3_loss: 0.0949 - val_dense_4_loss: 18.8570 - val_dense_5_loss: 0.0140\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 49.16859\n",
            "Epoch 129/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 36.5900 - dense_3_loss: 0.0531 - dense_4_loss: 17.6266 - dense_5_loss: 0.0151 - val_loss: 51.3094 - val_dense_3_loss: 0.0977 - val_dense_4_loss: 19.3337 - val_dense_5_loss: 0.0133\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 49.16859\n",
            "Epoch 130/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 35.6924 - dense_3_loss: 0.0521 - dense_4_loss: 17.3703 - dense_5_loss: 0.0135 - val_loss: 50.8599 - val_dense_3_loss: 0.0941 - val_dense_4_loss: 19.6304 - val_dense_5_loss: 0.0149\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 49.16859\n",
            "Epoch 131/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 35.0387 - dense_3_loss: 0.0505 - dense_4_loss: 17.3349 - dense_5_loss: 0.0128 - val_loss: 51.0778 - val_dense_3_loss: 0.0942 - val_dense_4_loss: 19.2120 - val_dense_5_loss: 0.0180\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 49.16859\n",
            "Epoch 132/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 36.3960 - dense_3_loss: 0.0530 - dense_4_loss: 17.3326 - dense_5_loss: 0.0158 - val_loss: 50.6802 - val_dense_3_loss: 0.0981 - val_dense_4_loss: 18.6058 - val_dense_5_loss: 0.0132\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 49.16859\n",
            "Epoch 133/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 35.5311 - dense_3_loss: 0.0522 - dense_4_loss: 17.2610 - dense_5_loss: 0.0131 - val_loss: 50.7864 - val_dense_3_loss: 0.0966 - val_dense_4_loss: 19.1946 - val_dense_5_loss: 0.0131\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 49.16859\n",
            "Epoch 134/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 34.5055 - dense_3_loss: 0.0496 - dense_4_loss: 17.0025 - dense_5_loss: 0.0131 - val_loss: 52.5785 - val_dense_3_loss: 0.0995 - val_dense_4_loss: 19.7853 - val_dense_5_loss: 0.0146\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 49.16859\n",
            "Epoch 135/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 35.1137 - dense_3_loss: 0.0514 - dense_4_loss: 17.0596 - dense_5_loss: 0.0132 - val_loss: 48.9557 - val_dense_3_loss: 0.0935 - val_dense_4_loss: 18.6201 - val_dense_5_loss: 0.0114\n",
            "\n",
            "Epoch 00135: val_loss improved from 49.16859 to 48.95567, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 136/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 33.9001 - dense_3_loss: 0.0485 - dense_4_loss: 16.8340 - dense_5_loss: 0.0126 - val_loss: 48.9889 - val_dense_3_loss: 0.0921 - val_dense_4_loss: 18.6227 - val_dense_5_loss: 0.0137\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 48.95567\n",
            "Epoch 137/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 34.8582 - dense_3_loss: 0.0518 - dense_4_loss: 16.7981 - dense_5_loss: 0.0125 - val_loss: 49.8922 - val_dense_3_loss: 0.0912 - val_dense_4_loss: 19.2457 - val_dense_5_loss: 0.0165\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 48.95567\n",
            "Epoch 138/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 34.0401 - dense_3_loss: 0.0484 - dense_4_loss: 16.9144 - dense_5_loss: 0.0131 - val_loss: 49.7847 - val_dense_3_loss: 0.0945 - val_dense_4_loss: 18.7463 - val_dense_5_loss: 0.0135\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 48.95567\n",
            "Epoch 139/200\n",
            "38000/38000 [==============================] - 14s 376us/step - loss: 34.2528 - dense_3_loss: 0.0496 - dense_4_loss: 16.8093 - dense_5_loss: 0.0129 - val_loss: 49.8313 - val_dense_3_loss: 0.0945 - val_dense_4_loss: 18.9165 - val_dense_5_loss: 0.0128\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 48.95567\n",
            "Epoch 140/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 35.1210 - dense_3_loss: 0.0520 - dense_4_loss: 16.8309 - dense_5_loss: 0.0135 - val_loss: 54.5863 - val_dense_3_loss: 0.1057 - val_dense_4_loss: 19.6215 - val_dense_5_loss: 0.0163\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 48.95567\n",
            "Epoch 141/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 34.2484 - dense_3_loss: 0.0500 - dense_4_loss: 16.6764 - dense_5_loss: 0.0128 - val_loss: 50.5977 - val_dense_3_loss: 0.0947 - val_dense_4_loss: 18.5594 - val_dense_5_loss: 0.0182\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 48.95567\n",
            "Epoch 142/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 34.0598 - dense_3_loss: 0.0492 - dense_4_loss: 16.5277 - dense_5_loss: 0.0138 - val_loss: 51.0487 - val_dense_3_loss: 0.0958 - val_dense_4_loss: 18.8353 - val_dense_5_loss: 0.0173\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 48.95567\n",
            "Epoch 143/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 34.4693 - dense_3_loss: 0.0510 - dense_4_loss: 16.5359 - dense_5_loss: 0.0132 - val_loss: 51.5413 - val_dense_3_loss: 0.0990 - val_dense_4_loss: 18.7297 - val_dense_5_loss: 0.0156\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 48.95567\n",
            "Epoch 144/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 33.7811 - dense_3_loss: 0.0493 - dense_4_loss: 16.3009 - dense_5_loss: 0.0134 - val_loss: 49.9554 - val_dense_3_loss: 0.0925 - val_dense_4_loss: 18.5464 - val_dense_5_loss: 0.0184\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 48.95567\n",
            "Epoch 145/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 33.5033 - dense_3_loss: 0.0480 - dense_4_loss: 16.3645 - dense_5_loss: 0.0137 - val_loss: 51.1728 - val_dense_3_loss: 0.0986 - val_dense_4_loss: 18.9846 - val_dense_5_loss: 0.0130\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 48.95567\n",
            "Epoch 146/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 33.9500 - dense_3_loss: 0.0494 - dense_4_loss: 16.3029 - dense_5_loss: 0.0141 - val_loss: 48.8783 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 18.6337 - val_dense_5_loss: 0.0117\n",
            "\n",
            "Epoch 00146: val_loss improved from 48.95567 to 48.87827, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 147/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 33.0258 - dense_3_loss: 0.0481 - dense_4_loss: 16.0948 - dense_5_loss: 0.0126 - val_loss: 50.2046 - val_dense_3_loss: 0.0970 - val_dense_4_loss: 18.3513 - val_dense_5_loss: 0.0138\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 48.87827\n",
            "Epoch 148/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 33.0013 - dense_3_loss: 0.0481 - dense_4_loss: 16.1515 - dense_5_loss: 0.0121 - val_loss: 51.6617 - val_dense_3_loss: 0.0998 - val_dense_4_loss: 19.2035 - val_dense_5_loss: 0.0126\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 48.87827\n",
            "Epoch 149/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 34.8829 - dense_3_loss: 0.0526 - dense_4_loss: 16.2210 - dense_5_loss: 0.0144 - val_loss: 48.7137 - val_dense_3_loss: 0.0937 - val_dense_4_loss: 18.3422 - val_dense_5_loss: 0.0114\n",
            "\n",
            "Epoch 00149: val_loss improved from 48.87827 to 48.71369, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 150/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 32.3927 - dense_3_loss: 0.0468 - dense_4_loss: 16.0303 - dense_5_loss: 0.0117 - val_loss: 51.4672 - val_dense_3_loss: 0.0952 - val_dense_4_loss: 19.9545 - val_dense_5_loss: 0.0148\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 48.71369\n",
            "Epoch 151/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 33.1652 - dense_3_loss: 0.0482 - dense_4_loss: 16.0620 - dense_5_loss: 0.0133 - val_loss: 51.6080 - val_dense_3_loss: 0.0945 - val_dense_4_loss: 18.3238 - val_dense_5_loss: 0.0247\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 48.71369\n",
            "Epoch 152/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 33.6233 - dense_3_loss: 0.0473 - dense_4_loss: 15.9179 - dense_5_loss: 0.0176 - val_loss: 48.8038 - val_dense_3_loss: 0.0928 - val_dense_4_loss: 18.7572 - val_dense_5_loss: 0.0110\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 48.71369\n",
            "Epoch 153/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 32.3723 - dense_3_loss: 0.0464 - dense_4_loss: 15.7137 - dense_5_loss: 0.0138 - val_loss: 52.6818 - val_dense_3_loss: 0.0928 - val_dense_4_loss: 18.7342 - val_dense_5_loss: 0.0305\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 48.71369\n",
            "Epoch 154/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 32.4717 - dense_3_loss: 0.0457 - dense_4_loss: 15.7266 - dense_5_loss: 0.0152 - val_loss: 48.2571 - val_dense_3_loss: 0.0914 - val_dense_4_loss: 18.2862 - val_dense_5_loss: 0.0127\n",
            "\n",
            "Epoch 00154: val_loss improved from 48.71369 to 48.25709, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 155/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 31.5510 - dense_3_loss: 0.0448 - dense_4_loss: 15.6338 - dense_5_loss: 0.0124 - val_loss: 50.9750 - val_dense_3_loss: 0.0964 - val_dense_4_loss: 18.7317 - val_dense_5_loss: 0.0167\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 48.25709\n",
            "Epoch 156/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 32.0734 - dense_3_loss: 0.0465 - dense_4_loss: 15.6941 - dense_5_loss: 0.0122 - val_loss: 50.2745 - val_dense_3_loss: 0.0978 - val_dense_4_loss: 18.4401 - val_dense_5_loss: 0.0125\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 48.25709\n",
            "Epoch 157/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 32.3688 - dense_3_loss: 0.0475 - dense_4_loss: 15.6149 - dense_5_loss: 0.0125 - val_loss: 49.6672 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 19.1159 - val_dense_5_loss: 0.0133\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 48.25709\n",
            "Epoch 158/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 31.5487 - dense_3_loss: 0.0451 - dense_4_loss: 15.4660 - dense_5_loss: 0.0128 - val_loss: 51.5701 - val_dense_3_loss: 0.0917 - val_dense_4_loss: 18.2676 - val_dense_5_loss: 0.0290\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 48.25709\n",
            "Epoch 159/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 32.2129 - dense_3_loss: 0.0468 - dense_4_loss: 15.4329 - dense_5_loss: 0.0137 - val_loss: 53.0234 - val_dense_3_loss: 0.1023 - val_dense_4_loss: 18.7393 - val_dense_5_loss: 0.0180\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 48.25709\n",
            "Epoch 160/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 32.8278 - dense_3_loss: 0.0490 - dense_4_loss: 15.5354 - dense_5_loss: 0.0129 - val_loss: 49.7832 - val_dense_3_loss: 0.0976 - val_dense_4_loss: 18.1967 - val_dense_5_loss: 0.0115\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 48.25709\n",
            "Epoch 161/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 31.3885 - dense_3_loss: 0.0457 - dense_4_loss: 15.2444 - dense_5_loss: 0.0122 - val_loss: 47.7225 - val_dense_3_loss: 0.0906 - val_dense_4_loss: 18.3534 - val_dense_5_loss: 0.0110\n",
            "\n",
            "Epoch 00161: val_loss improved from 48.25709 to 47.72247, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 162/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 30.6095 - dense_3_loss: 0.0439 - dense_4_loss: 15.1079 - dense_5_loss: 0.0117 - val_loss: 50.8123 - val_dense_3_loss: 0.1001 - val_dense_4_loss: 18.4472 - val_dense_5_loss: 0.0117\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 47.72247\n",
            "Epoch 163/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 32.8060 - dense_3_loss: 0.0492 - dense_4_loss: 15.4231 - dense_5_loss: 0.0131 - val_loss: 48.8993 - val_dense_3_loss: 0.0950 - val_dense_4_loss: 18.2224 - val_dense_5_loss: 0.0109\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 47.72247\n",
            "Epoch 164/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 31.4256 - dense_3_loss: 0.0453 - dense_4_loss: 15.0442 - dense_5_loss: 0.0140 - val_loss: 48.4020 - val_dense_3_loss: 0.0925 - val_dense_4_loss: 18.2953 - val_dense_5_loss: 0.0118\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 47.72247\n",
            "Epoch 165/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 31.3942 - dense_3_loss: 0.0454 - dense_4_loss: 15.1111 - dense_5_loss: 0.0132 - val_loss: 48.7969 - val_dense_3_loss: 0.0914 - val_dense_4_loss: 18.4383 - val_dense_5_loss: 0.0147\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 47.72247\n",
            "Epoch 166/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 30.8339 - dense_3_loss: 0.0438 - dense_4_loss: 15.0158 - dense_5_loss: 0.0134 - val_loss: 49.1770 - val_dense_3_loss: 0.0937 - val_dense_4_loss: 18.7708 - val_dense_5_loss: 0.0115\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 47.72247\n",
            "Epoch 167/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 30.1029 - dense_3_loss: 0.0434 - dense_4_loss: 14.7665 - dense_5_loss: 0.0116 - val_loss: 52.3461 - val_dense_3_loss: 0.0922 - val_dense_4_loss: 19.2629 - val_dense_5_loss: 0.0271\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 47.72247\n",
            "Epoch 168/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 30.9897 - dense_3_loss: 0.0441 - dense_4_loss: 14.7927 - dense_5_loss: 0.0148 - val_loss: 48.8732 - val_dense_3_loss: 0.0916 - val_dense_4_loss: 18.4058 - val_dense_5_loss: 0.0149\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 47.72247\n",
            "Epoch 169/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 31.1901 - dense_3_loss: 0.0458 - dense_4_loss: 14.8016 - dense_5_loss: 0.0133 - val_loss: 49.7701 - val_dense_3_loss: 0.0909 - val_dense_4_loss: 18.2204 - val_dense_5_loss: 0.0214\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 47.72247\n",
            "Epoch 170/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 30.7842 - dense_3_loss: 0.0435 - dense_4_loss: 14.7726 - dense_5_loss: 0.0148 - val_loss: 47.9229 - val_dense_3_loss: 0.0913 - val_dense_4_loss: 18.3389 - val_dense_5_loss: 0.0110\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 47.72247\n",
            "Epoch 171/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 30.1055 - dense_3_loss: 0.0434 - dense_4_loss: 14.5233 - dense_5_loss: 0.0128 - val_loss: 53.8408 - val_dense_3_loss: 0.1006 - val_dense_4_loss: 18.4010 - val_dense_5_loss: 0.0262\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 47.72247\n",
            "Epoch 172/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 30.8606 - dense_3_loss: 0.0438 - dense_4_loss: 14.5258 - dense_5_loss: 0.0159 - val_loss: 49.7648 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 18.5715 - val_dense_5_loss: 0.0165\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 47.72247\n",
            "Epoch 173/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 30.9356 - dense_3_loss: 0.0457 - dense_4_loss: 14.6000 - dense_5_loss: 0.0131 - val_loss: 48.5309 - val_dense_3_loss: 0.0911 - val_dense_4_loss: 18.7987 - val_dense_5_loss: 0.0120\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 47.72247\n",
            "Epoch 174/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 30.9520 - dense_3_loss: 0.0459 - dense_4_loss: 14.6248 - dense_5_loss: 0.0128 - val_loss: 47.7643 - val_dense_3_loss: 0.0905 - val_dense_4_loss: 18.2683 - val_dense_5_loss: 0.0118\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 47.72247\n",
            "Epoch 175/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 29.8175 - dense_3_loss: 0.0428 - dense_4_loss: 14.4450 - dense_5_loss: 0.0126 - val_loss: 49.0377 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 19.0170 - val_dense_5_loss: 0.0106\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 47.72247\n",
            "Epoch 176/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 31.0771 - dense_3_loss: 0.0468 - dense_4_loss: 14.5440 - dense_5_loss: 0.0124 - val_loss: 48.5636 - val_dense_3_loss: 0.0931 - val_dense_4_loss: 18.3930 - val_dense_5_loss: 0.0113\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 47.72247\n",
            "Epoch 177/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 30.2156 - dense_3_loss: 0.0441 - dense_4_loss: 14.3313 - dense_5_loss: 0.0133 - val_loss: 49.4419 - val_dense_3_loss: 0.0921 - val_dense_4_loss: 18.3639 - val_dense_5_loss: 0.0173\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 47.72247\n",
            "Epoch 178/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 29.6706 - dense_3_loss: 0.0437 - dense_4_loss: 14.1743 - dense_5_loss: 0.0119 - val_loss: 51.6600 - val_dense_3_loss: 0.0974 - val_dense_4_loss: 18.9190 - val_dense_5_loss: 0.0176\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 47.72247\n",
            "Epoch 179/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 29.3211 - dense_3_loss: 0.0424 - dense_4_loss: 14.1473 - dense_5_loss: 0.0122 - val_loss: 50.4508 - val_dense_3_loss: 0.0927 - val_dense_4_loss: 20.0230 - val_dense_5_loss: 0.0132\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 47.72247\n",
            "Epoch 180/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 29.6484 - dense_3_loss: 0.0429 - dense_4_loss: 14.3186 - dense_5_loss: 0.0123 - val_loss: 49.9022 - val_dense_3_loss: 0.0950 - val_dense_4_loss: 19.1247 - val_dense_5_loss: 0.0114\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 47.72247\n",
            "Epoch 181/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 29.7066 - dense_3_loss: 0.0432 - dense_4_loss: 14.2170 - dense_5_loss: 0.0127 - val_loss: 48.7944 - val_dense_3_loss: 0.0934 - val_dense_4_loss: 18.4511 - val_dense_5_loss: 0.0117\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 47.72247\n",
            "Epoch 182/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 30.3181 - dense_3_loss: 0.0450 - dense_4_loss: 14.3050 - dense_5_loss: 0.0125 - val_loss: 48.1977 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 18.2159 - val_dense_5_loss: 0.0105\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 47.72247\n",
            "Epoch 183/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 29.2200 - dense_3_loss: 0.0424 - dense_4_loss: 14.0448 - dense_5_loss: 0.0122 - val_loss: 48.1977 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 18.1923 - val_dense_5_loss: 0.0106\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 47.72247\n",
            "Epoch 184/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 29.0170 - dense_3_loss: 0.0423 - dense_4_loss: 13.8540 - dense_5_loss: 0.0124 - val_loss: 48.3600 - val_dense_3_loss: 0.0923 - val_dense_4_loss: 18.6118 - val_dense_5_loss: 0.0103\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 47.72247\n",
            "Epoch 185/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 29.0210 - dense_3_loss: 0.0417 - dense_4_loss: 14.0004 - dense_5_loss: 0.0125 - val_loss: 49.2048 - val_dense_3_loss: 0.0954 - val_dense_4_loss: 18.2735 - val_dense_5_loss: 0.0115\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 47.72247\n",
            "Epoch 186/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 28.9864 - dense_3_loss: 0.0429 - dense_4_loss: 13.7603 - dense_5_loss: 0.0117 - val_loss: 49.3477 - val_dense_3_loss: 0.0934 - val_dense_4_loss: 18.4156 - val_dense_5_loss: 0.0145\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 47.72247\n",
            "Epoch 187/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 28.9902 - dense_3_loss: 0.0423 - dense_4_loss: 13.7508 - dense_5_loss: 0.0128 - val_loss: 52.3084 - val_dense_3_loss: 0.0983 - val_dense_4_loss: 18.6795 - val_dense_5_loss: 0.0207\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 47.72247\n",
            "Epoch 188/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 29.2338 - dense_3_loss: 0.0434 - dense_4_loss: 13.7475 - dense_5_loss: 0.0124 - val_loss: 48.3554 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 18.1061 - val_dense_5_loss: 0.0118\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 47.72247\n",
            "Epoch 189/200\n",
            "38000/38000 [==============================] - 14s 377us/step - loss: 28.1117 - dense_3_loss: 0.0405 - dense_4_loss: 13.3972 - dense_5_loss: 0.0128 - val_loss: 48.4998 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 18.3218 - val_dense_5_loss: 0.0113\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 47.72247\n",
            "Epoch 190/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 28.5865 - dense_3_loss: 0.0421 - dense_4_loss: 13.5952 - dense_5_loss: 0.0119 - val_loss: 48.4001 - val_dense_3_loss: 0.0927 - val_dense_4_loss: 18.2108 - val_dense_5_loss: 0.0120\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 47.72247\n",
            "Epoch 191/200\n",
            "38000/38000 [==============================] - 14s 378us/step - loss: 28.2633 - dense_3_loss: 0.0409 - dense_4_loss: 13.4379 - dense_5_loss: 0.0128 - val_loss: 52.7027 - val_dense_3_loss: 0.0917 - val_dense_4_loss: 18.9440 - val_dense_5_loss: 0.0312\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 47.72247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PTmTP4dZBWa",
        "colab_type": "code",
        "outputId": "b4480798-4c7d-4f5c-8a78-6bc2c0f55249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "# MODELPATH1=\"CNN_11_model.hdf5\"\n",
        "if os.path.exists(MODELPATH):\n",
        "#     model.load_weights(MODELPATH)\n",
        "      model=load_model(MODELPATH)\n",
        "    # 若成功加载前面保存的参数，输出下列信息\n",
        "      print(\"checkpoint_loaded\")\n",
        "# Y_hat = model.predict(X)\n",
        "# X1 = np.concatenate([x_train1,x_val1])\n",
        "# X2 = np.concatenate([x_train2,x_val2])\n",
        "X_1,X_2 = resize_input(X)\n",
        "Y_hat = model.predict([X_1,X_2])\n",
        "# Y_hat2 = model.predict([x_val1,x_val2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint_loaded\n",
            "X1 shape: (47500, 5000, 1)  X2 shape: (47500, 50, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhhzJldgpGS7",
        "colab_type": "code",
        "outputId": "3a9c1b6a-4d4d-4d03-ab84-dbd3bca0d859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_hat=np.array(Y_hat)\n",
        "Y_hat=Y_hat.reshape(3,-1)\n",
        "print(Y_hat.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 47500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI2wuqumcGAA",
        "colab_type": "code",
        "outputId": "3384c4d1-8477-4834-aa09-8dd3dc6c5bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Y_hat=np.array(Y_hat)\n",
        "# Y_hat = Y_hat.reshape(3,42750)\n",
        "# Y_hat = np.transpose(Y_hat)\n",
        "# print(Y_hat.shape)\n",
        "# Y_hat2=np.array(Y_hat2)\n",
        "# Y_hat2 = Y_hat2.reshape(3,4750)\n",
        "# Y_hat2 = np.transpose(Y_hat2)\n",
        "# print(Y_hat2.shape)\n",
        "# Y_hat = np.concatenate([Y_hat,Y_hat2])\n",
        "# print(Y_hat.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-92a1ffc293ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_hat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42750\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_hat2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 142500 into shape (3,42750)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwyeRzI4lOXw",
        "colab_type": "code",
        "outputId": "46f27a74-e96d-4219-c7d7-1828ac66a8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# yy=np.concatenate([y_train,y_val])\n",
        "# print(yy.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Uc6fdFZNgH",
        "colab_type": "code",
        "outputId": "df42a271-6dc2-4276-816d-58ce3d064efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Y_hat = model.predict(X)\n",
        "# Y_hat=np.array(Y_hat)\n",
        "# Y_hat = Y_hat.reshape(3,X1.shape[0])\n",
        "# Y_hat=np.array(Y_hat)\n",
        "Y_hat = np.transpose(Y_hat)\n",
        "# Y_hat = np.transpose(Y_hat).reshape(-1,3)\n",
        "print(Y_hat.shape)\n",
        "err_mat = np.abs(Y-Y_hat)/Y\n",
        "err_mat = np.sum(err_mat,axis=1)\n",
        "err_mat = np.sum(err_mat)\n",
        "print(err_mat/Y.shape[0])\n",
        "#0.9704514192587647\n",
        "#10000d 0.9945707841335775\n",
        "#0.5936615853774215\n",
        "#0.32381799619055995"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3)\n",
            "0.38427104720186966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy68LySzkebL",
        "colab_type": "code",
        "outputId": "0e338496-fa3d-4113-b34c-3477328f5335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(Y_hat.shape,Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3) (47500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkMg_43QUYlH",
        "colab_type": "code",
        "outputId": "9be71767-c4db-482d-9bfa-c674705f7eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "err_WMAE = np.dot(np.abs(Y-Y_hat),np.array([300,1,200]))\n",
        "err_WMAE = np.sum(err_WMAE)/Y.shape[0]\n",
        "print(err_WMAE)\n",
        "#145.68255323961225\n",
        "#25.64659003243506\n",
        "\n",
        "##49.38923480439816"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29.826531793100273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23qSUi4qEn63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('MLP_WMAE_lastEpoch.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t15AqhB-aUG7",
        "colab_type": "code",
        "outputId": "0611fbef-17e1-45a8-c449-82621361c8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtest_1,Xtest_2 = resize_input(x_test)\n",
        "Y_test_hat = np.array(model.predict([Xtest_1,Xtest_2]))\n",
        "Y_test_hat = Y_test_hat.reshape(3,-1)\n",
        "Y_test_hat = np.transpose(Y_test_hat)\n",
        "\n",
        "Y_test_hat[:,0]=np.clip(Y_test_hat[:,0],0,1)\n",
        "Y_test_hat[:,1]=np.clip(Y_test_hat[:,1],25,250)\n",
        "Y_test_hat[:,2]=np.clip(Y_test_hat[:,2],0.5,1)\n",
        "\n",
        "np.savetxt(dirPath+'CNN_13.csv',Y_test_hat,delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 shape: (2500, 5000, 1)  X2 shape: (2500, 50, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n34h467dHye",
        "colab_type": "code",
        "outputId": "1551532d-b79a-4e92-fa1f-848eae154469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "!zip cnn_output_13 MLP_output/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: MLP_output/CNN_13.csv (deflated 61%)\n",
            "  adding: MLP_output/MLP_history.pickle (deflated 53%)\n",
            "  adding: MLP_output/MLP_model.hdf5 (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPtpSwm0dUaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv cnn_output_13.zip '/content/gdrive/My Drive/term_2019_1/MLTech/Final'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJNSUc9KxFIi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_0fZ47Sw5uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv MLP_output/MLP_history.pickle MLP_output/MLP_history1.pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra05uSdXW7KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/term_2019_1/MLTech/Final/cnn_output_11.zip' ./\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntSNW9WCqXl6",
        "colab_type": "code",
        "outputId": "77472d99-7992-454b-9f27-688e4f3c5bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!unzip cnn_output_11.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cnn_output_11.zip\n",
            "replace MLP_output/CNN_11.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: r\n",
            "new name: CNN_11_1.csv\n",
            "  inflating: CNN_11_1.csv            \n",
            "replace MLP_output/MLP_history.pickle? [y]es, [n]o, [A]ll, [N]one, [r]ename: r\n",
            "new name: CNN_11_hist.pickle\n",
            "  inflating: CNN_11_hist.pickle      \n",
            "replace MLP_output/MLP_model.hdf5? [y]es, [n]o, [A]ll, [N]one, [r]ename: r\n",
            "new name: CNN_11_model.hdf5\n",
            "  inflating: CNN_11_model.hdf5       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX9TbfnqqcI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm MLP_output/CNN_11_2.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHlgQojssgbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}