{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLFinal-CNN-16.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAPXN1XeRWlZ",
        "colab_type": "code",
        "outputId": "48102f94-8fa1-4a0e-cfd7-6a9c63122c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC-QrcB8SfYi",
        "colab_type": "code",
        "outputId": "4d06e7fc-d3b5-49c9-dd84-eac2261123cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://learner.csie.ntu.edu.tw/~judge/ml19spring/ml19spring.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-02 08:24:14--  https://learner.csie.ntu.edu.tw/~judge/ml19spring/ml19spring.zip\n",
            "Resolving learner.csie.ntu.edu.tw (learner.csie.ntu.edu.tw)... 140.112.90.193\n",
            "Connecting to learner.csie.ntu.edu.tw (learner.csie.ntu.edu.tw)|140.112.90.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3780741206 (3.5G) [application/zip]\n",
            "Saving to: ‘ml19spring.zip’\n",
            "\n",
            "ml19spring.zip      100%[===================>]   3.52G  16.3MB/s    in 5m 8s   \n",
            "\n",
            "2019-06-02 08:29:24 (11.7 MB/s) - ‘ml19spring.zip’ saved [3780741206/3780741206]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqFi8yQLSlm8",
        "colab_type": "code",
        "outputId": "a828c3c4-529e-4312-dc65-b3e63976e60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!unzip -q ml19spring.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open ml19spring.zip, ml19spring.zip.zip or ml19spring.zip.ZIP.\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4o6D8EJPRLS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRoIQ2-EbsYq",
        "colab_type": "code",
        "outputId": "3de0f499-9749-48e2-d212-e1257aa5d04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "# Save model to your Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThyMATs2TBoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "DataDir='./gdrive/My Drive/term_2019_1/MLTech/Final/data/'\n",
        "x_test = np.load(DataDir+'X_test.npz')['arr_0']\n",
        "X = np.load(DataDir+'X_train.npz')['arr_0']\n",
        "Y = np.load(DataDir+'Y_train.npz')['arr_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gab1tIdQTze0",
        "colab_type": "code",
        "outputId": "fd4cde43-6b71-4a13-c13e-02d0cb957577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "source": [
        "dirPath = './MLP_output/'\n",
        "!mkdir ./MLP_output\n",
        "print(X.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEqolJs4bSTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(copy=False)\n",
        "X = scaler.fit_transform(X)\n",
        "x_test = scaler.transform( x_test )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPeL0Z6ewL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_fea = np.load(DataDir+'important_feat_0.0002.npy')\n",
        "X=X[:,important_fea]\n",
        "x_test = x_test[:,important_fea]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ID5oapOlAQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del x_train,x_val,y_train,y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0otbP-OUN9X",
        "colab_type": "code",
        "outputId": "5bb193b5-f108-42d9-fb9d-0f6c062cf500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "x_train,x_val,y_train,y_val=train_test_split(X,Y,test_size=0.2,random_state=44)\n",
        "print(x_train.shape,x_val.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(38000, 10000) (9500, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAydVCGJmtnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model\n",
        "# 23.82688"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDlM_4wOcgLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del y_train_T,y_val_T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf5eojAHUdCk",
        "colab_type": "code",
        "outputId": "9020ed0d-7575-499d-cd7d-80376da1ccee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13960
        }
      },
      "source": [
        "import pickle\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "def MLP_MAE_onlyOutput(input_size):\n",
        "    same_input = Input(shape=(input_size,), dtype='float32',name='input')\n",
        "    X1 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X1 = PReLU()(X1)\n",
        "    X1 = Dense(128,kernel_initializer='normal')(X1)\n",
        "    X1 = PReLU()(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(68,kernel_initializer='normal',activation='tanh')(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(32,kernel_initializer='normal',activation='tanh')(X1)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    y1 = Dense(1,kernel_initializer='normal',name='output_y1')(X1)\n",
        "    \n",
        "    X2 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X2 = PReLU()(X2)\n",
        "    X2 = Dense(128,kernel_initializer='normal')(X2)\n",
        "    X2 = PReLU()(X2)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(68,kernel_initializer='normal',activation='tanh')(X2)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(32,kernel_initializer='normal',activation='tanh')(X2)\n",
        "#     X2=BatchNormalization()(X2)\n",
        "    y2 = Dense(1,kernel_initializer='normal',name='output_y2')(X2)\n",
        "    \n",
        "    X3 = Dense(512,kernel_initializer='normal')(same_input)\n",
        "    X3 = PReLU()(X3)\n",
        "    X3 = Dense(64,kernel_initializer='normal')(X3)\n",
        "    X3 = PReLU()(X3)\n",
        "    y3 = Dense(1,kernel_initializer='normal',name='output_y3')(X3)\n",
        "    model = Model(inputs=same_input, \n",
        "                 outputs=[y1,y2,y3])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'output_y1': 'mean_absolute_error',\n",
        "                       'output_y2': 'mean_absolute_error',\n",
        "                       'output_y3': 'mean_absolute_error'},loss_weights=[300,1,200])\n",
        "    return model\n",
        "def baseline_MLP(input_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512,input_dim=input_size,kernel_initializer='normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(128,kernel_initializer='normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(3,kernel_initializer='normal'))\n",
        "    model.compile(loss = 'mean_absolute_percentage_error',optimizer='adam')\n",
        "    return model\n",
        "def baseline_MLP_WMAE(input_size):\n",
        "    m_input = Input(shape=(input_size,), dtype='float32')\n",
        "    X = Dense(512,kernel_initializer='normal')(m_input)\n",
        "    X = PReLU()(X)\n",
        "    X = Dense(128,kernel_initializer='normal')(X)\n",
        "    X = PReLU()(X)\n",
        "    X1 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    X1 = Dense(32,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X1=BatchNormalization()(X1)\n",
        "    #     X1 = PReLU()(X1)\n",
        "    X2 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    X2 = Dense(32,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X2=BatchNormalization()(X2)\n",
        "    #     X2 = PReLU()(X2)\n",
        "    X3 = Dense(68,kernel_initializer='normal',activation='tanh')(X)\n",
        "    X3=BatchNormalization()(X3)\n",
        "#     X3 = PReLU()(X3)\n",
        "    y1 = Dense(1,kernel_initializer='normal')(X1)\n",
        "    y2 = Dense(1,kernel_initializer='normal')(X2)\n",
        "    y3 = Dense(1,kernel_initializer='normal')(X3)\n",
        "    model = Model(inputs=m_input, outputs=[y1,y2,y3])\n",
        "    model.compile(optimizer='adam', loss='mean_absolute_error',\n",
        "              loss_weights=[300,1,200])\n",
        "    return model\n",
        "def cnn_WMAE(input1_size,input2_size):\n",
        "    print(input1_size,input2_size)\n",
        "    m_input1 = Input(shape=(5000,1,), dtype='float32')\n",
        "    m_input2 = Input(shape=(100,50,), dtype='float32')\n",
        "    x11 = Conv1D(64, 7,strides=4,input_shape=(5000,1), padding='same', activation='relu')(m_input1)\n",
        "    x11 =MaxPooling1D(pool_size=3,strides=4)(x11)\n",
        "    x11 = Conv1D(192, 3,strides=2, padding='same', activation='relu')(x11)\n",
        "    x11 =MaxPooling1D(pool_size=3,strides=2)(x11)\n",
        "    tower_1 = Conv1D(96, 3, padding='same', activation='relu')(x11)\n",
        "    tower_1 = Conv1D(128, 3, padding='same', activation='relu')(tower_1)\n",
        "\n",
        "    tower_2 = Conv1D(16, 1, padding='same', activation='relu')(x11)\n",
        "    tower_2 = Conv1D(32, 5, padding='same', activation='relu')(tower_2)\n",
        "\n",
        "    tower_3 = MaxPooling1D(3, strides=1, padding='same')(x11)\n",
        "    tower_3 = Conv1D(32, 1, padding='same', activation='relu')(tower_3)\n",
        "    tower_4 = Conv1D(16, 1, padding='same', activation='relu')(x11)\n",
        "\n",
        "    x11 = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "    x11 = GlobalAveragePooling1D()(x11)\n",
        "    \n",
        "#     x11 = Flatten()(x11)\n",
        "#     x12 = Flatten()(x12)\n",
        "#     x2 = Conv2D(64, 5,strides=2,input_shape=(50,100,1), padding='same', activation='relu')(m_input2)\n",
        "#     x2 =MaxPooling1D(pool_size=2,strides=2)(x2)\n",
        "#     x2 = Conv2D(192, 3,strides=2, padding='same', activation='relu')(x2)\n",
        "# #     x11 =MaxPooling1D(pool_size=3,strides=2)(x11)\n",
        "#     tower_1 = Conv2D(96, 3, padding='same', activation='relu')(x2)\n",
        "#     tower_1 = Conv2D(128, 3, padding='same', activation='relu')(tower_1)\n",
        "\n",
        "#     tower_2 = Conv2D(16, 1, padding='same', activation='relu')(x2)\n",
        "#     tower_2 = Conv2D(32, 5, padding='same', activation='relu')(tower_2)\n",
        "\n",
        "#     tower_3 = MaxPooling1D(3, strides=1, padding='same')(x2)\n",
        "#     tower_3 = Conv2D(32, 1, padding='same', activation='relu')(tower_3)\n",
        "#     tower_4 = Conv2D(64, 1, padding='same', activation='relu')(x2)\n",
        "\n",
        "#     x2 = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "#     x2 = GlobalAveragePooling2D()(x2)\n",
        "    \n",
        "    x21 = Conv1D(64,kernel_size=3,input_shape=(100,50,), activation='relu')(m_input2)\n",
        "    x21 = Conv1D(32,kernel_size=1, activation='relu')(x21)\n",
        "    x21 =MaxPooling1D(pool_size=24)(x21)\n",
        "\n",
        "    x21 = Flatten()(x21)\n",
        "    merged = concatenate(\n",
        "        [x11, x21],\n",
        "        axis=-1)\n",
        "    merged = Dropout(0.15)(merged)\n",
        "    merged=BatchNormalization()(merged)\n",
        "    merged = Dense(units=200,kernel_initializer='normal')(merged)\n",
        "    merged = PReLU()(merged)\n",
        "    merged = Dense(units=64,kernel_initializer='normal', activation='tanh')(merged)\n",
        "#     yy2 = Dense(units=64,kernel_initializer='normal')(merged)\n",
        "#     yy2=PReLU()(yy2)\n",
        "#     yy3 = Dense(units=64,kernel_initializer='normal', activation='tanh')(merged)\n",
        "#     merged = Dropout(0.15)(merged)\n",
        "#     merged = Dense(units=128,kernel_initializer='normal')(merged)\n",
        "#     merged=PReLU()(merged)\n",
        "#     merged = Dropout(0.2)(merged)\n",
        "#     merged=BatchNormalization()(merged)\n",
        "    y1 = Dense(units=1)(merged)\n",
        "    y2 = Dense(units=1)(merged)\n",
        "    y3 = Dense(units=1)(merged)\n",
        "\n",
        "    model = Model(\n",
        "        inputs=[m_input1, m_input2],\n",
        "        outputs=[y1,y2,y3])\n",
        "#     adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.98)\n",
        "    model.compile(optimizer='adam', loss='mean_absolute_error',\n",
        "              loss_weights=[300,1,200])\n",
        "#     model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "    return model\n",
        "def resize_input(X):\n",
        "    X1 = X[:,:5000].reshape(-1,5000,1)\n",
        "    X2 = X[:,5000:].reshape(-1,50,100)\n",
        "    X2 = np.transpose(X2,(0,2,1))\n",
        "    print('X1 shape:',X1.shape,' X2 shape:',X2.shape)\n",
        "    return X1,X2\n",
        "dirPath = './MLP_output/' \n",
        "# model = baseline_MLP_WMAE(x_train.shape[1])\n",
        "# model = baseline_MLP(x_train.shape[1])\n",
        "# model = MLP_MAE_onlyOutput(x_train.shape[1])\n",
        "x_train1,x_train2 = resize_input(x_train)\n",
        "x_val1,x_val2 = resize_input(x_val)\n",
        "model = cnn_WMAE(x_train1.shape[1:3],x_train2.shape[1:4])\n",
        "model.summary()\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "NUM_EPOCHS = 200\n",
        "\n",
        "# checkpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss',patience=30)\n",
        "MODELPATH=dirPath+\"MLP_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(MODELPATH, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "print(y_train.shape)\n",
        "y_train = np.transpose(y_train)\n",
        "print(y_train.shape)\n",
        "y_val = np.transpose(y_val)\n",
        "print(y_val.shape)\n",
        "# history = model.fit(x=x_train,y=[y_train_T[0],y_train_T[1],y_train_T[2]],batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,[y_val_T[0],y_val_T[1],y_val_T[2]]),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "# history = model.fit(x=x_train,y=y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,y_val),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "# history = model.fit(x=x_train,y=y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "#         validation_data=(x_val,y_val),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "history = model.fit(x=[x_train1,x_train2],y=[y_train[0],y_train[1],y_train[2]],batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "        validation_data=([x_val1,x_val2],[y_val[0],y_val[1],y_val[2]]),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "file_his = open(dirPath+'MLP_history.pickle', 'wb')\n",
        "pickle.dump(history.history, file_his)\n",
        "file_his.close()\n",
        "#25 100 114"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0615 11:48:08.715872 140417854199680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0615 11:48:08.725764 140417854199680 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X1 shape: (38000, 5000, 1)  X2 shape: (38000, 100, 50)\n",
            "X1 shape: (9500, 5000, 1)  X2 shape: (9500, 100, 50)\n",
            "(5000, 1) (100, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 11:48:08.844748 140417854199680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "W0615 11:48:08.907122 140417854199680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 5000, 1)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 1250, 64)     512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 312, 64)      0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 156, 192)     37056       max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 77, 192)      0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 100, 50)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 77, 96)       55392       max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 77, 16)       3088        max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1D)  (None, 77, 192)      0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 98, 64)       9664        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 77, 128)      36992       conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 77, 32)       2592        conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 77, 32)       6176        max_pooling1d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 77, 16)       3088        max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 98, 32)       2080        conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 77, 208)      0           conv1d_14[0][0]                  \n",
            "                                                                 conv1d_16[0][0]                  \n",
            "                                                                 conv1d_17[0][0]                  \n",
            "                                                                 conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, 4, 32)        0           conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 208)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 128)          0           max_pooling1d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 336)          0           global_average_pooling1d_2[0][0] \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 336)          0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 336)          1344        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 200)          67400       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_1 (PReLU)               (None, 200)          200         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           12864       p_re_lu_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            65          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            65          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            65          dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 238,643\n",
            "Trainable params: 237,971\n",
            "Non-trainable params: 672\n",
            "__________________________________________________________________________________________________\n",
            "(38000, 3)\n",
            "(3, 38000)\n",
            "(3, 9500)\n",
            "Train on 38000 samples, validate on 9500 samples\n",
            "Epoch 1/200\n",
            "38000/38000 [==============================] - 21s 545us/step - loss: 251.5804 - dense_3_loss: 0.2814 - dense_4_loss: 136.8616 - dense_5_loss: 0.1514 - val_loss: 225.7830 - val_dense_3_loss: 0.2590 - val_dense_4_loss: 136.1710 - val_dense_5_loss: 0.0595\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 225.78299, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 2/200\n",
            "38000/38000 [==============================] - 9s 226us/step - loss: 220.6892 - dense_3_loss: 0.2425 - dense_4_loss: 135.6305 - dense_5_loss: 0.0616 - val_loss: 214.9362 - val_dense_3_loss: 0.2244 - val_dense_4_loss: 134.8895 - val_dense_5_loss: 0.0636\n",
            "\n",
            "Epoch 00002: val_loss improved from 225.78299 to 214.93619, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 3/200\n",
            "38000/38000 [==============================] - 9s 226us/step - loss: 208.7389 - dense_3_loss: 0.2153 - dense_4_loss: 134.0599 - dense_5_loss: 0.0504 - val_loss: 204.4821 - val_dense_3_loss: 0.2073 - val_dense_4_loss: 133.1599 - val_dense_5_loss: 0.0456\n",
            "\n",
            "Epoch 00003: val_loss improved from 214.93619 to 204.48214, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 4/200\n",
            "38000/38000 [==============================] - 9s 225us/step - loss: 193.5889 - dense_3_loss: 0.1745 - dense_4_loss: 132.1813 - dense_5_loss: 0.0453 - val_loss: 192.2897 - val_dense_3_loss: 0.1736 - val_dense_4_loss: 131.0617 - val_dense_5_loss: 0.0457\n",
            "\n",
            "Epoch 00004: val_loss improved from 204.48214 to 192.28972, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 5/200\n",
            "38000/38000 [==============================] - 9s 225us/step - loss: 178.0789 - dense_3_loss: 0.1302 - dense_4_loss: 130.0729 - dense_5_loss: 0.0447 - val_loss: 175.0074 - val_dense_3_loss: 0.1294 - val_dense_4_loss: 128.6478 - val_dense_5_loss: 0.0378\n",
            "\n",
            "Epoch 00005: val_loss improved from 192.28972 to 175.00741, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 6/200\n",
            "38000/38000 [==============================] - 9s 226us/step - loss: 171.0346 - dense_3_loss: 0.1178 - dense_4_loss: 127.4952 - dense_5_loss: 0.0410 - val_loss: 177.8906 - val_dense_3_loss: 0.1508 - val_dense_4_loss: 125.9275 - val_dense_5_loss: 0.0336\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 175.00741\n",
            "Epoch 7/200\n",
            "38000/38000 [==============================] - 9s 226us/step - loss: 165.5204 - dense_3_loss: 0.1127 - dense_4_loss: 124.6069 - dense_5_loss: 0.0355 - val_loss: 159.5974 - val_dense_3_loss: 0.1052 - val_dense_4_loss: 122.8685 - val_dense_5_loss: 0.0259\n",
            "\n",
            "Epoch 00007: val_loss improved from 175.00741 to 159.59744, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 8/200\n",
            "38000/38000 [==============================] - 9s 226us/step - loss: 160.8762 - dense_3_loss: 0.1092 - dense_4_loss: 121.2361 - dense_5_loss: 0.0344 - val_loss: 161.6304 - val_dense_3_loss: 0.1229 - val_dense_4_loss: 119.4211 - val_dense_5_loss: 0.0267\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 159.59744\n",
            "Epoch 9/200\n",
            "38000/38000 [==============================] - 9s 224us/step - loss: 155.3398 - dense_3_loss: 0.1035 - dense_4_loss: 117.7085 - dense_5_loss: 0.0328 - val_loss: 159.0702 - val_dense_3_loss: 0.1251 - val_dense_4_loss: 115.8675 - val_dense_5_loss: 0.0284\n",
            "\n",
            "Epoch 00009: val_loss improved from 159.59744 to 159.07023, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 10/200\n",
            "38000/38000 [==============================] - 9s 226us/step - loss: 152.8750 - dense_3_loss: 0.1062 - dense_4_loss: 114.0491 - dense_5_loss: 0.0349 - val_loss: 159.2109 - val_dense_3_loss: 0.1332 - val_dense_4_loss: 112.4546 - val_dense_5_loss: 0.0340\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 159.07023\n",
            "Epoch 11/200\n",
            "38000/38000 [==============================] - 9s 226us/step - loss: 148.2817 - dense_3_loss: 0.1029 - dense_4_loss: 110.4364 - dense_5_loss: 0.0349 - val_loss: 143.0449 - val_dense_3_loss: 0.0988 - val_dense_4_loss: 108.6766 - val_dense_5_loss: 0.0237\n",
            "\n",
            "Epoch 00011: val_loss improved from 159.07023 to 143.04488, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 12/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 143.0621 - dense_3_loss: 0.1000 - dense_4_loss: 106.8983 - dense_5_loss: 0.0308 - val_loss: 144.1313 - val_dense_3_loss: 0.1043 - val_dense_4_loss: 105.0688 - val_dense_5_loss: 0.0389\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 143.04488\n",
            "Epoch 13/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 139.3504 - dense_3_loss: 0.0995 - dense_4_loss: 103.4264 - dense_5_loss: 0.0303 - val_loss: 149.4211 - val_dense_3_loss: 0.1385 - val_dense_4_loss: 102.0179 - val_dense_5_loss: 0.0292\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 143.04488\n",
            "Epoch 14/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 135.5280 - dense_3_loss: 0.0997 - dense_4_loss: 100.1549 - dense_5_loss: 0.0273 - val_loss: 140.0757 - val_dense_3_loss: 0.1167 - val_dense_4_loss: 98.5332 - val_dense_5_loss: 0.0327\n",
            "\n",
            "Epoch 00014: val_loss improved from 143.04488 to 140.07567, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 15/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 132.1242 - dense_3_loss: 0.0987 - dense_4_loss: 97.0171 - dense_5_loss: 0.0274 - val_loss: 134.0091 - val_dense_3_loss: 0.1161 - val_dense_4_loss: 95.2559 - val_dense_5_loss: 0.0197\n",
            "\n",
            "Epoch 00015: val_loss improved from 140.07567 to 134.00907, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 16/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 129.5864 - dense_3_loss: 0.1001 - dense_4_loss: 93.9537 - dense_5_loss: 0.0280 - val_loss: 128.3161 - val_dense_3_loss: 0.1073 - val_dense_4_loss: 92.2274 - val_dense_5_loss: 0.0195\n",
            "\n",
            "Epoch 00016: val_loss improved from 134.00907 to 128.31610, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 17/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 125.6758 - dense_3_loss: 0.0976 - dense_4_loss: 90.8586 - dense_5_loss: 0.0276 - val_loss: 122.4482 - val_dense_3_loss: 0.0986 - val_dense_4_loss: 89.2195 - val_dense_5_loss: 0.0183\n",
            "\n",
            "Epoch 00017: val_loss improved from 128.31610 to 122.44821, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 18/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 121.6541 - dense_3_loss: 0.0967 - dense_4_loss: 88.2214 - dense_5_loss: 0.0221 - val_loss: 120.7126 - val_dense_3_loss: 0.0987 - val_dense_4_loss: 86.8205 - val_dense_5_loss: 0.0214\n",
            "\n",
            "Epoch 00018: val_loss improved from 122.44821 to 120.71256, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 19/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 119.0866 - dense_3_loss: 0.0959 - dense_4_loss: 85.8672 - dense_5_loss: 0.0222 - val_loss: 119.2726 - val_dense_3_loss: 0.1010 - val_dense_4_loss: 84.4884 - val_dense_5_loss: 0.0224\n",
            "\n",
            "Epoch 00019: val_loss improved from 120.71256 to 119.27259, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 20/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 117.0906 - dense_3_loss: 0.0965 - dense_4_loss: 83.7595 - dense_5_loss: 0.0219 - val_loss: 119.0442 - val_dense_3_loss: 0.1052 - val_dense_4_loss: 82.3799 - val_dense_5_loss: 0.0256\n",
            "\n",
            "Epoch 00020: val_loss improved from 119.27259 to 119.04423, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 21/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 114.7583 - dense_3_loss: 0.0953 - dense_4_loss: 81.7590 - dense_5_loss: 0.0221 - val_loss: 114.5778 - val_dense_3_loss: 0.0979 - val_dense_4_loss: 80.6779 - val_dense_5_loss: 0.0227\n",
            "\n",
            "Epoch 00021: val_loss improved from 119.04423 to 114.57777, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 22/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 112.5605 - dense_3_loss: 0.0959 - dense_4_loss: 79.8723 - dense_5_loss: 0.0196 - val_loss: 110.7263 - val_dense_3_loss: 0.0967 - val_dense_4_loss: 78.5720 - val_dense_5_loss: 0.0158\n",
            "\n",
            "Epoch 00022: val_loss improved from 114.57777 to 110.72633, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 23/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 110.3294 - dense_3_loss: 0.0938 - dense_4_loss: 78.0479 - dense_5_loss: 0.0206 - val_loss: 110.4556 - val_dense_3_loss: 0.1007 - val_dense_4_loss: 76.8781 - val_dense_5_loss: 0.0168\n",
            "\n",
            "Epoch 00023: val_loss improved from 110.72633 to 110.45556, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 24/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 109.1893 - dense_3_loss: 0.0965 - dense_4_loss: 76.2511 - dense_5_loss: 0.0199 - val_loss: 107.4706 - val_dense_3_loss: 0.0961 - val_dense_4_loss: 75.0955 - val_dense_5_loss: 0.0178\n",
            "\n",
            "Epoch 00024: val_loss improved from 110.45556 to 107.47059, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 25/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 106.5729 - dense_3_loss: 0.0937 - dense_4_loss: 74.3425 - dense_5_loss: 0.0205 - val_loss: 107.3767 - val_dense_3_loss: 0.1036 - val_dense_4_loss: 73.1570 - val_dense_5_loss: 0.0156\n",
            "\n",
            "Epoch 00025: val_loss improved from 107.47059 to 107.37666, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 26/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 104.9103 - dense_3_loss: 0.0952 - dense_4_loss: 72.4584 - dense_5_loss: 0.0195 - val_loss: 103.5525 - val_dense_3_loss: 0.0994 - val_dense_4_loss: 71.1529 - val_dense_5_loss: 0.0129\n",
            "\n",
            "Epoch 00026: val_loss improved from 107.37666 to 103.55250, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 27/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 103.3114 - dense_3_loss: 0.0966 - dense_4_loss: 70.6597 - dense_5_loss: 0.0184 - val_loss: 101.0856 - val_dense_3_loss: 0.0962 - val_dense_4_loss: 69.4295 - val_dense_5_loss: 0.0140\n",
            "\n",
            "Epoch 00027: val_loss improved from 103.55250 to 101.08561, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 28/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 101.5896 - dense_3_loss: 0.0956 - dense_4_loss: 68.8869 - dense_5_loss: 0.0202 - val_loss: 104.0621 - val_dense_3_loss: 0.1024 - val_dense_4_loss: 67.6693 - val_dense_5_loss: 0.0284\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 101.08561\n",
            "Epoch 29/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 99.4761 - dense_3_loss: 0.0946 - dense_4_loss: 67.1775 - dense_5_loss: 0.0196 - val_loss: 101.5153 - val_dense_3_loss: 0.1060 - val_dense_4_loss: 66.0108 - val_dense_5_loss: 0.0185\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 101.08561\n",
            "Epoch 30/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 97.6926 - dense_3_loss: 0.0942 - dense_4_loss: 65.4466 - dense_5_loss: 0.0200 - val_loss: 96.4484 - val_dense_3_loss: 0.0953 - val_dense_4_loss: 64.1650 - val_dense_5_loss: 0.0185\n",
            "\n",
            "Epoch 00030: val_loss improved from 101.08561 to 96.44843, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 31/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 95.1927 - dense_3_loss: 0.0926 - dense_4_loss: 63.6753 - dense_5_loss: 0.0186 - val_loss: 96.6181 - val_dense_3_loss: 0.0939 - val_dense_4_loss: 62.5130 - val_dense_5_loss: 0.0296\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 96.44843\n",
            "Epoch 32/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 94.0544 - dense_3_loss: 0.0937 - dense_4_loss: 61.9550 - dense_5_loss: 0.0200 - val_loss: 93.8291 - val_dense_3_loss: 0.0976 - val_dense_4_loss: 60.9081 - val_dense_5_loss: 0.0182\n",
            "\n",
            "Epoch 00032: val_loss improved from 96.44843 to 93.82906, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 33/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 92.5903 - dense_3_loss: 0.0947 - dense_4_loss: 60.2944 - dense_5_loss: 0.0194 - val_loss: 94.5142 - val_dense_3_loss: 0.1041 - val_dense_4_loss: 59.4766 - val_dense_5_loss: 0.0191\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 93.82906\n",
            "Epoch 34/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 90.4509 - dense_3_loss: 0.0940 - dense_4_loss: 58.8053 - dense_5_loss: 0.0172 - val_loss: 92.6019 - val_dense_3_loss: 0.0976 - val_dense_4_loss: 58.0470 - val_dense_5_loss: 0.0263\n",
            "\n",
            "Epoch 00034: val_loss improved from 93.82906 to 92.60190, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 35/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 90.2759 - dense_3_loss: 0.0945 - dense_4_loss: 57.2919 - dense_5_loss: 0.0232 - val_loss: 88.7560 - val_dense_3_loss: 0.0953 - val_dense_4_loss: 56.3750 - val_dense_5_loss: 0.0190\n",
            "\n",
            "Epoch 00035: val_loss improved from 92.60190 to 88.75602, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 36/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 87.1509 - dense_3_loss: 0.0934 - dense_4_loss: 55.8637 - dense_5_loss: 0.0163 - val_loss: 88.7757 - val_dense_3_loss: 0.1011 - val_dense_4_loss: 55.0628 - val_dense_5_loss: 0.0169\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 88.75602\n",
            "Epoch 37/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 86.0353 - dense_3_loss: 0.0923 - dense_4_loss: 54.4942 - dense_5_loss: 0.0193 - val_loss: 86.0805 - val_dense_3_loss: 0.0973 - val_dense_4_loss: 53.6158 - val_dense_5_loss: 0.0164\n",
            "\n",
            "Epoch 00037: val_loss improved from 88.75602 to 86.08045, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 38/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 84.1854 - dense_3_loss: 0.0920 - dense_4_loss: 53.3702 - dense_5_loss: 0.0161 - val_loss: 84.3532 - val_dense_3_loss: 0.0940 - val_dense_4_loss: 52.4825 - val_dense_5_loss: 0.0183\n",
            "\n",
            "Epoch 00038: val_loss improved from 86.08045 to 84.35325, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 39/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 83.4498 - dense_3_loss: 0.0939 - dense_4_loss: 51.8621 - dense_5_loss: 0.0171 - val_loss: 82.9373 - val_dense_3_loss: 0.0954 - val_dense_4_loss: 51.1971 - val_dense_5_loss: 0.0157\n",
            "\n",
            "Epoch 00039: val_loss improved from 84.35325 to 82.93731, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 40/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 82.1903 - dense_3_loss: 0.0924 - dense_4_loss: 50.5400 - dense_5_loss: 0.0197 - val_loss: 80.7657 - val_dense_3_loss: 0.0962 - val_dense_4_loss: 49.6408 - val_dense_5_loss: 0.0114\n",
            "\n",
            "Epoch 00040: val_loss improved from 82.93731 to 80.76572, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 41/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 80.0219 - dense_3_loss: 0.0926 - dense_4_loss: 49.1944 - dense_5_loss: 0.0153 - val_loss: 80.2636 - val_dense_3_loss: 0.0948 - val_dense_4_loss: 48.4671 - val_dense_5_loss: 0.0168\n",
            "\n",
            "Epoch 00041: val_loss improved from 80.76572 to 80.26363, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 42/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 79.7804 - dense_3_loss: 0.0939 - dense_4_loss: 48.0136 - dense_5_loss: 0.0180 - val_loss: 78.4726 - val_dense_3_loss: 0.0917 - val_dense_4_loss: 47.3994 - val_dense_5_loss: 0.0178\n",
            "\n",
            "Epoch 00042: val_loss improved from 80.26363 to 78.47259, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 43/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 77.5514 - dense_3_loss: 0.0919 - dense_4_loss: 46.7880 - dense_5_loss: 0.0160 - val_loss: 76.5958 - val_dense_3_loss: 0.0942 - val_dense_4_loss: 46.1994 - val_dense_5_loss: 0.0107\n",
            "\n",
            "Epoch 00043: val_loss improved from 78.47259 to 76.59576, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 44/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 76.5011 - dense_3_loss: 0.0923 - dense_4_loss: 45.6257 - dense_5_loss: 0.0159 - val_loss: 78.8933 - val_dense_3_loss: 0.0985 - val_dense_4_loss: 45.4120 - val_dense_5_loss: 0.0197\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 76.59576\n",
            "Epoch 45/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 75.4652 - dense_3_loss: 0.0925 - dense_4_loss: 44.7016 - dense_5_loss: 0.0151 - val_loss: 75.6943 - val_dense_3_loss: 0.0957 - val_dense_4_loss: 44.0867 - val_dense_5_loss: 0.0145\n",
            "\n",
            "Epoch 00045: val_loss improved from 76.59576 to 75.69429, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 46/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 74.1849 - dense_3_loss: 0.0923 - dense_4_loss: 43.4353 - dense_5_loss: 0.0153 - val_loss: 76.1008 - val_dense_3_loss: 0.0955 - val_dense_4_loss: 42.5331 - val_dense_5_loss: 0.0246\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 75.69429\n",
            "Epoch 47/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 72.8573 - dense_3_loss: 0.0908 - dense_4_loss: 42.3245 - dense_5_loss: 0.0164 - val_loss: 74.2440 - val_dense_3_loss: 0.0980 - val_dense_4_loss: 41.4535 - val_dense_5_loss: 0.0170\n",
            "\n",
            "Epoch 00047: val_loss improved from 75.69429 to 74.24396, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 48/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 71.8953 - dense_3_loss: 0.0912 - dense_4_loss: 41.3214 - dense_5_loss: 0.0160 - val_loss: 71.6105 - val_dense_3_loss: 0.0918 - val_dense_4_loss: 40.3947 - val_dense_5_loss: 0.0184\n",
            "\n",
            "Epoch 00048: val_loss improved from 74.24396 to 71.61048, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 49/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 71.4190 - dense_3_loss: 0.0918 - dense_4_loss: 40.3877 - dense_5_loss: 0.0174 - val_loss: 71.0989 - val_dense_3_loss: 0.0960 - val_dense_4_loss: 39.3884 - val_dense_5_loss: 0.0146\n",
            "\n",
            "Epoch 00049: val_loss improved from 71.61048 to 71.09894, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 50/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 70.4371 - dense_3_loss: 0.0924 - dense_4_loss: 39.3834 - dense_5_loss: 0.0166 - val_loss: 71.6397 - val_dense_3_loss: 0.0969 - val_dense_4_loss: 38.4323 - val_dense_5_loss: 0.0206\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 71.09894\n",
            "Epoch 51/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 69.4133 - dense_3_loss: 0.0915 - dense_4_loss: 38.4362 - dense_5_loss: 0.0176 - val_loss: 68.9844 - val_dense_3_loss: 0.0954 - val_dense_4_loss: 37.6292 - val_dense_5_loss: 0.0137\n",
            "\n",
            "Epoch 00051: val_loss improved from 71.09894 to 68.98436, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 52/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 68.3566 - dense_3_loss: 0.0911 - dense_4_loss: 37.5328 - dense_5_loss: 0.0175 - val_loss: 69.1637 - val_dense_3_loss: 0.0961 - val_dense_4_loss: 36.6327 - val_dense_5_loss: 0.0185\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 68.98436\n",
            "Epoch 53/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 66.5755 - dense_3_loss: 0.0905 - dense_4_loss: 36.7019 - dense_5_loss: 0.0136 - val_loss: 68.7073 - val_dense_3_loss: 0.1025 - val_dense_4_loss: 36.0824 - val_dense_5_loss: 0.0094\n",
            "\n",
            "Epoch 00053: val_loss improved from 68.98436 to 68.70732, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 54/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 66.7376 - dense_3_loss: 0.0915 - dense_4_loss: 35.8426 - dense_5_loss: 0.0172 - val_loss: 65.1113 - val_dense_3_loss: 0.0916 - val_dense_4_loss: 35.1579 - val_dense_5_loss: 0.0124\n",
            "\n",
            "Epoch 00054: val_loss improved from 68.70732 to 65.11127, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 55/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 65.0722 - dense_3_loss: 0.0907 - dense_4_loss: 35.0893 - dense_5_loss: 0.0139 - val_loss: 64.5887 - val_dense_3_loss: 0.0924 - val_dense_4_loss: 34.4886 - val_dense_5_loss: 0.0119\n",
            "\n",
            "Epoch 00055: val_loss improved from 65.11127 to 64.58872, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 56/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 64.6357 - dense_3_loss: 0.0910 - dense_4_loss: 34.2705 - dense_5_loss: 0.0154 - val_loss: 63.9655 - val_dense_3_loss: 0.0933 - val_dense_4_loss: 33.5823 - val_dense_5_loss: 0.0119\n",
            "\n",
            "Epoch 00056: val_loss improved from 64.58872 to 63.96555, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 57/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 63.2853 - dense_3_loss: 0.0898 - dense_4_loss: 33.4867 - dense_5_loss: 0.0144 - val_loss: 61.9071 - val_dense_3_loss: 0.0897 - val_dense_4_loss: 32.8701 - val_dense_5_loss: 0.0107\n",
            "\n",
            "Epoch 00057: val_loss improved from 63.96555 to 61.90714, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 58/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 62.6024 - dense_3_loss: 0.0901 - dense_4_loss: 32.7119 - dense_5_loss: 0.0143 - val_loss: 63.5841 - val_dense_3_loss: 0.0970 - val_dense_4_loss: 32.2386 - val_dense_5_loss: 0.0113\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 61.90714\n",
            "Epoch 59/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 61.8323 - dense_3_loss: 0.0896 - dense_4_loss: 32.0574 - dense_5_loss: 0.0145 - val_loss: 61.2394 - val_dense_3_loss: 0.0918 - val_dense_4_loss: 31.6007 - val_dense_5_loss: 0.0105\n",
            "\n",
            "Epoch 00059: val_loss improved from 61.90714 to 61.23937, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 60/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 61.1055 - dense_3_loss: 0.0891 - dense_4_loss: 31.4002 - dense_5_loss: 0.0149 - val_loss: 63.5975 - val_dense_3_loss: 0.0939 - val_dense_4_loss: 31.0132 - val_dense_5_loss: 0.0220\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 61.23937\n",
            "Epoch 61/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 60.8859 - dense_3_loss: 0.0897 - dense_4_loss: 30.7484 - dense_5_loss: 0.0161 - val_loss: 65.8136 - val_dense_3_loss: 0.1008 - val_dense_4_loss: 30.3338 - val_dense_5_loss: 0.0262\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 61.23937\n",
            "Epoch 62/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 61.6050 - dense_3_loss: 0.0923 - dense_4_loss: 30.1945 - dense_5_loss: 0.0186 - val_loss: 61.5336 - val_dense_3_loss: 0.0976 - val_dense_4_loss: 30.0110 - val_dense_5_loss: 0.0112\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 61.23937\n",
            "Epoch 63/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 59.4391 - dense_3_loss: 0.0890 - dense_4_loss: 29.6983 - dense_5_loss: 0.0152 - val_loss: 58.5837 - val_dense_3_loss: 0.0913 - val_dense_4_loss: 29.2359 - val_dense_5_loss: 0.0098\n",
            "\n",
            "Epoch 00063: val_loss improved from 61.23937 to 58.58366, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 64/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 59.2159 - dense_3_loss: 0.0910 - dense_4_loss: 29.1008 - dense_5_loss: 0.0140 - val_loss: 60.2076 - val_dense_3_loss: 0.0950 - val_dense_4_loss: 28.7270 - val_dense_5_loss: 0.0149\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 58.58366\n",
            "Epoch 65/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 58.3811 - dense_3_loss: 0.0902 - dense_4_loss: 28.5063 - dense_5_loss: 0.0141 - val_loss: 60.5564 - val_dense_3_loss: 0.0938 - val_dense_4_loss: 27.9444 - val_dense_5_loss: 0.0223\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 58.58366\n",
            "Epoch 66/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 57.7990 - dense_3_loss: 0.0898 - dense_4_loss: 28.0213 - dense_5_loss: 0.0141 - val_loss: 57.7081 - val_dense_3_loss: 0.0912 - val_dense_4_loss: 27.5207 - val_dense_5_loss: 0.0141\n",
            "\n",
            "Epoch 00066: val_loss improved from 58.58366 to 57.70814, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 67/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 58.0865 - dense_3_loss: 0.0902 - dense_4_loss: 27.5160 - dense_5_loss: 0.0175 - val_loss: 61.2503 - val_dense_3_loss: 0.1031 - val_dense_4_loss: 26.9649 - val_dense_5_loss: 0.0168\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 57.70814\n",
            "Epoch 68/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 57.2710 - dense_3_loss: 0.0912 - dense_4_loss: 27.1039 - dense_5_loss: 0.0140 - val_loss: 56.7020 - val_dense_3_loss: 0.0906 - val_dense_4_loss: 26.7993 - val_dense_5_loss: 0.0136\n",
            "\n",
            "Epoch 00068: val_loss improved from 57.70814 to 56.70205, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 69/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 56.4219 - dense_3_loss: 0.0899 - dense_4_loss: 26.5441 - dense_5_loss: 0.0145 - val_loss: 57.0435 - val_dense_3_loss: 0.0915 - val_dense_4_loss: 25.9736 - val_dense_5_loss: 0.0181\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 56.70205\n",
            "Epoch 70/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 56.3221 - dense_3_loss: 0.0899 - dense_4_loss: 26.2156 - dense_5_loss: 0.0157 - val_loss: 56.8871 - val_dense_3_loss: 0.0920 - val_dense_4_loss: 26.5428 - val_dense_5_loss: 0.0137\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 56.70205\n",
            "Epoch 71/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 55.3525 - dense_3_loss: 0.0888 - dense_4_loss: 25.8000 - dense_5_loss: 0.0146 - val_loss: 57.8803 - val_dense_3_loss: 0.0971 - val_dense_4_loss: 25.7155 - val_dense_5_loss: 0.0152\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 56.70205\n",
            "Epoch 72/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 55.3743 - dense_3_loss: 0.0895 - dense_4_loss: 25.5202 - dense_5_loss: 0.0150 - val_loss: 55.2045 - val_dense_3_loss: 0.0922 - val_dense_4_loss: 25.1095 - val_dense_5_loss: 0.0122\n",
            "\n",
            "Epoch 00072: val_loss improved from 56.70205 to 55.20448, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 73/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 55.1876 - dense_3_loss: 0.0897 - dense_4_loss: 25.2311 - dense_5_loss: 0.0152 - val_loss: 55.7159 - val_dense_3_loss: 0.0918 - val_dense_4_loss: 24.6806 - val_dense_5_loss: 0.0174\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 55.20448\n",
            "Epoch 74/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 54.2612 - dense_3_loss: 0.0890 - dense_4_loss: 24.7184 - dense_5_loss: 0.0142 - val_loss: 56.2283 - val_dense_3_loss: 0.0926 - val_dense_4_loss: 24.9363 - val_dense_5_loss: 0.0176\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 55.20448\n",
            "Epoch 75/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 53.5535 - dense_3_loss: 0.0877 - dense_4_loss: 24.3189 - dense_5_loss: 0.0146 - val_loss: 54.5211 - val_dense_3_loss: 0.0906 - val_dense_4_loss: 24.6519 - val_dense_5_loss: 0.0134\n",
            "\n",
            "Epoch 00075: val_loss improved from 55.20448 to 54.52105, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 76/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 53.0659 - dense_3_loss: 0.0878 - dense_4_loss: 24.0870 - dense_5_loss: 0.0132 - val_loss: 54.1153 - val_dense_3_loss: 0.0927 - val_dense_4_loss: 23.8715 - val_dense_5_loss: 0.0122\n",
            "\n",
            "Epoch 00076: val_loss improved from 54.52105 to 54.11530, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 77/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 52.7712 - dense_3_loss: 0.0884 - dense_4_loss: 23.7124 - dense_5_loss: 0.0127 - val_loss: 53.6425 - val_dense_3_loss: 0.0889 - val_dense_4_loss: 23.6322 - val_dense_5_loss: 0.0167\n",
            "\n",
            "Epoch 00077: val_loss improved from 54.11530 to 53.64250, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 78/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 52.7521 - dense_3_loss: 0.0886 - dense_4_loss: 23.4196 - dense_5_loss: 0.0138 - val_loss: 53.1307 - val_dense_3_loss: 0.0908 - val_dense_4_loss: 23.6040 - val_dense_5_loss: 0.0115\n",
            "\n",
            "Epoch 00078: val_loss improved from 53.64250 to 53.13070, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 79/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 52.7840 - dense_3_loss: 0.0881 - dense_4_loss: 23.1700 - dense_5_loss: 0.0159 - val_loss: 51.9773 - val_dense_3_loss: 0.0904 - val_dense_4_loss: 22.8313 - val_dense_5_loss: 0.0102\n",
            "\n",
            "Epoch 00079: val_loss improved from 53.13070 to 51.97734, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 80/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 51.8371 - dense_3_loss: 0.0876 - dense_4_loss: 22.9706 - dense_5_loss: 0.0130 - val_loss: 52.7475 - val_dense_3_loss: 0.0903 - val_dense_4_loss: 23.8427 - val_dense_5_loss: 0.0091\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 51.97734\n",
            "Epoch 81/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 52.2275 - dense_3_loss: 0.0882 - dense_4_loss: 22.7796 - dense_5_loss: 0.0150 - val_loss: 54.1922 - val_dense_3_loss: 0.0902 - val_dense_4_loss: 22.9844 - val_dense_5_loss: 0.0207\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 51.97734\n",
            "Epoch 82/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 53.0380 - dense_3_loss: 0.0909 - dense_4_loss: 22.4905 - dense_5_loss: 0.0164 - val_loss: 54.1445 - val_dense_3_loss: 0.0989 - val_dense_4_loss: 21.9072 - val_dense_5_loss: 0.0128\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 51.97734\n",
            "Epoch 83/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 51.5645 - dense_3_loss: 0.0891 - dense_4_loss: 22.2070 - dense_5_loss: 0.0132 - val_loss: 52.6835 - val_dense_3_loss: 0.0935 - val_dense_4_loss: 21.8813 - val_dense_5_loss: 0.0138\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 51.97734\n",
            "Epoch 84/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 51.5000 - dense_3_loss: 0.0894 - dense_4_loss: 22.0167 - dense_5_loss: 0.0134 - val_loss: 51.1598 - val_dense_3_loss: 0.0910 - val_dense_4_loss: 21.9554 - val_dense_5_loss: 0.0096\n",
            "\n",
            "Epoch 00084: val_loss improved from 51.97734 to 51.15982, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 85/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 51.3044 - dense_3_loss: 0.0876 - dense_4_loss: 21.8491 - dense_5_loss: 0.0158 - val_loss: 53.4494 - val_dense_3_loss: 0.0929 - val_dense_4_loss: 21.3022 - val_dense_5_loss: 0.0214\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 51.15982\n",
            "Epoch 86/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 51.0319 - dense_3_loss: 0.0889 - dense_4_loss: 21.6907 - dense_5_loss: 0.0133 - val_loss: 51.3861 - val_dense_3_loss: 0.0909 - val_dense_4_loss: 21.6120 - val_dense_5_loss: 0.0126\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 51.15982\n",
            "Epoch 87/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 50.2553 - dense_3_loss: 0.0876 - dense_4_loss: 21.4207 - dense_5_loss: 0.0127 - val_loss: 51.9596 - val_dense_3_loss: 0.0962 - val_dense_4_loss: 21.3305 - val_dense_5_loss: 0.0089\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 51.15982\n",
            "Epoch 88/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 50.2309 - dense_3_loss: 0.0871 - dense_4_loss: 21.2606 - dense_5_loss: 0.0141 - val_loss: 53.2710 - val_dense_3_loss: 0.0891 - val_dense_4_loss: 20.8468 - val_dense_5_loss: 0.0285\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 51.15982\n",
            "Epoch 89/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 50.3904 - dense_3_loss: 0.0874 - dense_4_loss: 21.2726 - dense_5_loss: 0.0145 - val_loss: 50.7422 - val_dense_3_loss: 0.0910 - val_dense_4_loss: 21.0143 - val_dense_5_loss: 0.0121\n",
            "\n",
            "Epoch 00089: val_loss improved from 51.15982 to 50.74218, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 90/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 50.8274 - dense_3_loss: 0.0894 - dense_4_loss: 21.0217 - dense_5_loss: 0.0149 - val_loss: 49.8130 - val_dense_3_loss: 0.0909 - val_dense_4_loss: 20.6958 - val_dense_5_loss: 0.0092\n",
            "\n",
            "Epoch 00090: val_loss improved from 50.74218 to 49.81305, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 91/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 49.4481 - dense_3_loss: 0.0875 - dense_4_loss: 20.7450 - dense_5_loss: 0.0122 - val_loss: 52.1035 - val_dense_3_loss: 0.0924 - val_dense_4_loss: 20.8371 - val_dense_5_loss: 0.0178\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 49.81305\n",
            "Epoch 92/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 49.8567 - dense_3_loss: 0.0877 - dense_4_loss: 20.6084 - dense_5_loss: 0.0147 - val_loss: 52.9646 - val_dense_3_loss: 0.0986 - val_dense_4_loss: 21.5007 - val_dense_5_loss: 0.0094\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 49.81305\n",
            "Epoch 93/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 50.1552 - dense_3_loss: 0.0888 - dense_4_loss: 20.7184 - dense_5_loss: 0.0140 - val_loss: 53.0061 - val_dense_3_loss: 0.0940 - val_dense_4_loss: 22.6393 - val_dense_5_loss: 0.0108\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 49.81305\n",
            "Epoch 94/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 49.0040 - dense_3_loss: 0.0868 - dense_4_loss: 20.4386 - dense_5_loss: 0.0126 - val_loss: 51.5441 - val_dense_3_loss: 0.0894 - val_dense_4_loss: 21.0177 - val_dense_5_loss: 0.0185\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 49.81305\n",
            "Epoch 95/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 49.7676 - dense_3_loss: 0.0868 - dense_4_loss: 20.2302 - dense_5_loss: 0.0175 - val_loss: 50.6836 - val_dense_3_loss: 0.0893 - val_dense_4_loss: 20.1698 - val_dense_5_loss: 0.0186\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 49.81305\n",
            "Epoch 96/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 49.2353 - dense_3_loss: 0.0874 - dense_4_loss: 20.1652 - dense_5_loss: 0.0142 - val_loss: 49.9094 - val_dense_3_loss: 0.0893 - val_dense_4_loss: 20.6034 - val_dense_5_loss: 0.0125\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 49.81305\n",
            "Epoch 97/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 48.7133 - dense_3_loss: 0.0863 - dense_4_loss: 20.0266 - dense_5_loss: 0.0141 - val_loss: 49.2380 - val_dense_3_loss: 0.0912 - val_dense_4_loss: 19.8390 - val_dense_5_loss: 0.0102\n",
            "\n",
            "Epoch 00097: val_loss improved from 49.81305 to 49.23796, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 98/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 48.2574 - dense_3_loss: 0.0866 - dense_4_loss: 19.9247 - dense_5_loss: 0.0118 - val_loss: 52.6996 - val_dense_3_loss: 0.0978 - val_dense_4_loss: 20.1921 - val_dense_5_loss: 0.0159\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 49.23796\n",
            "Epoch 99/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 49.0903 - dense_3_loss: 0.0883 - dense_4_loss: 19.8267 - dense_5_loss: 0.0139 - val_loss: 50.2302 - val_dense_3_loss: 0.0941 - val_dense_4_loss: 19.5241 - val_dense_5_loss: 0.0123\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 49.23796\n",
            "Epoch 100/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 48.5285 - dense_3_loss: 0.0870 - dense_4_loss: 19.7236 - dense_5_loss: 0.0135 - val_loss: 50.9751 - val_dense_3_loss: 0.0933 - val_dense_4_loss: 20.1309 - val_dense_5_loss: 0.0142\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 49.23796\n",
            "Epoch 101/200\n",
            "38000/38000 [==============================] - 9s 226us/step - loss: 48.5904 - dense_3_loss: 0.0873 - dense_4_loss: 19.6715 - dense_5_loss: 0.0137 - val_loss: 51.1160 - val_dense_3_loss: 0.0936 - val_dense_4_loss: 19.9847 - val_dense_5_loss: 0.0153\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 49.23796\n",
            "Epoch 102/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 48.2363 - dense_3_loss: 0.0862 - dense_4_loss: 19.5709 - dense_5_loss: 0.0141 - val_loss: 49.0647 - val_dense_3_loss: 0.0921 - val_dense_4_loss: 19.2946 - val_dense_5_loss: 0.0107\n",
            "\n",
            "Epoch 00102: val_loss improved from 49.23796 to 49.06475, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 103/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 47.9880 - dense_3_loss: 0.0869 - dense_4_loss: 19.4872 - dense_5_loss: 0.0122 - val_loss: 48.7685 - val_dense_3_loss: 0.0894 - val_dense_4_loss: 19.5874 - val_dense_5_loss: 0.0118\n",
            "\n",
            "Epoch 00103: val_loss improved from 49.06475 to 48.76853, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 104/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 47.5874 - dense_3_loss: 0.0855 - dense_4_loss: 19.3636 - dense_5_loss: 0.0129 - val_loss: 50.3621 - val_dense_3_loss: 0.0903 - val_dense_4_loss: 19.3461 - val_dense_5_loss: 0.0197\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 48.76853\n",
            "Epoch 105/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 47.5602 - dense_3_loss: 0.0854 - dense_4_loss: 19.2718 - dense_5_loss: 0.0133 - val_loss: 50.2075 - val_dense_3_loss: 0.0909 - val_dense_4_loss: 19.6950 - val_dense_5_loss: 0.0162\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 48.76853\n",
            "Epoch 106/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 47.7804 - dense_3_loss: 0.0860 - dense_4_loss: 19.4284 - dense_5_loss: 0.0128 - val_loss: 52.2036 - val_dense_3_loss: 0.1022 - val_dense_4_loss: 19.2026 - val_dense_5_loss: 0.0117\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 48.76853\n",
            "Epoch 107/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 48.8318 - dense_3_loss: 0.0879 - dense_4_loss: 19.3661 - dense_5_loss: 0.0155 - val_loss: 52.7776 - val_dense_3_loss: 0.1004 - val_dense_4_loss: 20.1180 - val_dense_5_loss: 0.0127\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 48.76853\n",
            "Epoch 108/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 47.9168 - dense_3_loss: 0.0869 - dense_4_loss: 19.2875 - dense_5_loss: 0.0128 - val_loss: 51.7180 - val_dense_3_loss: 0.1010 - val_dense_4_loss: 19.6107 - val_dense_5_loss: 0.0091\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 48.76853\n",
            "Epoch 109/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 47.4711 - dense_3_loss: 0.0867 - dense_4_loss: 19.0530 - dense_5_loss: 0.0121 - val_loss: 49.2928 - val_dense_3_loss: 0.0915 - val_dense_4_loss: 18.8814 - val_dense_5_loss: 0.0148\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 48.76853\n",
            "Epoch 110/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 47.1364 - dense_3_loss: 0.0856 - dense_4_loss: 19.0452 - dense_5_loss: 0.0121 - val_loss: 48.5647 - val_dense_3_loss: 0.0918 - val_dense_4_loss: 19.0972 - val_dense_5_loss: 0.0097\n",
            "\n",
            "Epoch 00110: val_loss improved from 48.76853 to 48.56467, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 111/200\n",
            "38000/38000 [==============================] - 9s 227us/step - loss: 47.4076 - dense_3_loss: 0.0865 - dense_4_loss: 19.0938 - dense_5_loss: 0.0119 - val_loss: 50.5179 - val_dense_3_loss: 0.0996 - val_dense_4_loss: 18.8436 - val_dense_5_loss: 0.0089\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 48.56467\n",
            "Epoch 112/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 47.8400 - dense_3_loss: 0.0882 - dense_4_loss: 19.0329 - dense_5_loss: 0.0118 - val_loss: 48.6604 - val_dense_3_loss: 0.0919 - val_dense_4_loss: 18.8804 - val_dense_5_loss: 0.0111\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 48.56467\n",
            "Epoch 113/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.7382 - dense_3_loss: 0.0853 - dense_4_loss: 18.8269 - dense_5_loss: 0.0116 - val_loss: 48.5546 - val_dense_3_loss: 0.0908 - val_dense_4_loss: 18.9070 - val_dense_5_loss: 0.0120\n",
            "\n",
            "Epoch 00113: val_loss improved from 48.56467 to 48.55457, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 114/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.8001 - dense_3_loss: 0.0858 - dense_4_loss: 18.7799 - dense_5_loss: 0.0114 - val_loss: 49.0839 - val_dense_3_loss: 0.0918 - val_dense_4_loss: 19.5013 - val_dense_5_loss: 0.0102\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 48.55457\n",
            "Epoch 115/200\n",
            "38000/38000 [==============================] - 9s 240us/step - loss: 46.7372 - dense_3_loss: 0.0849 - dense_4_loss: 18.7023 - dense_5_loss: 0.0129 - val_loss: 48.3741 - val_dense_3_loss: 0.0893 - val_dense_4_loss: 19.6101 - val_dense_5_loss: 0.0099\n",
            "\n",
            "Epoch 00115: val_loss improved from 48.55457 to 48.37408, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 116/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.7732 - dense_3_loss: 0.0857 - dense_4_loss: 18.6643 - dense_5_loss: 0.0119 - val_loss: 48.2145 - val_dense_3_loss: 0.0885 - val_dense_4_loss: 19.0483 - val_dense_5_loss: 0.0131\n",
            "\n",
            "Epoch 00116: val_loss improved from 48.37408 to 48.21449, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 117/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 47.7055 - dense_3_loss: 0.0854 - dense_4_loss: 18.8042 - dense_5_loss: 0.0165 - val_loss: 49.9076 - val_dense_3_loss: 0.0939 - val_dense_4_loss: 19.5560 - val_dense_5_loss: 0.0110\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 48.21449\n",
            "Epoch 118/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.9802 - dense_3_loss: 0.0854 - dense_4_loss: 18.6319 - dense_5_loss: 0.0137 - val_loss: 48.4011 - val_dense_3_loss: 0.0892 - val_dense_4_loss: 19.1524 - val_dense_5_loss: 0.0124\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 48.21449\n",
            "Epoch 119/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.9861 - dense_3_loss: 0.0850 - dense_4_loss: 18.6999 - dense_5_loss: 0.0139 - val_loss: 47.2699 - val_dense_3_loss: 0.0895 - val_dense_4_loss: 18.8438 - val_dense_5_loss: 0.0079\n",
            "\n",
            "Epoch 00119: val_loss improved from 48.21449 to 47.26994, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 120/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.5954 - dense_3_loss: 0.0855 - dense_4_loss: 18.5439 - dense_5_loss: 0.0120 - val_loss: 49.1995 - val_dense_3_loss: 0.0893 - val_dense_4_loss: 19.2982 - val_dense_5_loss: 0.0156\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 47.26994\n",
            "Epoch 121/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.4543 - dense_3_loss: 0.0855 - dense_4_loss: 18.5347 - dense_5_loss: 0.0114 - val_loss: 50.6382 - val_dense_3_loss: 0.0935 - val_dense_4_loss: 18.7217 - val_dense_5_loss: 0.0194\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 47.26994\n",
            "Epoch 122/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 47.6435 - dense_3_loss: 0.0867 - dense_4_loss: 18.6623 - dense_5_loss: 0.0148 - val_loss: 51.3693 - val_dense_3_loss: 0.0995 - val_dense_4_loss: 19.3433 - val_dense_5_loss: 0.0109\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 47.26994\n",
            "Epoch 123/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 46.5989 - dense_3_loss: 0.0857 - dense_4_loss: 18.5178 - dense_5_loss: 0.0118 - val_loss: 48.7046 - val_dense_3_loss: 0.0938 - val_dense_4_loss: 18.7600 - val_dense_5_loss: 0.0091\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 47.26994\n",
            "Epoch 124/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 47.1937 - dense_3_loss: 0.0871 - dense_4_loss: 18.5400 - dense_5_loss: 0.0127 - val_loss: 50.5021 - val_dense_3_loss: 0.0989 - val_dense_4_loss: 18.6702 - val_dense_5_loss: 0.0108\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 47.26994\n",
            "Epoch 125/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 47.3327 - dense_3_loss: 0.0872 - dense_4_loss: 18.4750 - dense_5_loss: 0.0135 - val_loss: 48.2716 - val_dense_3_loss: 0.0923 - val_dense_4_loss: 18.9779 - val_dense_5_loss: 0.0080\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 47.26994\n",
            "Epoch 126/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.7468 - dense_3_loss: 0.0859 - dense_4_loss: 18.3488 - dense_5_loss: 0.0131 - val_loss: 49.0338 - val_dense_3_loss: 0.0937 - val_dense_4_loss: 18.8289 - val_dense_5_loss: 0.0105\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 47.26994\n",
            "Epoch 127/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.1688 - dense_3_loss: 0.0847 - dense_4_loss: 18.3572 - dense_5_loss: 0.0120 - val_loss: 47.3816 - val_dense_3_loss: 0.0915 - val_dense_4_loss: 18.2546 - val_dense_5_loss: 0.0083\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 47.26994\n",
            "Epoch 128/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.1050 - dense_3_loss: 0.0841 - dense_4_loss: 18.3814 - dense_5_loss: 0.0124 - val_loss: 50.1155 - val_dense_3_loss: 0.0905 - val_dense_4_loss: 19.5759 - val_dense_5_loss: 0.0170\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 47.26994\n",
            "Epoch 129/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.8143 - dense_3_loss: 0.0840 - dense_4_loss: 18.3355 - dense_5_loss: 0.0114 - val_loss: 48.4264 - val_dense_3_loss: 0.0953 - val_dense_4_loss: 18.3105 - val_dense_5_loss: 0.0076\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 47.26994\n",
            "Epoch 130/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.3542 - dense_3_loss: 0.0852 - dense_4_loss: 18.1530 - dense_5_loss: 0.0132 - val_loss: 51.4781 - val_dense_3_loss: 0.0934 - val_dense_4_loss: 19.5981 - val_dense_5_loss: 0.0193\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 47.26994\n",
            "Epoch 131/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.5541 - dense_3_loss: 0.0856 - dense_4_loss: 18.4229 - dense_5_loss: 0.0123 - val_loss: 48.6115 - val_dense_3_loss: 0.0912 - val_dense_4_loss: 18.5595 - val_dense_5_loss: 0.0134\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 47.26994\n",
            "Epoch 132/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 47.0058 - dense_3_loss: 0.0866 - dense_4_loss: 18.4099 - dense_5_loss: 0.0131 - val_loss: 48.3291 - val_dense_3_loss: 0.0917 - val_dense_4_loss: 18.7802 - val_dense_5_loss: 0.0101\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 47.26994\n",
            "Epoch 133/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 46.2775 - dense_3_loss: 0.0846 - dense_4_loss: 18.2855 - dense_5_loss: 0.0131 - val_loss: 49.2083 - val_dense_3_loss: 0.0917 - val_dense_4_loss: 18.8123 - val_dense_5_loss: 0.0144\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 47.26994\n",
            "Epoch 134/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.7422 - dense_3_loss: 0.0854 - dense_4_loss: 18.4464 - dense_5_loss: 0.0134 - val_loss: 49.4473 - val_dense_3_loss: 0.0942 - val_dense_4_loss: 18.7243 - val_dense_5_loss: 0.0123\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 47.26994\n",
            "Epoch 135/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.2325 - dense_3_loss: 0.0840 - dense_4_loss: 18.2022 - dense_5_loss: 0.0142 - val_loss: 46.9453 - val_dense_3_loss: 0.0890 - val_dense_4_loss: 18.2598 - val_dense_5_loss: 0.0100\n",
            "\n",
            "Epoch 00135: val_loss improved from 47.26994 to 46.94534, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 136/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.8105 - dense_3_loss: 0.0836 - dense_4_loss: 18.2091 - dense_5_loss: 0.0125 - val_loss: 52.9711 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 19.5218 - val_dense_5_loss: 0.0278\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 46.94534\n",
            "Epoch 137/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 47.5585 - dense_3_loss: 0.0846 - dense_4_loss: 18.2672 - dense_5_loss: 0.0195 - val_loss: 48.6968 - val_dense_3_loss: 0.0961 - val_dense_4_loss: 18.2712 - val_dense_5_loss: 0.0080\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 46.94534\n",
            "Epoch 138/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.3639 - dense_3_loss: 0.0859 - dense_4_loss: 18.2086 - dense_5_loss: 0.0120 - val_loss: 46.9018 - val_dense_3_loss: 0.0897 - val_dense_4_loss: 18.3485 - val_dense_5_loss: 0.0082\n",
            "\n",
            "Epoch 00138: val_loss improved from 46.94534 to 46.90182, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 139/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.6833 - dense_3_loss: 0.0843 - dense_4_loss: 18.0904 - dense_5_loss: 0.0115 - val_loss: 49.7490 - val_dense_3_loss: 0.0924 - val_dense_4_loss: 18.0768 - val_dense_5_loss: 0.0198\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 46.90182\n",
            "Epoch 140/200\n",
            "38000/38000 [==============================] - 9s 228us/step - loss: 45.5289 - dense_3_loss: 0.0838 - dense_4_loss: 18.0926 - dense_5_loss: 0.0115 - val_loss: 48.8032 - val_dense_3_loss: 0.0925 - val_dense_4_loss: 19.2079 - val_dense_5_loss: 0.0092\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 46.90182\n",
            "Epoch 141/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.6114 - dense_3_loss: 0.0839 - dense_4_loss: 18.1115 - dense_5_loss: 0.0116 - val_loss: 51.5183 - val_dense_3_loss: 0.1050 - val_dense_4_loss: 18.3731 - val_dense_5_loss: 0.0082\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 46.90182\n",
            "Epoch 142/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 46.5945 - dense_3_loss: 0.0852 - dense_4_loss: 18.1110 - dense_5_loss: 0.0146 - val_loss: 48.0649 - val_dense_3_loss: 0.0925 - val_dense_4_loss: 18.0831 - val_dense_5_loss: 0.0112\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 46.90182\n",
            "Epoch 143/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.7246 - dense_3_loss: 0.0847 - dense_4_loss: 18.1096 - dense_5_loss: 0.0111 - val_loss: 49.9281 - val_dense_3_loss: 0.0902 - val_dense_4_loss: 18.4302 - val_dense_5_loss: 0.0222\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 46.90182\n",
            "Epoch 144/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 46.4624 - dense_3_loss: 0.0841 - dense_4_loss: 18.1459 - dense_5_loss: 0.0154 - val_loss: 49.4016 - val_dense_3_loss: 0.0917 - val_dense_4_loss: 18.7799 - val_dense_5_loss: 0.0156\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 46.90182\n",
            "Epoch 145/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.7310 - dense_3_loss: 0.0836 - dense_4_loss: 18.0380 - dense_5_loss: 0.0130 - val_loss: 47.7781 - val_dense_3_loss: 0.0919 - val_dense_4_loss: 18.4000 - val_dense_5_loss: 0.0090\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 46.90182\n",
            "Epoch 146/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.3871 - dense_3_loss: 0.0834 - dense_4_loss: 18.1135 - dense_5_loss: 0.0113 - val_loss: 47.6996 - val_dense_3_loss: 0.0905 - val_dense_4_loss: 18.0473 - val_dense_5_loss: 0.0126\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 46.90182\n",
            "Epoch 147/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.0481 - dense_3_loss: 0.0826 - dense_4_loss: 17.9094 - dense_5_loss: 0.0117 - val_loss: 47.3981 - val_dense_3_loss: 0.0900 - val_dense_4_loss: 18.4379 - val_dense_5_loss: 0.0098\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 46.90182\n",
            "Epoch 148/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.1754 - dense_3_loss: 0.0825 - dense_4_loss: 17.8943 - dense_5_loss: 0.0126 - val_loss: 47.3473 - val_dense_3_loss: 0.0918 - val_dense_4_loss: 18.1691 - val_dense_5_loss: 0.0082\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 46.90182\n",
            "Epoch 149/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.8143 - dense_3_loss: 0.0837 - dense_4_loss: 18.1217 - dense_5_loss: 0.0129 - val_loss: 47.7268 - val_dense_3_loss: 0.0914 - val_dense_4_loss: 18.2269 - val_dense_5_loss: 0.0104\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 46.90182\n",
            "Epoch 150/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.6019 - dense_3_loss: 0.0843 - dense_4_loss: 18.0343 - dense_5_loss: 0.0114 - val_loss: 48.2771 - val_dense_3_loss: 0.0932 - val_dense_4_loss: 18.6320 - val_dense_5_loss: 0.0084\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 46.90182\n",
            "Epoch 151/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.7540 - dense_3_loss: 0.0844 - dense_4_loss: 18.0765 - dense_5_loss: 0.0118 - val_loss: 50.6580 - val_dense_3_loss: 0.0956 - val_dense_4_loss: 20.0438 - val_dense_5_loss: 0.0097\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 46.90182\n",
            "Epoch 152/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.4863 - dense_3_loss: 0.0834 - dense_4_loss: 18.0836 - dense_5_loss: 0.0120 - val_loss: 52.8279 - val_dense_3_loss: 0.0945 - val_dense_4_loss: 18.2433 - val_dense_5_loss: 0.0311\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 46.90182\n",
            "Epoch 153/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.2586 - dense_3_loss: 0.0838 - dense_4_loss: 17.9235 - dense_5_loss: 0.0160 - val_loss: 47.9960 - val_dense_3_loss: 0.0922 - val_dense_4_loss: 18.0671 - val_dense_5_loss: 0.0114\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 46.90182\n",
            "Epoch 154/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.4718 - dense_3_loss: 0.0840 - dense_4_loss: 17.9792 - dense_5_loss: 0.0115 - val_loss: 46.6491 - val_dense_3_loss: 0.0898 - val_dense_4_loss: 18.0620 - val_dense_5_loss: 0.0083\n",
            "\n",
            "Epoch 00154: val_loss improved from 46.90182 to 46.64907, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 155/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 44.8523 - dense_3_loss: 0.0827 - dense_4_loss: 17.8798 - dense_5_loss: 0.0108 - val_loss: 48.8957 - val_dense_3_loss: 0.0922 - val_dense_4_loss: 18.3282 - val_dense_5_loss: 0.0145\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 46.64907\n",
            "Epoch 156/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.3309 - dense_3_loss: 0.0833 - dense_4_loss: 17.9484 - dense_5_loss: 0.0119 - val_loss: 48.3236 - val_dense_3_loss: 0.0915 - val_dense_4_loss: 18.8389 - val_dense_5_loss: 0.0102\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 46.64907\n",
            "Epoch 157/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.7643 - dense_3_loss: 0.0829 - dense_4_loss: 18.0125 - dense_5_loss: 0.0144 - val_loss: 48.8410 - val_dense_3_loss: 0.0902 - val_dense_4_loss: 18.0391 - val_dense_5_loss: 0.0188\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 46.64907\n",
            "Epoch 158/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.0160 - dense_3_loss: 0.0819 - dense_4_loss: 17.7926 - dense_5_loss: 0.0133 - val_loss: 49.2092 - val_dense_3_loss: 0.0934 - val_dense_4_loss: 18.0954 - val_dense_5_loss: 0.0155\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 46.64907\n",
            "Epoch 159/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 46.0739 - dense_3_loss: 0.0841 - dense_4_loss: 18.0130 - dense_5_loss: 0.0142 - val_loss: 47.8664 - val_dense_3_loss: 0.0918 - val_dense_4_loss: 18.1428 - val_dense_5_loss: 0.0109\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 46.64907\n",
            "Epoch 160/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 44.8845 - dense_3_loss: 0.0825 - dense_4_loss: 17.8807 - dense_5_loss: 0.0113 - val_loss: 47.7132 - val_dense_3_loss: 0.0909 - val_dense_4_loss: 18.2864 - val_dense_5_loss: 0.0107\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 46.64907\n",
            "Epoch 161/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.2832 - dense_3_loss: 0.0834 - dense_4_loss: 17.8166 - dense_5_loss: 0.0122 - val_loss: 53.5542 - val_dense_3_loss: 0.1033 - val_dense_4_loss: 18.3024 - val_dense_5_loss: 0.0214\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 46.64907\n",
            "Epoch 162/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.3563 - dense_3_loss: 0.0829 - dense_4_loss: 17.8883 - dense_5_loss: 0.0130 - val_loss: 49.7957 - val_dense_3_loss: 0.0913 - val_dense_4_loss: 18.3832 - val_dense_5_loss: 0.0201\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 46.64907\n",
            "Epoch 163/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.5540 - dense_3_loss: 0.0825 - dense_4_loss: 17.8857 - dense_5_loss: 0.0146 - val_loss: 51.2116 - val_dense_3_loss: 0.0951 - val_dense_4_loss: 18.0771 - val_dense_5_loss: 0.0230\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 46.64907\n",
            "Epoch 164/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 45.5018 - dense_3_loss: 0.0827 - dense_4_loss: 17.7384 - dense_5_loss: 0.0147 - val_loss: 48.2406 - val_dense_3_loss: 0.0934 - val_dense_4_loss: 17.9058 - val_dense_5_loss: 0.0115\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 46.64907\n",
            "Epoch 165/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 45.1361 - dense_3_loss: 0.0828 - dense_4_loss: 17.8250 - dense_5_loss: 0.0123 - val_loss: 47.1290 - val_dense_3_loss: 0.0897 - val_dense_4_loss: 18.0193 - val_dense_5_loss: 0.0110\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 46.64907\n",
            "Epoch 166/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 44.1968 - dense_3_loss: 0.0813 - dense_4_loss: 17.6608 - dense_5_loss: 0.0107 - val_loss: 47.7860 - val_dense_3_loss: 0.0923 - val_dense_4_loss: 18.3981 - val_dense_5_loss: 0.0085\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 46.64907\n",
            "Epoch 167/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.3559 - dense_3_loss: 0.0825 - dense_4_loss: 17.8204 - dense_5_loss: 0.0140 - val_loss: 49.7774 - val_dense_3_loss: 0.0961 - val_dense_4_loss: 18.3143 - val_dense_5_loss: 0.0132\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 46.64907\n",
            "Epoch 168/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 45.4809 - dense_3_loss: 0.0830 - dense_4_loss: 17.7882 - dense_5_loss: 0.0139 - val_loss: 51.3978 - val_dense_3_loss: 0.1043 - val_dense_4_loss: 18.3271 - val_dense_5_loss: 0.0089\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 46.64907\n",
            "Epoch 169/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 45.4498 - dense_3_loss: 0.0833 - dense_4_loss: 17.7701 - dense_5_loss: 0.0134 - val_loss: 50.1323 - val_dense_3_loss: 0.0916 - val_dense_4_loss: 19.6499 - val_dense_5_loss: 0.0150\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 46.64907\n",
            "Epoch 170/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 44.2704 - dense_3_loss: 0.0813 - dense_4_loss: 17.6936 - dense_5_loss: 0.0109 - val_loss: 46.9495 - val_dense_3_loss: 0.0903 - val_dense_4_loss: 18.2192 - val_dense_5_loss: 0.0082\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 46.64907\n",
            "Epoch 171/200\n",
            "38000/38000 [==============================] - 9s 229us/step - loss: 44.0470 - dense_3_loss: 0.0806 - dense_4_loss: 17.6545 - dense_5_loss: 0.0110 - val_loss: 47.0614 - val_dense_3_loss: 0.0909 - val_dense_4_loss: 18.0264 - val_dense_5_loss: 0.0088\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 46.64907\n",
            "Epoch 172/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.0919 - dense_3_loss: 0.0820 - dense_4_loss: 17.7547 - dense_5_loss: 0.0137 - val_loss: 49.6373 - val_dense_3_loss: 0.0940 - val_dense_4_loss: 18.8257 - val_dense_5_loss: 0.0131\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 46.64907\n",
            "Epoch 173/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 45.1465 - dense_3_loss: 0.0824 - dense_4_loss: 17.6797 - dense_5_loss: 0.0137 - val_loss: 47.7069 - val_dense_3_loss: 0.0917 - val_dense_4_loss: 18.5486 - val_dense_5_loss: 0.0083\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 46.64907\n",
            "Epoch 174/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 44.3286 - dense_3_loss: 0.0811 - dense_4_loss: 17.7136 - dense_5_loss: 0.0114 - val_loss: 50.9400 - val_dense_3_loss: 0.1000 - val_dense_4_loss: 18.8231 - val_dense_5_loss: 0.0106\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 46.64907\n",
            "Epoch 175/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 45.0871 - dense_3_loss: 0.0828 - dense_4_loss: 17.8688 - dense_5_loss: 0.0118 - val_loss: 48.2144 - val_dense_3_loss: 0.0930 - val_dense_4_loss: 17.8914 - val_dense_5_loss: 0.0121\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 46.64907\n",
            "Epoch 176/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 44.7535 - dense_3_loss: 0.0818 - dense_4_loss: 17.6409 - dense_5_loss: 0.0128 - val_loss: 46.7969 - val_dense_3_loss: 0.0898 - val_dense_4_loss: 18.1573 - val_dense_5_loss: 0.0085\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 46.64907\n",
            "Epoch 177/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 44.3391 - dense_3_loss: 0.0813 - dense_4_loss: 17.6532 - dense_5_loss: 0.0115 - val_loss: 47.5760 - val_dense_3_loss: 0.0896 - val_dense_4_loss: 18.7645 - val_dense_5_loss: 0.0096\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 46.64907\n",
            "Epoch 178/200\n",
            "38000/38000 [==============================] - 9s 233us/step - loss: 44.2345 - dense_3_loss: 0.0810 - dense_4_loss: 17.6222 - dense_5_loss: 0.0116 - val_loss: 48.9980 - val_dense_3_loss: 0.0894 - val_dense_4_loss: 17.9273 - val_dense_5_loss: 0.0213\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 46.64907\n",
            "Epoch 179/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 45.1228 - dense_3_loss: 0.0817 - dense_4_loss: 17.5728 - dense_5_loss: 0.0152 - val_loss: 47.8347 - val_dense_3_loss: 0.0929 - val_dense_4_loss: 18.1472 - val_dense_5_loss: 0.0091\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 46.64907\n",
            "Epoch 180/200\n",
            "38000/38000 [==============================] - 9s 233us/step - loss: 44.2851 - dense_3_loss: 0.0809 - dense_4_loss: 17.6477 - dense_5_loss: 0.0118 - val_loss: 47.9343 - val_dense_3_loss: 0.0907 - val_dense_4_loss: 18.3293 - val_dense_5_loss: 0.0119\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 46.64907\n",
            "Epoch 181/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 44.1352 - dense_3_loss: 0.0801 - dense_4_loss: 17.5442 - dense_5_loss: 0.0128 - val_loss: 52.2400 - val_dense_3_loss: 0.0927 - val_dense_4_loss: 18.7618 - val_dense_5_loss: 0.0284\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 46.64907\n",
            "Epoch 182/200\n",
            "38000/38000 [==============================] - 9s 230us/step - loss: 44.7373 - dense_3_loss: 0.0810 - dense_4_loss: 17.6236 - dense_5_loss: 0.0140 - val_loss: 47.2923 - val_dense_3_loss: 0.0904 - val_dense_4_loss: 18.6053 - val_dense_5_loss: 0.0078\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 46.64907\n",
            "Epoch 183/200\n",
            "38000/38000 [==============================] - 9s 232us/step - loss: 44.2712 - dense_3_loss: 0.0816 - dense_4_loss: 17.5213 - dense_5_loss: 0.0114 - val_loss: 47.2253 - val_dense_3_loss: 0.0906 - val_dense_4_loss: 18.2628 - val_dense_5_loss: 0.0089\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 46.64907\n",
            "Epoch 184/200\n",
            "38000/38000 [==============================] - 9s 231us/step - loss: 44.5215 - dense_3_loss: 0.0809 - dense_4_loss: 17.6996 - dense_5_loss: 0.0128 - val_loss: 48.5374 - val_dense_3_loss: 0.0927 - val_dense_4_loss: 18.2871 - val_dense_5_loss: 0.0122\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 46.64907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PTmTP4dZBWa",
        "colab_type": "code",
        "outputId": "adb409e7-5a65-45da-e8fb-3ead0124b89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "# MODELPATH1=\"CNN_11_model.hdf5\"\n",
        "if os.path.exists(MODELPATH):\n",
        "#     model.load_weights(MODELPATH)\n",
        "      model=load_model(MODELPATH)\n",
        "    # 若成功加载前面保存的参数，输出下列信息\n",
        "      print(\"checkpoint_loaded\")\n",
        "# Y_hat = model.predict(X)\n",
        "# X1 = np.concatenate([x_train1,x_val1])\n",
        "# X2 = np.concatenate([x_train2,x_val2])\n",
        "X_1,X_2 = resize_input(X)\n",
        "Y_hat = model.predict([X_1,X_2])\n",
        "# Y_hat2 = model.predict([x_val1,x_val2])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint_loaded\n",
            "X1 shape: (47500, 5000, 1)  X2 shape: (47500, 100, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhhzJldgpGS7",
        "colab_type": "code",
        "outputId": "9631c9c7-fae8-42e4-f872-67a9bbdfcc4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_hat=np.array(Y_hat)\n",
        "Y_hat=Y_hat.reshape(3,-1)\n",
        "print(Y_hat.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 47500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI2wuqumcGAA",
        "colab_type": "code",
        "outputId": "3384c4d1-8477-4834-aa09-8dd3dc6c5bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Y_hat=np.array(Y_hat)\n",
        "# Y_hat = Y_hat.reshape(3,42750)\n",
        "# Y_hat = np.transpose(Y_hat)\n",
        "# print(Y_hat.shape)\n",
        "# Y_hat2=np.array(Y_hat2)\n",
        "# Y_hat2 = Y_hat2.reshape(3,4750)\n",
        "# Y_hat2 = np.transpose(Y_hat2)\n",
        "# print(Y_hat2.shape)\n",
        "# Y_hat = np.concatenate([Y_hat,Y_hat2])\n",
        "# print(Y_hat.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-92a1ffc293ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_hat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42750\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_hat2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 142500 into shape (3,42750)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwyeRzI4lOXw",
        "colab_type": "code",
        "outputId": "46f27a74-e96d-4219-c7d7-1828ac66a8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# yy=np.concatenate([y_train,y_val])\n",
        "# print(yy.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Uc6fdFZNgH",
        "colab_type": "code",
        "outputId": "c274501f-ef79-439d-b8ba-4ecde5e2eab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Y_hat = model.predict(X)\n",
        "# Y_hat=np.array(Y_hat)\n",
        "# Y_hat = Y_hat.reshape(3,X1.shape[0])\n",
        "# Y_hat=np.array(Y_hat)\n",
        "Y_hat = np.transpose(Y_hat)\n",
        "# Y_hat = np.transpose(Y_hat).reshape(-1,3)\n",
        "print(Y_hat.shape)\n",
        "err_mat = np.abs(Y-Y_hat)/Y\n",
        "err_mat = np.sum(err_mat,axis=1)\n",
        "err_mat = np.sum(err_mat)\n",
        "print(err_mat/Y.shape[0])\n",
        "#0.9704514192587647\n",
        "#10000d 0.9945707841335775\n",
        "#0.5936615853774215\n",
        "#0.32381799619055995"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3)\n",
            "0.44162815180735504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy68LySzkebL",
        "colab_type": "code",
        "outputId": "0e338496-fa3d-4113-b34c-3477328f5335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(Y_hat.shape,Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47500, 3) (47500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkMg_43QUYlH",
        "colab_type": "code",
        "outputId": "10d675bd-bb0f-422a-80e5-6bef5a12f7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "err_WMAE = np.dot(np.abs(Y-Y_hat),np.array([300,1,200]))\n",
        "err_WMAE = np.sum(err_WMAE)/Y.shape[0]\n",
        "print(err_WMAE)\n",
        "#145.68255323961225\n",
        "#25.64659003243506\n",
        "\n",
        "##49.38923480439816"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43.63045013435174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23qSUi4qEn63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('MLP_WMAE_lastEpoch.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t15AqhB-aUG7",
        "colab_type": "code",
        "outputId": "5d23a04f-46c6-4c24-a692-d33478aa7978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtest_1,Xtest_2 = resize_input(x_test)\n",
        "Y_test_hat = np.array(model.predict([Xtest_1,Xtest_2]))\n",
        "Y_test_hat = Y_test_hat.reshape(3,-1)\n",
        "Y_test_hat = np.transpose(Y_test_hat)\n",
        "\n",
        "Y_test_hat[:,0]=np.clip(Y_test_hat[:,0],0,1)\n",
        "Y_test_hat[:,1]=np.clip(Y_test_hat[:,1],25,250)\n",
        "Y_test_hat[:,2]=np.clip(Y_test_hat[:,2],0.5,1)\n",
        "\n",
        "np.savetxt(dirPath+'CNN_16.csv',Y_test_hat,delimiter=',')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 shape: (2500, 5000, 1)  X2 shape: (2500, 100, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n34h467dHye",
        "colab_type": "code",
        "outputId": "4a4da004-860c-4d31-9a2c-750421053601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "!zip cnn_output_16 MLP_output/*"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: MLP_output/CNN_16.csv (deflated 61%)\n",
            "  adding: MLP_output/MLP_history.pickle (deflated 53%)\n",
            "  adding: MLP_output/MLP_model.hdf5 (deflated 15%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPtpSwm0dUaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv cnn_output_16.zip '/content/gdrive/My Drive/term_2019_1/MLTech/Final'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJNSUc9KxFIi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_0fZ47Sw5uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv MLP_output/MLP_history.pickle MLP_output/MLP_history1.pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra05uSdXW7KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/term_2019_1/MLTech/Final/cnn_output_11.zip' ./\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntSNW9WCqXl6",
        "colab_type": "code",
        "outputId": "77472d99-7992-454b-9f27-688e4f3c5bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!unzip cnn_output_11.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cnn_output_11.zip\n",
            "replace MLP_output/CNN_11.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: r\n",
            "new name: CNN_11_1.csv\n",
            "  inflating: CNN_11_1.csv            \n",
            "replace MLP_output/MLP_history.pickle? [y]es, [n]o, [A]ll, [N]one, [r]ename: r\n",
            "new name: CNN_11_hist.pickle\n",
            "  inflating: CNN_11_hist.pickle      \n",
            "replace MLP_output/MLP_model.hdf5? [y]es, [n]o, [A]ll, [N]one, [r]ename: r\n",
            "new name: CNN_11_model.hdf5\n",
            "  inflating: CNN_11_model.hdf5       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX9TbfnqqcI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm MLP_output/CNN_11_2.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHlgQojssgbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}