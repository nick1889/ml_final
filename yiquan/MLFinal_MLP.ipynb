{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLFinal-MLP.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAPXN1XeRWlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c116f4c6-5ade-4d3e-9232-3f648b8608ae"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC-QrcB8SfYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d06e7fc-d3b5-49c9-dd84-eac2261123cf"
      },
      "source": [
        "!wget https://learner.csie.ntu.edu.tw/~judge/ml19spring/ml19spring.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-02 08:24:14--  https://learner.csie.ntu.edu.tw/~judge/ml19spring/ml19spring.zip\n",
            "Resolving learner.csie.ntu.edu.tw (learner.csie.ntu.edu.tw)... 140.112.90.193\n",
            "Connecting to learner.csie.ntu.edu.tw (learner.csie.ntu.edu.tw)|140.112.90.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3780741206 (3.5G) [application/zip]\n",
            "Saving to: ‘ml19spring.zip’\n",
            "\n",
            "ml19spring.zip      100%[===================>]   3.52G  16.3MB/s    in 5m 8s   \n",
            "\n",
            "2019-06-02 08:29:24 (11.7 MB/s) - ‘ml19spring.zip’ saved [3780741206/3780741206]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqFi8yQLSlm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "2635e8ab-4f43-4a73-a287-3c02416f1789"
      },
      "source": [
        "!unzip -q ml19spring.zip\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ml19spring.zip\tsample_data  X_test.npz  X_train.npz  Y_train.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThyMATs2TBoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x_test = np.load('X_test.npz')['arr_0']\n",
        "X = np.load('X_train.npz')['arr_0']\n",
        "Y = np.load('Y_train.npz')['arr_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gab1tIdQTze0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dirPath = './MLP_output/'\n",
        "!mkdir ./MLP_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPeL0Z6ewL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_fea = np.load('important_feat.npy')\n",
        "X=X[:,important_fea]\n",
        "x_test = x_test[:,important_fea]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0otbP-OUN9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af6cac7f-ca86-471c-a87d-126028e4ea92"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "x_train,x_val,y_train,y_val=train_test_split(X,Y,test_size=0.1,random_state=44)\n",
        "print(x_train.shape,x_val.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(42750, 3752) (4750, 3752)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf5eojAHUdCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6038
        },
        "outputId": "93a691ea-79ce-4441-c5d8-cbd04959dc11"
      },
      "source": [
        "import pickle\n",
        "from keras.layers import *\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "def baseline_MLP(input_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512,input_dim=input_size,kernel_initializer='normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(128,kernel_initializer='normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(3,kernel_initializer='normal'))\n",
        "    model.compile(loss = 'mean_absolute_percentage_error',optimizer='adam')\n",
        "    return model\n",
        "def baseline_MLP_WMAE(input_size):\n",
        "    m_input = Input(shape=(input_size,), dtype='float32')\n",
        "    X = Dense(512,kernel_initializer='normal')(m_input)\n",
        "    X = PReLU()(X)\n",
        "    X = Dense(128,kernel_initializer='normal')(X)\n",
        "    X = PReLU()(X)\n",
        "    X1 = Dense(68,kernel_initializer='normal',activation = 'tanh')(X)\n",
        "    X2 = Dense(68,kernel_initializer='normal',activation = 'tanh')(X)\n",
        "    X3 = Dense(68,kernel_initializer='normal',activation = 'tanh')(X)\n",
        "    y1 = Dense(1,kernel_initializer='normal')(X1)\n",
        "    y2 = Dense(1,kernel_initializer='normal')(X2)\n",
        "    y3 = Dense(1,kernel_initializer='normal')(X3)\n",
        "    model = Model(inputs=m_input, outputs=[y1,y2,y3])\n",
        "    model.compile(optimizer='adam', loss='mean_absolute_error',\n",
        "              loss_weights=[300,1,200])\n",
        "    return model\n",
        "  \n",
        "dirPath = './MLP_output/' \n",
        "model = baseline_MLP_WMAE(x_train.shape[1])\n",
        "model.summary()\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "# checkpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss',patience=10)\n",
        "MODELPATH=dirPath+\"MLP_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(MODELPATH, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "print(y_train.shape)\n",
        "y_train_T = np.transpose(y_train)\n",
        "print(y_train_T.shape)\n",
        "y_val_T = np.transpose(y_val)\n",
        "print(y_val_T.shape)\n",
        "history = model.fit(x=x_train,y=[y_train_T[0],y_train_T[1],y_train_T[2]],batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "        validation_data=(x_val,[y_val_T[0],y_val_T[1],y_val_T[2]]),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "file_his = open(dirPath+'MLP_history.pickle', 'wb')\n",
        "pickle.dump(history.history, file_his)\n",
        "file_his.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 3752)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_47 (Dense)                (None, 512)          1921536     input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_23 (PReLU)              (None, 512)          512         dense_47[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (None, 128)          65664       p_re_lu_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_24 (PReLU)              (None, 128)          128         dense_48[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_49 (Dense)                (None, 68)           8772        p_re_lu_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_50 (Dense)                (None, 68)           8772        p_re_lu_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_51 (Dense)                (None, 68)           8772        p_re_lu_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_52 (Dense)                (None, 1)            69          dense_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_53 (Dense)                (None, 1)            69          dense_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_54 (Dense)                (None, 1)            69          dense_51[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,014,363\n",
            "Trainable params: 2,014,363\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "(42750, 3)\n",
            "(3, 42750)\n",
            "(3, 4750)\n",
            "Train on 42750 samples, validate on 4750 samples\n",
            "Epoch 1/100\n",
            "42750/42750 [==============================] - 8s 181us/step - loss: 195.5886 - dense_52_loss: 0.2497 - dense_53_loss: 112.1667 - dense_54_loss: 0.0425 - val_loss: 171.8077 - val_dense_52_loss: 0.2480 - val_dense_53_loss: 92.0514 - val_dense_54_loss: 0.0267\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 171.80769, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 2/100\n",
            "42750/42750 [==============================] - 6s 152us/step - loss: 158.8237 - dense_52_loss: 0.2438 - dense_53_loss: 79.4981 - dense_54_loss: 0.0309 - val_loss: 149.2446 - val_dense_52_loss: 0.2420 - val_dense_53_loss: 69.5731 - val_dense_54_loss: 0.0354\n",
            "\n",
            "Epoch 00002: val_loss improved from 171.80769 to 149.24458, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 3/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 136.6120 - dense_52_loss: 0.2364 - dense_53_loss: 59.3820 - dense_54_loss: 0.0316 - val_loss: 125.3837 - val_dense_52_loss: 0.2313 - val_dense_53_loss: 51.1475 - val_dense_54_loss: 0.0242\n",
            "\n",
            "Epoch 00003: val_loss improved from 149.24458 to 125.38367, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 4/100\n",
            "42750/42750 [==============================] - 7s 152us/step - loss: 122.2515 - dense_52_loss: 0.2320 - dense_53_loss: 46.7631 - dense_54_loss: 0.0295 - val_loss: 122.1013 - val_dense_52_loss: 0.2401 - val_dense_53_loss: 44.9351 - val_dense_54_loss: 0.0256\n",
            "\n",
            "Epoch 00004: val_loss improved from 125.38367 to 122.10132, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 5/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 114.2732 - dense_52_loss: 0.2296 - dense_53_loss: 40.1933 - dense_54_loss: 0.0260 - val_loss: 110.2599 - val_dense_52_loss: 0.2271 - val_dense_53_loss: 36.8027 - val_dense_54_loss: 0.0266\n",
            "\n",
            "Epoch 00005: val_loss improved from 122.10132 to 110.25991, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 6/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 110.8381 - dense_52_loss: 0.2271 - dense_53_loss: 37.4241 - dense_54_loss: 0.0264 - val_loss: 107.1669 - val_dense_52_loss: 0.2268 - val_dense_53_loss: 34.6187 - val_dense_54_loss: 0.0226\n",
            "\n",
            "Epoch 00006: val_loss improved from 110.25991 to 107.16690, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 7/100\n",
            "42750/42750 [==============================] - 7s 155us/step - loss: 108.8887 - dense_52_loss: 0.2263 - dense_53_loss: 35.7988 - dense_54_loss: 0.0260 - val_loss: 105.7570 - val_dense_52_loss: 0.2265 - val_dense_53_loss: 33.4894 - val_dense_54_loss: 0.0215\n",
            "\n",
            "Epoch 00007: val_loss improved from 107.16690 to 105.75705, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 8/100\n",
            "42750/42750 [==============================] - 7s 162us/step - loss: 105.2248 - dense_52_loss: 0.2245 - dense_53_loss: 33.3772 - dense_54_loss: 0.0225 - val_loss: 103.3147 - val_dense_52_loss: 0.2244 - val_dense_53_loss: 31.6902 - val_dense_54_loss: 0.0215\n",
            "\n",
            "Epoch 00008: val_loss improved from 105.75705 to 103.31468, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 9/100\n",
            "42750/42750 [==============================] - 7s 163us/step - loss: 104.4335 - dense_52_loss: 0.2234 - dense_53_loss: 32.9016 - dense_54_loss: 0.0226 - val_loss: 101.9558 - val_dense_52_loss: 0.2228 - val_dense_53_loss: 31.6737 - val_dense_54_loss: 0.0173\n",
            "\n",
            "Epoch 00009: val_loss improved from 103.31468 to 101.95582, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 10/100\n",
            "42750/42750 [==============================] - 7s 164us/step - loss: 104.4964 - dense_52_loss: 0.2234 - dense_53_loss: 32.7746 - dense_54_loss: 0.0235 - val_loss: 102.5741 - val_dense_52_loss: 0.2228 - val_dense_53_loss: 31.6963 - val_dense_54_loss: 0.0202\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 101.95582\n",
            "Epoch 11/100\n",
            "42750/42750 [==============================] - 7s 165us/step - loss: 102.4091 - dense_52_loss: 0.2210 - dense_53_loss: 31.8112 - dense_54_loss: 0.0214 - val_loss: 106.7019 - val_dense_52_loss: 0.2264 - val_dense_53_loss: 33.5980 - val_dense_54_loss: 0.0260\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 101.95582\n",
            "Epoch 12/100\n",
            "42750/42750 [==============================] - 7s 163us/step - loss: 102.4186 - dense_52_loss: 0.2203 - dense_53_loss: 31.8240 - dense_54_loss: 0.0225 - val_loss: 106.1065 - val_dense_52_loss: 0.2269 - val_dense_53_loss: 33.4791 - val_dense_54_loss: 0.0228\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 101.95582\n",
            "Epoch 13/100\n",
            "42750/42750 [==============================] - 7s 161us/step - loss: 101.3394 - dense_52_loss: 0.2190 - dense_53_loss: 31.4494 - dense_54_loss: 0.0210 - val_loss: 102.7304 - val_dense_52_loss: 0.2236 - val_dense_53_loss: 32.2687 - val_dense_54_loss: 0.0169\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 101.95582\n",
            "Epoch 14/100\n",
            "42750/42750 [==============================] - 7s 162us/step - loss: 101.0304 - dense_52_loss: 0.2186 - dense_53_loss: 31.2736 - dense_54_loss: 0.0209 - val_loss: 98.7447 - val_dense_52_loss: 0.2193 - val_dense_53_loss: 29.3931 - val_dense_54_loss: 0.0178\n",
            "\n",
            "Epoch 00014: val_loss improved from 101.95582 to 98.74471, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 15/100\n",
            "42750/42750 [==============================] - 7s 165us/step - loss: 100.0680 - dense_52_loss: 0.2169 - dense_53_loss: 30.9503 - dense_54_loss: 0.0202 - val_loss: 102.1547 - val_dense_52_loss: 0.2185 - val_dense_53_loss: 32.3864 - val_dense_54_loss: 0.0211\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 98.74471\n",
            "Epoch 16/100\n",
            "42750/42750 [==============================] - 7s 163us/step - loss: 100.0427 - dense_52_loss: 0.2171 - dense_53_loss: 30.8761 - dense_54_loss: 0.0202 - val_loss: 99.7577 - val_dense_52_loss: 0.2216 - val_dense_53_loss: 29.5982 - val_dense_54_loss: 0.0184\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 98.74471\n",
            "Epoch 17/100\n",
            "42750/42750 [==============================] - 7s 163us/step - loss: 99.2367 - dense_52_loss: 0.2161 - dense_53_loss: 30.5239 - dense_54_loss: 0.0195 - val_loss: 102.3835 - val_dense_52_loss: 0.2206 - val_dense_53_loss: 31.3796 - val_dense_54_loss: 0.0242\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 98.74471\n",
            "Epoch 18/100\n",
            "42750/42750 [==============================] - 7s 162us/step - loss: 98.8407 - dense_52_loss: 0.2149 - dense_53_loss: 30.5286 - dense_54_loss: 0.0192 - val_loss: 100.8050 - val_dense_52_loss: 0.2268 - val_dense_53_loss: 29.5874 - val_dense_54_loss: 0.0160\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 98.74471\n",
            "Epoch 19/100\n",
            "42750/42750 [==============================] - 7s 163us/step - loss: 97.9583 - dense_52_loss: 0.2137 - dense_53_loss: 30.1228 - dense_54_loss: 0.0187 - val_loss: 96.9569 - val_dense_52_loss: 0.2152 - val_dense_53_loss: 28.8078 - val_dense_54_loss: 0.0180\n",
            "\n",
            "Epoch 00019: val_loss improved from 98.74471 to 96.95687, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 20/100\n",
            "42750/42750 [==============================] - 7s 163us/step - loss: 97.8151 - dense_52_loss: 0.2133 - dense_53_loss: 30.1657 - dense_54_loss: 0.0183 - val_loss: 98.9892 - val_dense_52_loss: 0.2162 - val_dense_53_loss: 29.6587 - val_dense_54_loss: 0.0223\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 96.95687\n",
            "Epoch 21/100\n",
            "42750/42750 [==============================] - 7s 169us/step - loss: 97.6234 - dense_52_loss: 0.2134 - dense_53_loss: 29.9385 - dense_54_loss: 0.0183 - val_loss: 97.3079 - val_dense_52_loss: 0.2163 - val_dense_53_loss: 28.9639 - val_dense_54_loss: 0.0172\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 96.95687\n",
            "Epoch 22/100\n",
            "42750/42750 [==============================] - 7s 160us/step - loss: 97.0586 - dense_52_loss: 0.2122 - dense_53_loss: 29.8485 - dense_54_loss: 0.0177 - val_loss: 97.2934 - val_dense_52_loss: 0.2142 - val_dense_53_loss: 29.4299 - val_dense_54_loss: 0.0180\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 96.95687\n",
            "Epoch 23/100\n",
            "42750/42750 [==============================] - 7s 155us/step - loss: 96.2142 - dense_52_loss: 0.2108 - dense_53_loss: 29.5816 - dense_54_loss: 0.0169 - val_loss: 111.8217 - val_dense_52_loss: 0.2471 - val_dense_53_loss: 32.8459 - val_dense_54_loss: 0.0243\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 96.95687\n",
            "Epoch 24/100\n",
            "42750/42750 [==============================] - 6s 152us/step - loss: 96.5252 - dense_52_loss: 0.2114 - dense_53_loss: 29.6192 - dense_54_loss: 0.0174 - val_loss: 97.5225 - val_dense_52_loss: 0.2145 - val_dense_53_loss: 29.2359 - val_dense_54_loss: 0.0197\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 96.95687\n",
            "Epoch 25/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 96.4424 - dense_52_loss: 0.2108 - dense_53_loss: 29.6562 - dense_54_loss: 0.0177 - val_loss: 96.4165 - val_dense_52_loss: 0.2134 - val_dense_53_loss: 29.5409 - val_dense_54_loss: 0.0143\n",
            "\n",
            "Epoch 00025: val_loss improved from 96.95687 to 96.41646, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 26/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 96.8886 - dense_52_loss: 0.2109 - dense_53_loss: 29.9477 - dense_54_loss: 0.0184 - val_loss: 98.6608 - val_dense_52_loss: 0.2169 - val_dense_53_loss: 29.7532 - val_dense_54_loss: 0.0193\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 96.41646\n",
            "Epoch 27/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 95.5846 - dense_52_loss: 0.2092 - dense_53_loss: 29.3944 - dense_54_loss: 0.0171 - val_loss: 99.7137 - val_dense_52_loss: 0.2251 - val_dense_53_loss: 29.3178 - val_dense_54_loss: 0.0143\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 96.41646\n",
            "Epoch 28/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 95.8328 - dense_52_loss: 0.2094 - dense_53_loss: 29.4983 - dense_54_loss: 0.0176 - val_loss: 96.3011 - val_dense_52_loss: 0.2163 - val_dense_53_loss: 28.6539 - val_dense_54_loss: 0.0137\n",
            "\n",
            "Epoch 00028: val_loss improved from 96.41646 to 96.30114, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 29/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 95.5684 - dense_52_loss: 0.2090 - dense_53_loss: 29.4131 - dense_54_loss: 0.0172 - val_loss: 94.5194 - val_dense_52_loss: 0.2113 - val_dense_53_loss: 28.4732 - val_dense_54_loss: 0.0133\n",
            "\n",
            "Epoch 00029: val_loss improved from 96.30114 to 94.51943, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 30/100\n",
            "42750/42750 [==============================] - 6s 152us/step - loss: 95.1078 - dense_52_loss: 0.2080 - dense_53_loss: 29.2986 - dense_54_loss: 0.0171 - val_loss: 93.8413 - val_dense_52_loss: 0.2081 - val_dense_53_loss: 28.6453 - val_dense_54_loss: 0.0139\n",
            "\n",
            "Epoch 00030: val_loss improved from 94.51943 to 93.84129, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 31/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 95.3994 - dense_52_loss: 0.2090 - dense_53_loss: 29.3289 - dense_54_loss: 0.0169 - val_loss: 93.6377 - val_dense_52_loss: 0.2080 - val_dense_53_loss: 28.1622 - val_dense_54_loss: 0.0154\n",
            "\n",
            "Epoch 00031: val_loss improved from 93.84129 to 93.63774, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 32/100\n",
            "42750/42750 [==============================] - 7s 152us/step - loss: 94.9069 - dense_52_loss: 0.2077 - dense_53_loss: 29.1635 - dense_54_loss: 0.0171 - val_loss: 97.6022 - val_dense_52_loss: 0.2118 - val_dense_53_loss: 29.8433 - val_dense_54_loss: 0.0211\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 93.63774\n",
            "Epoch 33/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 95.0361 - dense_52_loss: 0.2081 - dense_53_loss: 29.1630 - dense_54_loss: 0.0173 - val_loss: 94.8904 - val_dense_52_loss: 0.2097 - val_dense_53_loss: 29.0086 - val_dense_54_loss: 0.0149\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 93.63774\n",
            "Epoch 34/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 93.9214 - dense_52_loss: 0.2062 - dense_53_loss: 28.8326 - dense_54_loss: 0.0161 - val_loss: 96.6912 - val_dense_52_loss: 0.2191 - val_dense_53_loss: 28.3384 - val_dense_54_loss: 0.0131\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 93.63774\n",
            "Epoch 35/100\n",
            "42750/42750 [==============================] - 6s 152us/step - loss: 93.7921 - dense_52_loss: 0.2061 - dense_53_loss: 28.7716 - dense_54_loss: 0.0159 - val_loss: 93.7133 - val_dense_52_loss: 0.2083 - val_dense_53_loss: 28.1537 - val_dense_54_loss: 0.0153\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 93.63774\n",
            "Epoch 36/100\n",
            "42750/42750 [==============================] - 6s 152us/step - loss: 93.9580 - dense_52_loss: 0.2062 - dense_53_loss: 28.8456 - dense_54_loss: 0.0162 - val_loss: 96.5116 - val_dense_52_loss: 0.2094 - val_dense_53_loss: 28.9813 - val_dense_54_loss: 0.0236\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 93.63774\n",
            "Epoch 37/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 94.6198 - dense_52_loss: 0.2068 - dense_53_loss: 29.1720 - dense_54_loss: 0.0170 - val_loss: 97.3012 - val_dense_52_loss: 0.2154 - val_dense_53_loss: 28.0989 - val_dense_54_loss: 0.0230\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 93.63774\n",
            "Epoch 38/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 94.5117 - dense_52_loss: 0.2063 - dense_53_loss: 29.2002 - dense_54_loss: 0.0170 - val_loss: 93.3586 - val_dense_52_loss: 0.2072 - val_dense_53_loss: 28.5016 - val_dense_54_loss: 0.0134\n",
            "\n",
            "Epoch 00038: val_loss improved from 93.63774 to 93.35861, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 39/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 93.9142 - dense_52_loss: 0.2056 - dense_53_loss: 28.9186 - dense_54_loss: 0.0165 - val_loss: 96.0984 - val_dense_52_loss: 0.2087 - val_dense_53_loss: 29.8620 - val_dense_54_loss: 0.0182\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 93.35861\n",
            "Epoch 40/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 93.2961 - dense_52_loss: 0.2049 - dense_53_loss: 28.6306 - dense_54_loss: 0.0160 - val_loss: 93.0815 - val_dense_52_loss: 0.2066 - val_dense_53_loss: 28.4028 - val_dense_54_loss: 0.0136\n",
            "\n",
            "Epoch 00040: val_loss improved from 93.35861 to 93.08150, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 41/100\n",
            "42750/42750 [==============================] - 7s 152us/step - loss: 93.1279 - dense_52_loss: 0.2045 - dense_53_loss: 28.5983 - dense_54_loss: 0.0159 - val_loss: 94.2511 - val_dense_52_loss: 0.2088 - val_dense_53_loss: 27.9089 - val_dense_54_loss: 0.0186\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 93.08150\n",
            "Epoch 42/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 93.3293 - dense_52_loss: 0.2049 - dense_53_loss: 28.6258 - dense_54_loss: 0.0161 - val_loss: 93.3879 - val_dense_52_loss: 0.2075 - val_dense_53_loss: 28.4932 - val_dense_54_loss: 0.0132\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 93.08150\n",
            "Epoch 43/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 93.6012 - dense_52_loss: 0.2047 - dense_53_loss: 28.9085 - dense_54_loss: 0.0164 - val_loss: 94.1703 - val_dense_52_loss: 0.2067 - val_dense_53_loss: 27.9666 - val_dense_54_loss: 0.0210\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 93.08150\n",
            "Epoch 44/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 93.4790 - dense_52_loss: 0.2045 - dense_53_loss: 28.8695 - dense_54_loss: 0.0163 - val_loss: 93.5829 - val_dense_52_loss: 0.2102 - val_dense_53_loss: 27.6031 - val_dense_54_loss: 0.0147\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 93.08150\n",
            "Epoch 45/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 93.1861 - dense_52_loss: 0.2042 - dense_53_loss: 28.7306 - dense_54_loss: 0.0160 - val_loss: 102.2984 - val_dense_52_loss: 0.2232 - val_dense_53_loss: 32.4149 - val_dense_54_loss: 0.0147\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 93.08150\n",
            "Epoch 46/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 92.6689 - dense_52_loss: 0.2030 - dense_53_loss: 28.6202 - dense_54_loss: 0.0157 - val_loss: 92.9801 - val_dense_52_loss: 0.2074 - val_dense_53_loss: 27.9265 - val_dense_54_loss: 0.0142\n",
            "\n",
            "Epoch 00046: val_loss improved from 93.08150 to 92.98007, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 47/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 93.1965 - dense_52_loss: 0.2040 - dense_53_loss: 28.7217 - dense_54_loss: 0.0164 - val_loss: 91.6892 - val_dense_52_loss: 0.2050 - val_dense_53_loss: 27.8064 - val_dense_54_loss: 0.0119\n",
            "\n",
            "Epoch 00047: val_loss improved from 92.98007 to 91.68916, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 48/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 92.2985 - dense_52_loss: 0.2028 - dense_53_loss: 28.3918 - dense_54_loss: 0.0154 - val_loss: 95.2701 - val_dense_52_loss: 0.2137 - val_dense_53_loss: 28.4312 - val_dense_54_loss: 0.0136\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 91.68916\n",
            "Epoch 49/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 92.8443 - dense_52_loss: 0.2035 - dense_53_loss: 28.5774 - dense_54_loss: 0.0161 - val_loss: 104.4234 - val_dense_52_loss: 0.2263 - val_dense_53_loss: 30.6066 - val_dense_54_loss: 0.0296\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 91.68916\n",
            "Epoch 50/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 92.4679 - dense_52_loss: 0.2028 - dense_53_loss: 28.4908 - dense_54_loss: 0.0157 - val_loss: 90.4644 - val_dense_52_loss: 0.2015 - val_dense_53_loss: 27.7541 - val_dense_54_loss: 0.0114\n",
            "\n",
            "Epoch 00050: val_loss improved from 91.68916 to 90.46436, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 51/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 92.5386 - dense_52_loss: 0.2025 - dense_53_loss: 28.5703 - dense_54_loss: 0.0162 - val_loss: 91.4769 - val_dense_52_loss: 0.2036 - val_dense_53_loss: 27.8966 - val_dense_54_loss: 0.0125\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 90.46436\n",
            "Epoch 52/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 92.2470 - dense_52_loss: 0.2025 - dense_53_loss: 28.4038 - dense_54_loss: 0.0155 - val_loss: 92.3944 - val_dense_52_loss: 0.2037 - val_dense_53_loss: 28.8534 - val_dense_54_loss: 0.0122\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 90.46436\n",
            "Epoch 53/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 91.9402 - dense_52_loss: 0.2018 - dense_53_loss: 28.3425 - dense_54_loss: 0.0153 - val_loss: 92.2276 - val_dense_52_loss: 0.2037 - val_dense_53_loss: 28.3523 - val_dense_54_loss: 0.0139\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 90.46436\n",
            "Epoch 54/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 91.5702 - dense_52_loss: 0.2011 - dense_53_loss: 28.2265 - dense_54_loss: 0.0151 - val_loss: 92.3822 - val_dense_52_loss: 0.2029 - val_dense_53_loss: 27.9387 - val_dense_54_loss: 0.0179\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 90.46436\n",
            "Epoch 55/100\n",
            "42750/42750 [==============================] - 6s 149us/step - loss: 91.9995 - dense_52_loss: 0.2017 - dense_53_loss: 28.3746 - dense_54_loss: 0.0155 - val_loss: 91.5209 - val_dense_52_loss: 0.2042 - val_dense_53_loss: 27.8299 - val_dense_54_loss: 0.0121\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 90.46436\n",
            "Epoch 56/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 92.3375 - dense_52_loss: 0.2020 - dense_53_loss: 28.5815 - dense_54_loss: 0.0157 - val_loss: 93.7962 - val_dense_52_loss: 0.2053 - val_dense_53_loss: 29.2004 - val_dense_54_loss: 0.0150\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 90.46436\n",
            "Epoch 57/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 92.1758 - dense_52_loss: 0.2020 - dense_53_loss: 28.4447 - dense_54_loss: 0.0156 - val_loss: 98.0035 - val_dense_52_loss: 0.2145 - val_dense_53_loss: 30.0741 - val_dense_54_loss: 0.0179\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 90.46436\n",
            "Epoch 58/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 91.9879 - dense_52_loss: 0.2014 - dense_53_loss: 28.4379 - dense_54_loss: 0.0156 - val_loss: 92.3107 - val_dense_52_loss: 0.2045 - val_dense_53_loss: 28.3458 - val_dense_54_loss: 0.0130\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 90.46436\n",
            "Epoch 59/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 91.0156 - dense_52_loss: 0.2001 - dense_53_loss: 28.0468 - dense_54_loss: 0.0146 - val_loss: 90.0232 - val_dense_52_loss: 0.2006 - val_dense_53_loss: 27.3521 - val_dense_54_loss: 0.0124\n",
            "\n",
            "Epoch 00059: val_loss improved from 90.46436 to 90.02320, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 60/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 91.0461 - dense_52_loss: 0.1999 - dense_53_loss: 28.0516 - dense_54_loss: 0.0151 - val_loss: 90.7148 - val_dense_52_loss: 0.2003 - val_dense_53_loss: 27.8533 - val_dense_54_loss: 0.0139\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 90.02320\n",
            "Epoch 61/100\n",
            "42750/42750 [==============================] - 6s 152us/step - loss: 91.8669 - dense_52_loss: 0.2009 - dense_53_loss: 28.4818 - dense_54_loss: 0.0156 - val_loss: 89.9770 - val_dense_52_loss: 0.2010 - val_dense_53_loss: 27.4788 - val_dense_54_loss: 0.0111\n",
            "\n",
            "Epoch 00061: val_loss improved from 90.02320 to 89.97703, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 62/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 90.7944 - dense_52_loss: 0.1995 - dense_53_loss: 28.0499 - dense_54_loss: 0.0144 - val_loss: 95.6130 - val_dense_52_loss: 0.2070 - val_dense_53_loss: 28.4298 - val_dense_54_loss: 0.0254\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 89.97703\n",
            "Epoch 63/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 91.1252 - dense_52_loss: 0.2001 - dense_53_loss: 28.0615 - dense_54_loss: 0.0151 - val_loss: 108.0820 - val_dense_52_loss: 0.2207 - val_dense_53_loss: 37.5064 - val_dense_54_loss: 0.0219\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 89.97703\n",
            "Epoch 64/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 91.0415 - dense_52_loss: 0.1996 - dense_53_loss: 28.1339 - dense_54_loss: 0.0151 - val_loss: 90.7623 - val_dense_52_loss: 0.2018 - val_dense_53_loss: 27.9034 - val_dense_54_loss: 0.0116\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 89.97703\n",
            "Epoch 65/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 91.6700 - dense_52_loss: 0.2011 - dense_53_loss: 28.2373 - dense_54_loss: 0.0155 - val_loss: 111.1216 - val_dense_52_loss: 0.2386 - val_dense_53_loss: 34.4275 - val_dense_54_loss: 0.0255\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 89.97703\n",
            "Epoch 66/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 90.9289 - dense_52_loss: 0.1995 - dense_53_loss: 28.0945 - dense_54_loss: 0.0149 - val_loss: 92.4838 - val_dense_52_loss: 0.2049 - val_dense_53_loss: 27.3169 - val_dense_54_loss: 0.0185\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 89.97703\n",
            "Epoch 67/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 90.6695 - dense_52_loss: 0.1992 - dense_53_loss: 27.9664 - dense_54_loss: 0.0148 - val_loss: 97.2338 - val_dense_52_loss: 0.2112 - val_dense_53_loss: 29.7585 - val_dense_54_loss: 0.0206\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 89.97703\n",
            "Epoch 68/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 90.4186 - dense_52_loss: 0.1984 - dense_53_loss: 27.9342 - dense_54_loss: 0.0148 - val_loss: 89.7351 - val_dense_52_loss: 0.2018 - val_dense_53_loss: 26.9996 - val_dense_54_loss: 0.0110\n",
            "\n",
            "Epoch 00068: val_loss improved from 89.97703 to 89.73506, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 69/100\n",
            "42750/42750 [==============================] - 7s 159us/step - loss: 89.8516 - dense_52_loss: 0.1976 - dense_53_loss: 27.7424 - dense_54_loss: 0.0142 - val_loss: 89.0168 - val_dense_52_loss: 0.1993 - val_dense_53_loss: 27.1148 - val_dense_54_loss: 0.0105\n",
            "\n",
            "Epoch 00069: val_loss improved from 89.73506 to 89.01684, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 70/100\n",
            "42750/42750 [==============================] - 7s 155us/step - loss: 90.7528 - dense_52_loss: 0.1990 - dense_53_loss: 28.1360 - dense_54_loss: 0.0147 - val_loss: 88.9716 - val_dense_52_loss: 0.1997 - val_dense_53_loss: 27.0010 - val_dense_54_loss: 0.0103\n",
            "\n",
            "Epoch 00070: val_loss improved from 89.01684 to 88.97160, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 71/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 89.8328 - dense_52_loss: 0.1970 - dense_53_loss: 27.8237 - dense_54_loss: 0.0146 - val_loss: 93.9224 - val_dense_52_loss: 0.2090 - val_dense_53_loss: 28.7031 - val_dense_54_loss: 0.0125\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 88.97160\n",
            "Epoch 72/100\n",
            "42750/42750 [==============================] - 7s 153us/step - loss: 89.6592 - dense_52_loss: 0.1972 - dense_53_loss: 27.7053 - dense_54_loss: 0.0140 - val_loss: 91.4531 - val_dense_52_loss: 0.2002 - val_dense_53_loss: 27.7135 - val_dense_54_loss: 0.0184\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 88.97160\n",
            "Epoch 73/100\n",
            "42750/42750 [==============================] - 7s 152us/step - loss: 90.4770 - dense_52_loss: 0.1984 - dense_53_loss: 27.9804 - dense_54_loss: 0.0148 - val_loss: 92.3338 - val_dense_52_loss: 0.2079 - val_dense_53_loss: 27.4443 - val_dense_54_loss: 0.0126\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 88.97160\n",
            "Epoch 74/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 90.0207 - dense_52_loss: 0.1976 - dense_53_loss: 27.8263 - dense_54_loss: 0.0146 - val_loss: 90.2463 - val_dense_52_loss: 0.2035 - val_dense_53_loss: 26.9829 - val_dense_54_loss: 0.0110\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 88.97160\n",
            "Epoch 75/100\n",
            "42750/42750 [==============================] - 6s 152us/step - loss: 89.9826 - dense_52_loss: 0.1975 - dense_53_loss: 27.8222 - dense_54_loss: 0.0145 - val_loss: 91.8890 - val_dense_52_loss: 0.2000 - val_dense_53_loss: 27.5814 - val_dense_54_loss: 0.0216\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 88.97160\n",
            "Epoch 76/100\n",
            "42750/42750 [==============================] - 6s 150us/step - loss: 90.3254 - dense_52_loss: 0.1982 - dense_53_loss: 27.9404 - dense_54_loss: 0.0147 - val_loss: 91.6726 - val_dense_52_loss: 0.2061 - val_dense_53_loss: 27.3038 - val_dense_54_loss: 0.0127\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 88.97160\n",
            "Epoch 77/100\n",
            "42750/42750 [==============================] - 6s 149us/step - loss: 90.3221 - dense_52_loss: 0.1980 - dense_53_loss: 27.9727 - dense_54_loss: 0.0148 - val_loss: 99.7535 - val_dense_52_loss: 0.2174 - val_dense_53_loss: 29.2857 - val_dense_54_loss: 0.0262\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 88.97160\n",
            "Epoch 78/100\n",
            "42750/42750 [==============================] - 6s 151us/step - loss: 90.4968 - dense_52_loss: 0.1981 - dense_53_loss: 28.0734 - dense_54_loss: 0.0150 - val_loss: 100.6481 - val_dense_52_loss: 0.2077 - val_dense_53_loss: 33.7563 - val_dense_54_loss: 0.0229\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 88.97160\n",
            "Epoch 79/100\n",
            "42750/42750 [==============================] - 6s 149us/step - loss: 89.7422 - dense_52_loss: 0.1968 - dense_53_loss: 27.7974 - dense_54_loss: 0.0145 - val_loss: 91.3181 - val_dense_52_loss: 0.2033 - val_dense_53_loss: 27.8432 - val_dense_54_loss: 0.0124\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 88.97160\n",
            "Epoch 80/100\n",
            "42750/42750 [==============================] - 6s 149us/step - loss: 90.1877 - dense_52_loss: 0.1978 - dense_53_loss: 27.9374 - dense_54_loss: 0.0146 - val_loss: 90.2749 - val_dense_52_loss: 0.2022 - val_dense_53_loss: 27.4125 - val_dense_54_loss: 0.0111\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 88.97160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNNvOyXnMn0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6871
        },
        "outputId": "3171164f-3278-4154-f9ca-832a28cdb190"
      },
      "source": [
        "if os.path.exists(MODELPATH):\n",
        "    model.load_weights(MODELPATH)\n",
        "    # 若成功加载前面保存的参数，输出下列信息\n",
        "    print(\"checkpoint_loaded\")\n",
        "    \n",
        "# checkpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss',patience=30)\n",
        "MODELPATH=dirPath+\"MLP_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(MODELPATH, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# print(y_train.shape)\n",
        "# y_train_T = np.transpose(y_train)\n",
        "# print(y_train_T.shape)\n",
        "# y_val_T = np.transpose(y_val)\n",
        "# print(y_val_T.shape)\n",
        "history = model.fit(x=x_train,y=[y_train_T[0],y_train_T[1],y_train_T[2]],batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
        "        validation_data=(x_val,[y_val_T[0],y_val_T[1],y_val_T[2]]),shuffle=True,callbacks=[early_stopping,checkpoint])\n",
        "file_his = open(dirPath+'MLP_history.pickle', 'wb')\n",
        "pickle.dump(history.history, file_his)\n",
        "file_his.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint_loaded\n",
            "Train on 42750 samples, validate on 4750 samples\n",
            "Epoch 1/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 90.1463 - dense_20_loss: 0.1984 - dense_21_loss: 27.8983 - dense_22_loss: 0.0136 - val_loss: 91.5557 - val_dense_20_loss: 0.2048 - val_dense_21_loss: 27.8636 - val_dense_22_loss: 0.0112\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 91.55566, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 2/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 90.9698 - dense_20_loss: 0.1992 - dense_21_loss: 28.1545 - dense_22_loss: 0.0153 - val_loss: 90.0882 - val_dense_20_loss: 0.2015 - val_dense_21_loss: 27.4224 - val_dense_22_loss: 0.0111\n",
            "\n",
            "Epoch 00002: val_loss improved from 91.55566 to 90.08820, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 3/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 91.0576 - dense_20_loss: 0.1997 - dense_21_loss: 28.2387 - dense_22_loss: 0.0146 - val_loss: 92.9224 - val_dense_20_loss: 0.2060 - val_dense_21_loss: 27.5015 - val_dense_22_loss: 0.0181\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 90.08820\n",
            "Epoch 4/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 90.1398 - dense_20_loss: 0.1986 - dense_21_loss: 27.8546 - dense_22_loss: 0.0135 - val_loss: 100.4957 - val_dense_20_loss: 0.2103 - val_dense_21_loss: 31.2922 - val_dense_22_loss: 0.0305\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 90.08820\n",
            "Epoch 5/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 90.4634 - dense_20_loss: 0.1986 - dense_21_loss: 27.9521 - dense_22_loss: 0.0146 - val_loss: 90.7406 - val_dense_20_loss: 0.2021 - val_dense_21_loss: 27.4878 - val_dense_22_loss: 0.0131\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 90.08820\n",
            "Epoch 6/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.7462 - dense_20_loss: 0.1975 - dense_21_loss: 27.8012 - dense_22_loss: 0.0135 - val_loss: 90.9009 - val_dense_20_loss: 0.2023 - val_dense_21_loss: 27.8383 - val_dense_22_loss: 0.0119\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 90.08820\n",
            "Epoch 7/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 91.7632 - dense_20_loss: 0.2013 - dense_21_loss: 28.3169 - dense_22_loss: 0.0153 - val_loss: 94.2881 - val_dense_20_loss: 0.2058 - val_dense_21_loss: 30.1510 - val_dense_22_loss: 0.0119\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 90.08820\n",
            "Epoch 8/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 89.7130 - dense_20_loss: 0.1973 - dense_21_loss: 27.7401 - dense_22_loss: 0.0140 - val_loss: 91.1588 - val_dense_20_loss: 0.2024 - val_dense_21_loss: 27.8590 - val_dense_22_loss: 0.0129\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 90.08820\n",
            "Epoch 9/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 89.8503 - dense_20_loss: 0.1977 - dense_21_loss: 27.7725 - dense_22_loss: 0.0138 - val_loss: 91.7506 - val_dense_20_loss: 0.2014 - val_dense_21_loss: 29.0383 - val_dense_22_loss: 0.0115\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 90.08820\n",
            "Epoch 10/100\n",
            "42750/42750 [==============================] - 2s 44us/step - loss: 92.2095 - dense_20_loss: 0.2015 - dense_21_loss: 28.6400 - dense_22_loss: 0.0157 - val_loss: 94.3766 - val_dense_20_loss: 0.2092 - val_dense_21_loss: 28.8389 - val_dense_22_loss: 0.0139\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 90.08820\n",
            "Epoch 11/100\n",
            "42750/42750 [==============================] - 2s 44us/step - loss: 89.4785 - dense_20_loss: 0.1965 - dense_21_loss: 27.6153 - dense_22_loss: 0.0145 - val_loss: 89.5524 - val_dense_20_loss: 0.1997 - val_dense_21_loss: 27.3927 - val_dense_22_loss: 0.0113\n",
            "\n",
            "Epoch 00011: val_loss improved from 90.08820 to 89.55237, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 12/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 90.0263 - dense_20_loss: 0.1971 - dense_21_loss: 27.8627 - dense_22_loss: 0.0151 - val_loss: 96.6517 - val_dense_20_loss: 0.2150 - val_dense_21_loss: 29.4143 - val_dense_22_loss: 0.0137\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 89.55237\n",
            "Epoch 13/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 92.6517 - dense_20_loss: 0.2020 - dense_21_loss: 28.7523 - dense_22_loss: 0.0165 - val_loss: 90.2276 - val_dense_20_loss: 0.2007 - val_dense_21_loss: 27.5664 - val_dense_22_loss: 0.0123\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 89.55237\n",
            "Epoch 14/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 90.9222 - dense_20_loss: 0.1994 - dense_21_loss: 28.2514 - dense_22_loss: 0.0143 - val_loss: 90.6370 - val_dense_20_loss: 0.2010 - val_dense_21_loss: 27.4884 - val_dense_22_loss: 0.0142\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 89.55237\n",
            "Epoch 15/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 88.8328 - dense_20_loss: 0.1960 - dense_21_loss: 27.4567 - dense_22_loss: 0.0129 - val_loss: 92.2226 - val_dense_20_loss: 0.2027 - val_dense_21_loss: 29.2047 - val_dense_22_loss: 0.0110\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 89.55237\n",
            "Epoch 16/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 89.7396 - dense_20_loss: 0.1974 - dense_21_loss: 27.8060 - dense_22_loss: 0.0135 - val_loss: 90.4644 - val_dense_20_loss: 0.1991 - val_dense_21_loss: 28.2181 - val_dense_22_loss: 0.0125\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 89.55237\n",
            "Epoch 17/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 90.4015 - dense_20_loss: 0.1985 - dense_21_loss: 28.0260 - dense_22_loss: 0.0141 - val_loss: 89.6575 - val_dense_20_loss: 0.2006 - val_dense_21_loss: 27.2702 - val_dense_22_loss: 0.0110\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 89.55237\n",
            "Epoch 18/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 91.8705 - dense_20_loss: 0.2010 - dense_21_loss: 28.4785 - dense_22_loss: 0.0154 - val_loss: 91.7569 - val_dense_20_loss: 0.2042 - val_dense_21_loss: 27.5925 - val_dense_22_loss: 0.0145\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 89.55237\n",
            "Epoch 19/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 90.5475 - dense_20_loss: 0.1980 - dense_21_loss: 28.1724 - dense_22_loss: 0.0149 - val_loss: 90.3697 - val_dense_20_loss: 0.1998 - val_dense_21_loss: 28.1947 - val_dense_22_loss: 0.0111\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 89.55237\n",
            "Epoch 20/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 90.3094 - dense_20_loss: 0.1981 - dense_21_loss: 28.0263 - dense_22_loss: 0.0142 - val_loss: 94.5764 - val_dense_20_loss: 0.2091 - val_dense_21_loss: 28.6325 - val_dense_22_loss: 0.0161\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 89.55237\n",
            "Epoch 21/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.9628 - dense_20_loss: 0.1973 - dense_21_loss: 28.0009 - dense_22_loss: 0.0139 - val_loss: 96.7977 - val_dense_20_loss: 0.2121 - val_dense_21_loss: 29.5602 - val_dense_22_loss: 0.0180\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 89.55237\n",
            "Epoch 22/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 91.1493 - dense_20_loss: 0.1998 - dense_21_loss: 28.2294 - dense_22_loss: 0.0150 - val_loss: 95.6690 - val_dense_20_loss: 0.2172 - val_dense_21_loss: 27.9179 - val_dense_22_loss: 0.0130\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 89.55237\n",
            "Epoch 23/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.6296 - dense_20_loss: 0.1972 - dense_21_loss: 27.7253 - dense_22_loss: 0.0137 - val_loss: 90.5430 - val_dense_20_loss: 0.2001 - val_dense_21_loss: 28.0440 - val_dense_22_loss: 0.0123\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 89.55237\n",
            "Epoch 24/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 91.6030 - dense_20_loss: 0.2001 - dense_21_loss: 28.4095 - dense_22_loss: 0.0158 - val_loss: 91.4475 - val_dense_20_loss: 0.2009 - val_dense_21_loss: 28.0130 - val_dense_22_loss: 0.0158\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 89.55237\n",
            "Epoch 25/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.9233 - dense_20_loss: 0.1972 - dense_21_loss: 27.8986 - dense_22_loss: 0.0143 - val_loss: 90.4562 - val_dense_20_loss: 0.1992 - val_dense_21_loss: 27.2224 - val_dense_22_loss: 0.0174\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 89.55237\n",
            "Epoch 26/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 89.1371 - dense_20_loss: 0.1961 - dense_21_loss: 27.6698 - dense_22_loss: 0.0132 - val_loss: 99.3356 - val_dense_20_loss: 0.2217 - val_dense_21_loss: 29.3608 - val_dense_22_loss: 0.0173\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 89.55237\n",
            "Epoch 27/100\n",
            "42750/42750 [==============================] - 2s 48us/step - loss: 90.7470 - dense_20_loss: 0.1987 - dense_21_loss: 28.2219 - dense_22_loss: 0.0146 - val_loss: 90.6014 - val_dense_20_loss: 0.2035 - val_dense_21_loss: 27.4363 - val_dense_22_loss: 0.0106\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 89.55237\n",
            "Epoch 28/100\n",
            "42750/42750 [==============================] - 2s 48us/step - loss: 89.8199 - dense_20_loss: 0.1972 - dense_21_loss: 27.8942 - dense_22_loss: 0.0139 - val_loss: 91.7679 - val_dense_20_loss: 0.2010 - val_dense_21_loss: 28.3542 - val_dense_22_loss: 0.0156\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 89.55237\n",
            "Epoch 29/100\n",
            "42750/42750 [==============================] - 2s 48us/step - loss: 91.9201 - dense_20_loss: 0.1996 - dense_21_loss: 28.7426 - dense_22_loss: 0.0165 - val_loss: 97.9376 - val_dense_20_loss: 0.2150 - val_dense_21_loss: 30.0868 - val_dense_22_loss: 0.0167\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 89.55237\n",
            "Epoch 30/100\n",
            "42750/42750 [==============================] - 2s 48us/step - loss: 88.7922 - dense_20_loss: 0.1954 - dense_21_loss: 27.5878 - dense_22_loss: 0.0129 - val_loss: 101.5072 - val_dense_20_loss: 0.2238 - val_dense_21_loss: 30.6229 - val_dense_22_loss: 0.0187\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 89.55237\n",
            "Epoch 31/100\n",
            "42750/42750 [==============================] - 2s 49us/step - loss: 89.8884 - dense_20_loss: 0.1968 - dense_21_loss: 27.8487 - dense_22_loss: 0.0150 - val_loss: 90.9910 - val_dense_20_loss: 0.2040 - val_dense_21_loss: 27.4287 - val_dense_22_loss: 0.0118\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 89.55237\n",
            "Epoch 32/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.6518 - dense_20_loss: 0.1965 - dense_21_loss: 27.8868 - dense_22_loss: 0.0140 - val_loss: 91.7880 - val_dense_20_loss: 0.2014 - val_dense_21_loss: 27.4233 - val_dense_22_loss: 0.0197\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 89.55237\n",
            "Epoch 33/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 89.6385 - dense_20_loss: 0.1970 - dense_21_loss: 27.7074 - dense_22_loss: 0.0142 - val_loss: 102.5449 - val_dense_20_loss: 0.2188 - val_dense_21_loss: 33.4506 - val_dense_22_loss: 0.0173\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 89.55237\n",
            "Epoch 34/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.1792 - dense_20_loss: 0.1960 - dense_21_loss: 27.6940 - dense_22_loss: 0.0134 - val_loss: 97.5912 - val_dense_20_loss: 0.2172 - val_dense_21_loss: 29.9256 - val_dense_22_loss: 0.0126\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 89.55237\n",
            "Epoch 35/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.1197 - dense_20_loss: 0.1954 - dense_21_loss: 27.7189 - dense_22_loss: 0.0138 - val_loss: 89.0879 - val_dense_20_loss: 0.1980 - val_dense_21_loss: 27.5052 - val_dense_22_loss: 0.0109\n",
            "\n",
            "Epoch 00035: val_loss improved from 89.55237 to 89.08792, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 36/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.9900 - dense_20_loss: 0.1951 - dense_21_loss: 27.6559 - dense_22_loss: 0.0140 - val_loss: 94.3649 - val_dense_20_loss: 0.2077 - val_dense_21_loss: 28.8362 - val_dense_22_loss: 0.0162\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 89.08792\n",
            "Epoch 37/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.0624 - dense_20_loss: 0.1956 - dense_21_loss: 27.6234 - dense_22_loss: 0.0138 - val_loss: 97.4076 - val_dense_20_loss: 0.2129 - val_dense_21_loss: 29.2828 - val_dense_22_loss: 0.0213\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 89.08792\n",
            "Epoch 38/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 89.3078 - dense_20_loss: 0.1965 - dense_21_loss: 27.5742 - dense_22_loss: 0.0139 - val_loss: 89.5157 - val_dense_20_loss: 0.1991 - val_dense_21_loss: 27.3065 - val_dense_22_loss: 0.0124\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 89.08792\n",
            "Epoch 39/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 89.3285 - dense_20_loss: 0.1961 - dense_21_loss: 27.7974 - dense_22_loss: 0.0135 - val_loss: 96.5995 - val_dense_20_loss: 0.2060 - val_dense_21_loss: 30.5994 - val_dense_22_loss: 0.0210\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 89.08792\n",
            "Epoch 40/100\n",
            "42750/42750 [==============================] - 2s 45us/step - loss: 90.3427 - dense_20_loss: 0.1973 - dense_21_loss: 28.2402 - dense_22_loss: 0.0146 - val_loss: 94.1958 - val_dense_20_loss: 0.2126 - val_dense_21_loss: 27.9366 - val_dense_22_loss: 0.0124\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 89.08792\n",
            "Epoch 41/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.5021 - dense_20_loss: 0.1945 - dense_21_loss: 27.5150 - dense_22_loss: 0.0131 - val_loss: 93.3077 - val_dense_20_loss: 0.2059 - val_dense_21_loss: 28.1012 - val_dense_22_loss: 0.0172\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 89.08792\n",
            "Epoch 42/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.6493 - dense_20_loss: 0.1969 - dense_21_loss: 27.6967 - dense_22_loss: 0.0145 - val_loss: 91.6792 - val_dense_20_loss: 0.2049 - val_dense_21_loss: 27.8288 - val_dense_22_loss: 0.0119\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 89.08792\n",
            "Epoch 43/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.3182 - dense_20_loss: 0.1961 - dense_21_loss: 27.7485 - dense_22_loss: 0.0138 - val_loss: 91.8052 - val_dense_20_loss: 0.2045 - val_dense_21_loss: 27.3141 - val_dense_22_loss: 0.0158\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 89.08792\n",
            "Epoch 44/100\n",
            "42750/42750 [==============================] - 2s 48us/step - loss: 88.2542 - dense_20_loss: 0.1944 - dense_21_loss: 27.4210 - dense_22_loss: 0.0126 - val_loss: 89.5696 - val_dense_20_loss: 0.1993 - val_dense_21_loss: 27.2870 - val_dense_22_loss: 0.0125\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 89.08792\n",
            "Epoch 45/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 88.8171 - dense_20_loss: 0.1952 - dense_21_loss: 27.5729 - dense_22_loss: 0.0134 - val_loss: 90.6850 - val_dense_20_loss: 0.1990 - val_dense_21_loss: 27.3726 - val_dense_22_loss: 0.0181\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 89.08792\n",
            "Epoch 46/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 90.6283 - dense_20_loss: 0.1975 - dense_21_loss: 28.1646 - dense_22_loss: 0.0161 - val_loss: 91.4233 - val_dense_20_loss: 0.2003 - val_dense_21_loss: 29.0536 - val_dense_22_loss: 0.0115\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 89.08792\n",
            "Epoch 47/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.8033 - dense_20_loss: 0.1933 - dense_21_loss: 27.1913 - dense_22_loss: 0.0131 - val_loss: 88.6679 - val_dense_20_loss: 0.1972 - val_dense_21_loss: 27.0389 - val_dense_22_loss: 0.0124\n",
            "\n",
            "Epoch 00047: val_loss improved from 89.08792 to 88.66788, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 48/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.0133 - dense_20_loss: 0.1954 - dense_21_loss: 27.6439 - dense_22_loss: 0.0137 - val_loss: 92.6012 - val_dense_20_loss: 0.2004 - val_dense_21_loss: 29.3263 - val_dense_22_loss: 0.0158\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 88.66788\n",
            "Epoch 49/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.6667 - dense_20_loss: 0.1944 - dense_21_loss: 27.6342 - dense_22_loss: 0.0136 - val_loss: 90.9937 - val_dense_20_loss: 0.1999 - val_dense_21_loss: 28.0342 - val_dense_22_loss: 0.0150\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 88.66788\n",
            "Epoch 50/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.5836 - dense_20_loss: 0.1927 - dense_21_loss: 27.2823 - dense_22_loss: 0.0124 - val_loss: 90.0662 - val_dense_20_loss: 0.2006 - val_dense_21_loss: 27.0611 - val_dense_22_loss: 0.0141\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 88.66788\n",
            "Epoch 51/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.7539 - dense_20_loss: 0.1946 - dense_21_loss: 27.6670 - dense_22_loss: 0.0135 - val_loss: 94.0237 - val_dense_20_loss: 0.2071 - val_dense_21_loss: 28.3161 - val_dense_22_loss: 0.0180\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 88.66788\n",
            "Epoch 52/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.8664 - dense_20_loss: 0.1937 - dense_21_loss: 27.2700 - dense_22_loss: 0.0124 - val_loss: 90.0793 - val_dense_20_loss: 0.1989 - val_dense_21_loss: 27.9217 - val_dense_22_loss: 0.0125\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 88.66788\n",
            "Epoch 53/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.8316 - dense_20_loss: 0.1951 - dense_21_loss: 27.5579 - dense_22_loss: 0.0137 - val_loss: 88.3734 - val_dense_20_loss: 0.1976 - val_dense_21_loss: 27.0227 - val_dense_22_loss: 0.0104\n",
            "\n",
            "Epoch 00053: val_loss improved from 88.66788 to 88.37345, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 54/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 87.9019 - dense_20_loss: 0.1936 - dense_21_loss: 27.2643 - dense_22_loss: 0.0128 - val_loss: 94.6429 - val_dense_20_loss: 0.1999 - val_dense_21_loss: 31.7377 - val_dense_22_loss: 0.0147\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 88.37345\n",
            "Epoch 55/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.1371 - dense_20_loss: 0.1951 - dense_21_loss: 27.7770 - dense_22_loss: 0.0141 - val_loss: 91.8810 - val_dense_20_loss: 0.1974 - val_dense_21_loss: 28.2398 - val_dense_22_loss: 0.0221\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 88.37345\n",
            "Epoch 56/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.0898 - dense_20_loss: 0.1937 - dense_21_loss: 27.3842 - dense_22_loss: 0.0130 - val_loss: 90.9383 - val_dense_20_loss: 0.2032 - val_dense_21_loss: 27.7341 - val_dense_22_loss: 0.0112\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 88.37345\n",
            "Epoch 57/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.6388 - dense_20_loss: 0.1931 - dense_21_loss: 27.2127 - dense_22_loss: 0.0125 - val_loss: 90.5343 - val_dense_20_loss: 0.2030 - val_dense_21_loss: 27.3983 - val_dense_22_loss: 0.0112\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 88.37345\n",
            "Epoch 58/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.4943 - dense_20_loss: 0.1926 - dense_21_loss: 27.1776 - dense_22_loss: 0.0128 - val_loss: 97.1868 - val_dense_20_loss: 0.2095 - val_dense_21_loss: 31.3146 - val_dense_22_loss: 0.0151\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 88.37345\n",
            "Epoch 59/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 90.2514 - dense_20_loss: 0.1970 - dense_21_loss: 28.1365 - dense_22_loss: 0.0151 - val_loss: 89.4696 - val_dense_20_loss: 0.1980 - val_dense_21_loss: 27.8179 - val_dense_22_loss: 0.0112\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 88.37345\n",
            "Epoch 60/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.8453 - dense_20_loss: 0.1931 - dense_21_loss: 27.3444 - dense_22_loss: 0.0129 - val_loss: 102.8828 - val_dense_20_loss: 0.2221 - val_dense_21_loss: 32.6407 - val_dense_22_loss: 0.0181\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 88.37345\n",
            "Epoch 61/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.5695 - dense_20_loss: 0.1961 - dense_21_loss: 27.9134 - dense_22_loss: 0.0141 - val_loss: 89.0844 - val_dense_20_loss: 0.1972 - val_dense_21_loss: 27.2359 - val_dense_22_loss: 0.0134\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 88.37345\n",
            "Epoch 62/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 88.5875 - dense_20_loss: 0.1948 - dense_21_loss: 27.5068 - dense_22_loss: 0.0132 - val_loss: 90.3207 - val_dense_20_loss: 0.1983 - val_dense_21_loss: 28.1045 - val_dense_22_loss: 0.0136\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 88.37345\n",
            "Epoch 63/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.1160 - dense_20_loss: 0.1932 - dense_21_loss: 27.2953 - dense_22_loss: 0.0143 - val_loss: 88.2626 - val_dense_20_loss: 0.1974 - val_dense_21_loss: 26.9613 - val_dense_22_loss: 0.0104\n",
            "\n",
            "Epoch 00063: val_loss improved from 88.37345 to 88.26264, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 64/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.9253 - dense_20_loss: 0.1934 - dense_21_loss: 27.3281 - dense_22_loss: 0.0129 - val_loss: 88.5874 - val_dense_20_loss: 0.1985 - val_dense_21_loss: 26.9395 - val_dense_22_loss: 0.0105\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 88.26264\n",
            "Epoch 65/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.7629 - dense_20_loss: 0.1948 - dense_21_loss: 27.4542 - dense_22_loss: 0.0144 - val_loss: 90.2865 - val_dense_20_loss: 0.2014 - val_dense_21_loss: 27.8119 - val_dense_22_loss: 0.0103\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 88.26264\n",
            "Epoch 66/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.0246 - dense_20_loss: 0.1920 - dense_21_loss: 27.0532 - dense_22_loss: 0.0119 - val_loss: 113.5306 - val_dense_20_loss: 0.2534 - val_dense_21_loss: 33.0696 - val_dense_22_loss: 0.0223\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 88.26264\n",
            "Epoch 67/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.9585 - dense_20_loss: 0.1948 - dense_21_loss: 27.7047 - dense_22_loss: 0.0141 - val_loss: 90.2750 - val_dense_20_loss: 0.1989 - val_dense_21_loss: 28.5885 - val_dense_22_loss: 0.0101\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 88.26264\n",
            "Epoch 68/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 86.8737 - dense_20_loss: 0.1913 - dense_21_loss: 27.1050 - dense_22_loss: 0.0119 - val_loss: 90.1113 - val_dense_20_loss: 0.1972 - val_dense_21_loss: 27.5604 - val_dense_22_loss: 0.0169\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 88.26264\n",
            "Epoch 69/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.7795 - dense_20_loss: 0.1928 - dense_21_loss: 27.3020 - dense_22_loss: 0.0132 - val_loss: 107.8075 - val_dense_20_loss: 0.2257 - val_dense_21_loss: 33.3018 - val_dense_22_loss: 0.0339\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 88.26264\n",
            "Epoch 70/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.5647 - dense_20_loss: 0.1965 - dense_21_loss: 27.8630 - dense_22_loss: 0.0138 - val_loss: 90.3450 - val_dense_20_loss: 0.1982 - val_dense_21_loss: 27.1338 - val_dense_22_loss: 0.0188\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 88.26264\n",
            "Epoch 71/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.5845 - dense_20_loss: 0.1924 - dense_21_loss: 27.2347 - dense_22_loss: 0.0132 - val_loss: 88.9720 - val_dense_20_loss: 0.1976 - val_dense_21_loss: 27.3373 - val_dense_22_loss: 0.0118\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 88.26264\n",
            "Epoch 72/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.6786 - dense_20_loss: 0.1957 - dense_21_loss: 28.0889 - dense_22_loss: 0.0144 - val_loss: 90.7336 - val_dense_20_loss: 0.1986 - val_dense_21_loss: 27.0427 - val_dense_22_loss: 0.0206\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 88.26264\n",
            "Epoch 73/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.3694 - dense_20_loss: 0.1920 - dense_21_loss: 27.2484 - dense_22_loss: 0.0126 - val_loss: 92.4017 - val_dense_20_loss: 0.2034 - val_dense_21_loss: 27.9612 - val_dense_22_loss: 0.0171\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 88.26264\n",
            "Epoch 74/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.0138 - dense_20_loss: 0.1932 - dense_21_loss: 27.3253 - dense_22_loss: 0.0136 - val_loss: 90.2099 - val_dense_20_loss: 0.1991 - val_dense_21_loss: 27.2562 - val_dense_22_loss: 0.0161\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 88.26264\n",
            "Epoch 75/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.7764 - dense_20_loss: 0.1945 - dense_21_loss: 27.6708 - dense_22_loss: 0.0138 - val_loss: 90.8509 - val_dense_20_loss: 0.2005 - val_dense_21_loss: 27.5269 - val_dense_22_loss: 0.0159\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 88.26264\n",
            "Epoch 76/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 86.3574 - dense_20_loss: 0.1899 - dense_21_loss: 26.9859 - dense_22_loss: 0.0120 - val_loss: 91.6229 - val_dense_20_loss: 0.2027 - val_dense_21_loss: 28.1374 - val_dense_22_loss: 0.0134\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 88.26264\n",
            "Epoch 77/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 86.3657 - dense_20_loss: 0.1903 - dense_21_loss: 26.8191 - dense_22_loss: 0.0123 - val_loss: 88.6291 - val_dense_20_loss: 0.1964 - val_dense_21_loss: 26.9838 - val_dense_22_loss: 0.0137\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 88.26264\n",
            "Epoch 78/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 87.2123 - dense_20_loss: 0.1920 - dense_21_loss: 27.1236 - dense_22_loss: 0.0125 - val_loss: 94.3718 - val_dense_20_loss: 0.2083 - val_dense_21_loss: 29.6436 - val_dense_22_loss: 0.0112\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 88.26264\n",
            "Epoch 79/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.4447 - dense_20_loss: 0.1920 - dense_21_loss: 27.1799 - dense_22_loss: 0.0133 - val_loss: 98.1855 - val_dense_20_loss: 0.2210 - val_dense_21_loss: 29.4723 - val_dense_22_loss: 0.0120\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 88.26264\n",
            "Epoch 80/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 89.3740 - dense_20_loss: 0.1948 - dense_21_loss: 28.0066 - dense_22_loss: 0.0147 - val_loss: 89.3566 - val_dense_20_loss: 0.1978 - val_dense_21_loss: 27.4283 - val_dense_22_loss: 0.0129\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 88.26264\n",
            "Epoch 81/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.7301 - dense_20_loss: 0.1927 - dense_21_loss: 27.3051 - dense_22_loss: 0.0130 - val_loss: 88.5166 - val_dense_20_loss: 0.1974 - val_dense_21_loss: 26.9856 - val_dense_22_loss: 0.0116\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 88.26264\n",
            "Epoch 82/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 86.7008 - dense_20_loss: 0.1911 - dense_21_loss: 26.9220 - dense_22_loss: 0.0123 - val_loss: 88.4641 - val_dense_20_loss: 0.1962 - val_dense_21_loss: 27.1771 - val_dense_22_loss: 0.0122\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 88.26264\n",
            "Epoch 83/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 86.5804 - dense_20_loss: 0.1903 - dense_21_loss: 26.9731 - dense_22_loss: 0.0125 - val_loss: 91.4652 - val_dense_20_loss: 0.1982 - val_dense_21_loss: 29.2285 - val_dense_22_loss: 0.0138\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 88.26264\n",
            "Epoch 84/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 86.7264 - dense_20_loss: 0.1906 - dense_21_loss: 27.0775 - dense_22_loss: 0.0124 - val_loss: 89.6347 - val_dense_20_loss: 0.1985 - val_dense_21_loss: 27.6011 - val_dense_22_loss: 0.0125\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 88.26264\n",
            "Epoch 85/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 86.1149 - dense_20_loss: 0.1893 - dense_21_loss: 26.9852 - dense_22_loss: 0.0117 - val_loss: 88.9999 - val_dense_20_loss: 0.1986 - val_dense_21_loss: 26.9152 - val_dense_22_loss: 0.0125\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 88.26264\n",
            "Epoch 86/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 87.1863 - dense_20_loss: 0.1916 - dense_21_loss: 27.2083 - dense_22_loss: 0.0124 - val_loss: 94.0844 - val_dense_20_loss: 0.2088 - val_dense_21_loss: 28.2873 - val_dense_22_loss: 0.0158\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 88.26264\n",
            "Epoch 87/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 86.5576 - dense_20_loss: 0.1904 - dense_21_loss: 27.0692 - dense_22_loss: 0.0118 - val_loss: 92.8520 - val_dense_20_loss: 0.2094 - val_dense_21_loss: 27.8953 - val_dense_22_loss: 0.0107\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 88.26264\n",
            "Epoch 88/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 89.1593 - dense_20_loss: 0.1945 - dense_21_loss: 27.9754 - dense_22_loss: 0.0142 - val_loss: 88.7458 - val_dense_20_loss: 0.1975 - val_dense_21_loss: 27.1075 - val_dense_22_loss: 0.0119\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 88.26264\n",
            "Epoch 89/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.1795 - dense_20_loss: 0.1932 - dense_21_loss: 27.5487 - dense_22_loss: 0.0133 - val_loss: 93.4014 - val_dense_20_loss: 0.2003 - val_dense_21_loss: 28.1812 - val_dense_22_loss: 0.0256\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 88.26264\n",
            "Epoch 90/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 86.6552 - dense_20_loss: 0.1896 - dense_21_loss: 27.0771 - dense_22_loss: 0.0135 - val_loss: 92.4478 - val_dense_20_loss: 0.2030 - val_dense_21_loss: 27.8903 - val_dense_22_loss: 0.0183\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 88.26264\n",
            "Epoch 91/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.0686 - dense_20_loss: 0.1914 - dense_21_loss: 27.1361 - dense_22_loss: 0.0126 - val_loss: 91.7918 - val_dense_20_loss: 0.2042 - val_dense_21_loss: 28.2644 - val_dense_22_loss: 0.0113\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 88.26264\n",
            "Epoch 92/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 86.1831 - dense_20_loss: 0.1894 - dense_21_loss: 27.0133 - dense_22_loss: 0.0118 - val_loss: 87.0868 - val_dense_20_loss: 0.1941 - val_dense_21_loss: 26.7510 - val_dense_22_loss: 0.0105\n",
            "\n",
            "Epoch 00092: val_loss improved from 88.26264 to 87.08681, saving model to ./MLP_output/MLP_model.hdf5\n",
            "Epoch 93/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 86.6696 - dense_20_loss: 0.1905 - dense_21_loss: 27.0801 - dense_22_loss: 0.0121 - val_loss: 88.6844 - val_dense_20_loss: 0.1992 - val_dense_21_loss: 26.8247 - val_dense_22_loss: 0.0106\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 87.08681\n",
            "Epoch 94/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 85.8853 - dense_20_loss: 0.1889 - dense_21_loss: 26.8940 - dense_22_loss: 0.0116 - val_loss: 90.8789 - val_dense_20_loss: 0.2041 - val_dense_21_loss: 27.4344 - val_dense_22_loss: 0.0111\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 87.08681\n",
            "Epoch 95/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 85.6176 - dense_20_loss: 0.1883 - dense_21_loss: 26.7640 - dense_22_loss: 0.0119 - val_loss: 93.4866 - val_dense_20_loss: 0.2063 - val_dense_21_loss: 29.4870 - val_dense_22_loss: 0.0105\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 87.08681\n",
            "Epoch 96/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 87.0710 - dense_20_loss: 0.1917 - dense_21_loss: 26.9846 - dense_22_loss: 0.0129 - val_loss: 96.3466 - val_dense_20_loss: 0.2147 - val_dense_21_loss: 29.7573 - val_dense_22_loss: 0.0109\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 87.08681\n",
            "Epoch 97/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 88.1521 - dense_20_loss: 0.1934 - dense_21_loss: 27.4774 - dense_22_loss: 0.0132 - val_loss: 95.2435 - val_dense_20_loss: 0.2082 - val_dense_21_loss: 30.3384 - val_dense_22_loss: 0.0123\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 87.08681\n",
            "Epoch 98/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 87.1758 - dense_20_loss: 0.1910 - dense_21_loss: 27.1671 - dense_22_loss: 0.0136 - val_loss: 88.4460 - val_dense_20_loss: 0.1957 - val_dense_21_loss: 27.2259 - val_dense_22_loss: 0.0125\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 87.08681\n",
            "Epoch 99/100\n",
            "42750/42750 [==============================] - 2s 46us/step - loss: 86.2373 - dense_20_loss: 0.1897 - dense_21_loss: 26.8826 - dense_22_loss: 0.0122 - val_loss: 91.6989 - val_dense_20_loss: 0.2033 - val_dense_21_loss: 28.5591 - val_dense_22_loss: 0.0108\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 87.08681\n",
            "Epoch 100/100\n",
            "42750/42750 [==============================] - 2s 47us/step - loss: 87.8767 - dense_20_loss: 0.1926 - dense_21_loss: 27.4784 - dense_22_loss: 0.0131 - val_loss: 87.4311 - val_dense_20_loss: 0.1945 - val_dense_21_loss: 27.0095 - val_dense_22_loss: 0.0104\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 87.08681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PTmTP4dZBWa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e39ed70-0f13-45c3-8af9-9bf0914844b7"
      },
      "source": [
        "import os\n",
        "if os.path.exists(MODELPATH):\n",
        "    model.load_weights(MODELPATH)\n",
        "    # 若成功加载前面保存的参数，输出下列信息\n",
        "    print(\"checkpoint_loaded\")\n",
        "Y_hat = model.predict(X)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint_loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Uc6fdFZNgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6025aa1-3126-4700-ad45-56969e270444"
      },
      "source": [
        "Y_hat = np.transpose(Y_hat).reshape(-1,3)\n",
        "err_mat = np.abs(Y-Y_hat)/Y\n",
        "err_mat = np.sum(err_mat,axis=1)\n",
        "err_mat = np.sum(err_mat)\n",
        "print(err_mat/Y.shape[0])\n",
        "#0.9704514192587647"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.3430223805562302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkMg_43QUYlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e7c1dc1-e47a-479a-afd0-5bb4b103616b"
      },
      "source": [
        "err_WMAE = np.dot(np.abs(Y-Y_hat),np.array([300,1,200]))\n",
        "err_WMAE = np.sum(err_WMAE)/Y.shape[0]\n",
        "print(err_WMAE)\n",
        "#145.68255323961225"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88.47774705235244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t15AqhB-aUG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test_hat = model.predict(x_test)\n",
        "Y_test_hat = np.transpose(Y_test_hat).reshape(-1,3)\n",
        "np.savetxt(dirPath+'MLP_Multioutput_1.csv',Y_test_hat,delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRoIQ2-EbsYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "f5917dcc-6ae9-42eb-cdb9-3ee2bafac727"
      },
      "source": [
        "# Save model to your Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n34h467dHye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "31ca8a4b-4f5a-473c-a4d3-788200b427c9"
      },
      "source": [
        "!zip MLP_output MLP_output/*"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: MLP_output/MLP.csv (deflated 61%)\n",
            "  adding: MLP_output/MLP_history.pickle (deflated 41%)\n",
            "  adding: MLP_output/MLP_model.hdf5 (deflated 43%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPtpSwm0dUaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv MLP_output.zip '/gdrive/My Drive/term_2019_1/MLTech/Final'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}